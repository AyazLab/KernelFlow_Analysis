{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import snirf\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# %matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from scipy.signal import firwin, lfilter\n",
    "from typing import Union\n",
    "from statistics import mean\n",
    "from behav_analysis import Participant_Behav\n",
    "from data_functions import Data_Functions, load_results\n",
    "\n",
    "\n",
    "class Process_Flow:\n",
    "    \"\"\"\n",
    "    This class contains functions for processing Kernel Flow data.\n",
    "    Wrapper around an snirf.Snirf object.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filepath: str) -> None:\n",
    "        \"\"\"\n",
    "        Initialize by loading SNIRF file.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to SNIRF file.\n",
    "        \"\"\"\n",
    "        self.data_fun = Data_Functions()\n",
    "        self.snirf_file = self.load_snirf(filepath)\n",
    "\n",
    "    def load_snirf(self, filepath: str) -> snirf.Snirf:\n",
    "        \"\"\"\n",
    "        Load SNIRF file.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            snirf.Snirf: SNIRF file object.\n",
    "        \"\"\"\n",
    "        return snirf.Snirf(filepath, \"r+\", dynamic_loading=True)\n",
    "\n",
    "    def get_time_origin(\n",
    "        self, fmt: str = \"datetime\", offset=True\n",
    "    ) -> Union[datetime.datetime, float]:\n",
    "        \"\"\"\n",
    "        Get the time origin (start time) from the SNIRF file.\n",
    "\n",
    "        Args:\n",
    "            fmt (str, optional): Format to get the time origin in: \"datetime\" or \"timestamp\". Defaults to \"datetime\".\n",
    "            offset (bool): Offset the datetime by 4 hours. Defaults to True.\n",
    "\n",
    "        Raises:\n",
    "            Exception: Invalid fmt argument.\n",
    "\n",
    "        Returns:\n",
    "            Union[datetime.datetime, float]:\n",
    "                datetime.datetime: Time origin datetime.\n",
    "                -or-\n",
    "                float: Time origin timestamp.\n",
    "        \"\"\"\n",
    "        start_date = self.snirf_file.nirs[0].metaDataTags.MeasurementDate\n",
    "        start_time = self.snirf_file.nirs[0].metaDataTags.MeasurementTime\n",
    "        start_str = start_date + \" \" + start_time\n",
    "        if offset:\n",
    "            time_origin = datetime.datetime.strptime(\n",
    "                start_str, \"%Y-%m-%d %H:%M:%S\"\n",
    "            ) - datetime.timedelta(\n",
    "                hours=4\n",
    "            )  # 4 hour offset\n",
    "        else:\n",
    "            time_origin = datetime.datetime.strptime(start_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "        if fmt.lower() == \"datetime\":\n",
    "            return time_origin\n",
    "        elif fmt.lower() == \"timestamp\":\n",
    "            return datetime.datetime.timestamp(time_origin)\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"Invalid 'fmt' argument. Must be 'datetime' or 'timestamp'.\"\n",
    "            )\n",
    "\n",
    "    def get_subject_ID(self) -> str:\n",
    "        \"\"\"\n",
    "        Get the subject ID from the SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            str: Subject ID.\n",
    "        \"\"\"\n",
    "        return self.snirf_file.nirs[0].metaDataTags.SubjectID\n",
    "\n",
    "    def get_time_rel(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get the relative time array from the SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Relative time array.\n",
    "        \"\"\"\n",
    "        return self.snirf_file.nirs[0].data[0].time\n",
    "\n",
    "    def get_time_abs(self, fmt: str = \"datetime\") -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Convert relative time array into an absolute time array.\n",
    "\n",
    "        Args:\n",
    "            fmt (str, optional): Format to get the time array in: \"datetime\" or \"timestamp\". Defaults to \"datetime\".\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Absolute time array.\n",
    "        \"\"\"\n",
    "        time_rel = self.get_time_rel()\n",
    "        if fmt.lower() == \"datetime\":\n",
    "            time_origin_dt = self.get_time_origin(\"datetime\")\n",
    "            return np.array(\n",
    "                [\n",
    "                    datetime.timedelta(seconds=time_rel[i]) + time_origin_dt\n",
    "                    for i in range(len(time_rel))\n",
    "                ]\n",
    "            )\n",
    "        elif fmt.lower() == \"timestamp\":\n",
    "            time_origin_ts = self.get_time_origin(\"timestamp\")\n",
    "            return time_rel + time_origin_ts\n",
    "\n",
    "    def get_data(\n",
    "        self, fmt: str = \"array\", cols: list[int | list | tuple] = None\n",
    "    ) -> Union[np.ndarray, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Get timeseries data from the SNIRF file.\n",
    "\n",
    "        Args:\n",
    "            fmt (str): Format of data (np.ndarray or pd.DataFrame). Defaults to \"array\".\n",
    "            cols (list[int | list | tuple]): Data cols to select. Single col, list of cols, or slice of cols.\n",
    "                                             Defaults to None (all columns).\n",
    "\n",
    "        Raises:\n",
    "            Exception: Invalid fmt argument.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Timeseries data array.\n",
    "        \"\"\"\n",
    "        if cols or cols == 0:\n",
    "            if isinstance(cols, tuple):\n",
    "                data = (\n",
    "                    self.snirf_file.nirs[0].data[0].dataTimeSeries[:, cols[0] : cols[1]]\n",
    "                )\n",
    "            else:\n",
    "                data = self.snirf_file.nirs[0].data[0].dataTimeSeries[:, cols]\n",
    "        else:\n",
    "            data = self.snirf_file.nirs[0].data[0].dataTimeSeries\n",
    "\n",
    "        if \"array\" in fmt.lower():\n",
    "            return data\n",
    "        elif \"dataframe\" in fmt.lower():\n",
    "            return pd.DataFrame(data)\n",
    "        else:\n",
    "            raise Exception(\"Invalid fmt argument. Must be 'array' or 'dataframe'.\")\n",
    "\n",
    "    def get_source_pos_2d(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get the 2D source position array.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: 2D source position array.\n",
    "        \"\"\"\n",
    "        return self.snirf_file.nirs[0].probe.sourcePos2D\n",
    "\n",
    "    def get_source_pos_3d(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get the 3D source position array.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: 3D source position array.\n",
    "        \"\"\"\n",
    "        return self.snirf_file.nirs[0].probe.sourcePos3D\n",
    "\n",
    "    def get_detector_pos_2d(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get the 2D detector position array.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: 2D detector position array.\n",
    "        \"\"\"\n",
    "        return self.snirf_file.nirs[0].probe.detectorPos2D\n",
    "\n",
    "    def get_detector_pos_3d(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get the 3D detector position array.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: 3D detector position array.\n",
    "        \"\"\"\n",
    "        return self.snirf_file.nirs[0].probe.detectorPos3D\n",
    "\n",
    "    def get_measurement_list(self) -> np.array:\n",
    "        \"\"\"\n",
    "        Get the data measurement list.\n",
    "\n",
    "        Returns:\n",
    "            np.array: Data measurement list array.\n",
    "        \"\"\"\n",
    "        return self.snirf_file.nirs[0].data[0].measurementList\n",
    "\n",
    "    def get_source_labels(self) -> np.array:\n",
    "        \"\"\"\n",
    "        Get the source labels.\n",
    "\n",
    "        Returns:\n",
    "            np.array: Source label array.\n",
    "        \"\"\"\n",
    "        return self.snirf_file.nirs[0].probe.sourceLabels\n",
    "\n",
    "    def get_detector_labels(self) -> np.array:\n",
    "        \"\"\"\n",
    "        Get the detector labels.\n",
    "\n",
    "        Returns:\n",
    "            np.array: Detector label array.\n",
    "        \"\"\"\n",
    "        return self.snirf_file.nirs[0].probe.detectorLabels\n",
    "\n",
    "    def get_marker_df(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get a DataFrame of marker data from the \"stim\" part of the SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Marker \"stim\" data.\n",
    "        \"\"\"\n",
    "        marker_data = self.snirf_file.nirs[0].stim[0].data\n",
    "        marker_data_cols = self.snirf_file.nirs[0].stim[0].dataLabels\n",
    "        return pd.DataFrame(marker_data, columns=marker_data_cols)\n",
    "\n",
    "    def get_unique_data_types(self) -> list:\n",
    "        \"\"\"\n",
    "        Get unique data types from the SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            list: Unique data types.\n",
    "        \"\"\"\n",
    "        data_types = []\n",
    "        for i in range(len(self.snirf_file.nirs[0].data[0].measurementList)):\n",
    "            data_type = self.snirf_file.nirs[0].data[0].measurementList[i].dataType\n",
    "            if data_type not in data_types:\n",
    "                data_types.append(data_type)\n",
    "        return data_types\n",
    "\n",
    "    def get_data_type_label(self, channel_num: int) -> str:\n",
    "        \"\"\"\n",
    "        Get the data type label for a channel(s).\n",
    "\n",
    "        Args:\n",
    "            channel_num (int): Channel number to get the data type label of.\n",
    "\n",
    "        Returns:\n",
    "            str: Data type label of the channel.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            self.snirf_file.nirs[0].data[0].measurementList[channel_num].dataTypeLabel\n",
    "        )\n",
    "\n",
    "    def get_unique_data_type_labels(self) -> list:\n",
    "        \"\"\"\n",
    "        Get unique data type labels from the SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            list: Unique data type labels.\n",
    "        \"\"\"\n",
    "        data_type_labels = []\n",
    "        for i in range(len(self.snirf_file.nirs[0].data[0].measurementList)):\n",
    "            data_type_label = (\n",
    "                self.snirf_file.nirs[0].data[0].measurementList[i].dataTypeLabel\n",
    "            )\n",
    "            if data_type_label not in data_type_labels:\n",
    "                data_type_labels.append(data_type_label)\n",
    "        return data_type_labels\n",
    "\n",
    "    def create_source_dict(self) -> dict:\n",
    "        \"\"\"\n",
    "        Count the occurrences of each source index.\n",
    "\n",
    "        Returns:\n",
    "            dict: Counts for each source index.\n",
    "        \"\"\"\n",
    "        source_dict = {}\n",
    "        for i in range(len(self.snirf_file.nirs[0].data[0].measurementList)):\n",
    "            source = self.snirf_file.nirs[0].data[0].measurementList[i].sourceIndex\n",
    "            source_dict[source] = source_dict.get(source, 0) + 1\n",
    "        source_dict = self.data_fun.sort_dict(source_dict, \"keys\")\n",
    "        return source_dict\n",
    "\n",
    "    def create_detector_dict(self) -> dict:\n",
    "        \"\"\"\n",
    "        Count the occurrences of each detector index.\n",
    "\n",
    "        Returns:\n",
    "            dict: Counts for each detector index.\n",
    "        \"\"\"\n",
    "        detector_dict = {}\n",
    "        for i in range(len(self.snirf_file.nirs[0].data[0].measurementList)):\n",
    "            detector = self.snirf_file.nirs[0].data[0].measurementList[i].detectorIndex\n",
    "            detector_dict[detector] = detector_dict.get(detector, 0) + 1\n",
    "        detector_dict = self.data_fun.sort_dict(detector_dict, \"keys\")\n",
    "        return detector_dict\n",
    "\n",
    "    def create_measurement_list_df(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create a DataFrame with all the data measurement list information.\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Data measurement list DataFrame.\n",
    "        \"\"\"\n",
    "        measurement_list = self.get_measurement_list()\n",
    "        dict_list = []\n",
    "\n",
    "        for i in range(len(measurement_list)):\n",
    "            measurement_list_i = measurement_list[i]\n",
    "            measurement_dict = {}\n",
    "            measurement_dict[\"measurement_list_index\"] = i + 1\n",
    "            measurement_dict[\"data_type\"] = measurement_list_i.dataType\n",
    "            measurement_dict[\"data_type_index\"] = measurement_list_i.dataTypeLabel\n",
    "            measurement_dict[\"detector_index\"] = measurement_list_i.detectorIndex\n",
    "            measurement_dict[\"source_index\"] = measurement_list_i.sourceIndex\n",
    "            dict_list.append(measurement_dict)\n",
    "\n",
    "        measurement_list_df = pd.DataFrame(dict_list)\n",
    "        return measurement_list_df\n",
    "\n",
    "    def create_source_df(self, dim: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create a DataFrame with the source labels and 2D or 3D source positions.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Source labels and positions.\n",
    "        \"\"\"\n",
    "        source_labels = self.get_source_labels()\n",
    "        if dim.lower() == \"2d\":\n",
    "            source_pos_2d = self.get_source_pos_2d()\n",
    "            source_data = [\n",
    "                (label, *lst) for label, lst in zip(source_labels, source_pos_2d)\n",
    "            ]\n",
    "            source_df = pd.DataFrame(\n",
    "                source_data, columns=[\"source_label\", \"source_x_pos\", \"source_y_pos\"]\n",
    "            )\n",
    "        elif dim.lower() == \"3d\":\n",
    "            source_pos_3d = self.get_source_pos_3d()\n",
    "            source_data = [\n",
    "                (label, *lst) for label, lst in zip(source_labels, source_pos_3d)\n",
    "            ]\n",
    "            source_df = pd.DataFrame(\n",
    "                source_data,\n",
    "                columns=[\n",
    "                    \"source_label\",\n",
    "                    \"source_x_pos\",\n",
    "                    \"source_y_pos\",\n",
    "                    \"source_pos_z\",\n",
    "                ],\n",
    "            )\n",
    "        f = lambda x: int(x.lstrip(\"S\"))\n",
    "        source_df.insert(1, \"source_index\", source_df[\"source_label\"].apply(f))\n",
    "        return source_df\n",
    "\n",
    "    def create_detector_df(self, dim: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create a DataFrame with the detector labels and 2D or 3D detector positions.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Detector labels and positions.\n",
    "        \"\"\"\n",
    "        detector_labels = self.get_detector_labels()\n",
    "        if dim.lower() == \"2d\":\n",
    "            detector_pos_2d = self.get_detector_pos_2d()\n",
    "            detector_data = [\n",
    "                (label, *lst) for label, lst in zip(detector_labels, detector_pos_2d)\n",
    "            ]\n",
    "            detector_df = pd.DataFrame(\n",
    "                detector_data,\n",
    "                columns=[\"detector_label\", \"detector_x_pos\", \"detector_y_pos\"],\n",
    "            )\n",
    "        elif dim.lower() == \"3d\":\n",
    "            detector_pos_3d = self.get_detector_pos_3d()\n",
    "            detector_data = [\n",
    "                (label, *lst) for label, lst in zip(detector_labels, detector_pos_3d)\n",
    "            ]\n",
    "            detector_df = pd.DataFrame(\n",
    "                detector_data,\n",
    "                columns=[\n",
    "                    \"detector_label\",\n",
    "                    \"detector_x_pos\",\n",
    "                    \"detector_y_pos\",\n",
    "                    \"detector_pos_z\",\n",
    "                ],\n",
    "            )\n",
    "        f = lambda x: int(x[1:3])\n",
    "        detector_df.insert(1, \"source_index\", detector_df[\"detector_label\"].apply(f))\n",
    "        detector_df.insert(1, \"detector_index\", range(1, detector_df.shape[0] + 1))\n",
    "        return detector_df\n",
    "\n",
    "    def create_source_detector_df(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create a DataFrame with the source and detector information for the inter-module channels.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Source and detector information for inter-module channels.\n",
    "        \"\"\"\n",
    "        measurement_list_df = self.create_measurement_list_df()\n",
    "        source_df = self.create_source_df(\"3D\")\n",
    "        detector_df = self.create_detector_df(\"3D\")\n",
    "        source_merge = pd.merge(measurement_list_df, source_df, on=\"source_index\")\n",
    "        merged_source_detector_df = pd.merge(\n",
    "            source_merge, detector_df, on=[\"detector_index\", \"source_index\"]\n",
    "        )\n",
    "        return merged_source_detector_df\n",
    "\n",
    "    def plot_pos_2d(self) -> None:\n",
    "        \"\"\"\n",
    "        Plot the detector/source 2D positions.\n",
    "        \"\"\"\n",
    "        detector_pos_2d = self.get_detector_pos_2d()\n",
    "        x_detector = detector_pos_2d[:, 0]\n",
    "        y_detector = detector_pos_2d[:, 1]\n",
    "\n",
    "        source_pos_2d = self.get_source_pos_2d()\n",
    "        x_source = source_pos_2d[:, 0]\n",
    "        y_source = source_pos_2d[:, 1]\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.scatter(x_detector, y_detector)\n",
    "        ax.scatter(x_source, y_source)\n",
    "        ax.set_title(\"Detector/Source 2D Plot\")\n",
    "        ax.set_xlabel(\"X-Position (mm)\")\n",
    "        ax.set_ylabel(\"Y-Position (mm)\")\n",
    "        ax.legend([\"Detector\", \"Source\"])\n",
    "\n",
    "    def plot_pos_3d(self) -> None:\n",
    "        \"\"\"\n",
    "        Plot the detector/source 3D positions.\n",
    "        \"\"\"\n",
    "        detector_pos_3d = self.get_detector_pos_3d()\n",
    "        x_detector = detector_pos_3d[:, 0]\n",
    "        y_detector = detector_pos_3d[:, 1]\n",
    "        z_detector = detector_pos_3d[:, 2]\n",
    "\n",
    "        source_pos_3d = self.get_source_pos_3d()\n",
    "        x_source = source_pos_3d[:, 0]\n",
    "        y_source = source_pos_3d[:, 1]\n",
    "        z_source = source_pos_3d[:, 2]\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection=\"3d\")\n",
    "        ax.scatter(x_detector, y_detector, z_detector)\n",
    "        ax.scatter(x_source, y_source, z_source)\n",
    "        ax.set_title(\"Detector/Source 3D Plot\")\n",
    "        ax.set_xlabel(\"X-Position (mm)\")\n",
    "        ax.set_ylabel(\"Y-Position (mm)\")\n",
    "        ax.set_zlabel(\"Z-Position (mm)\")\n",
    "        ax.legend([\"Detector\", \"Source\"])\n",
    "\n",
    "\n",
    "class Participant_Flow:\n",
    "    \"\"\"\n",
    "    This class contains functions, data structures, and info necessary for\n",
    "    processing Kernel Flow data from the experiments.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, par_num):\n",
    "        self.data_fun = Data_Functions()\n",
    "        self.adj_ts_markers = True\n",
    "        self.par_behav = Participant_Behav(par_num, self.adj_ts_markers)\n",
    "        self.par_num, self.par_ID = self.data_fun.process_par(par_num)\n",
    "        self.flow_raw_data_dir = os.path.join(\n",
    "            self.par_behav.raw_data_dir, self.par_ID, \"kernel_data\"\n",
    "        )\n",
    "        self.flow_processed_data_dir = os.path.join(\n",
    "            os.getcwd(), \"processed_data\", \"flow\"\n",
    "        )\n",
    "        self.flow_session_dict = self.create_flow_session_dict(wrapper=True)\n",
    "        self.time_offset_dict = self.create_time_offset_dict()\n",
    "        self.plot_color_dict = {\n",
    "            0: \"purple\",\n",
    "            1: \"orange\",\n",
    "            2: \"green\",\n",
    "            3: \"yellow\",\n",
    "            4: \"pink\",\n",
    "            5: \"skyblue\",\n",
    "        }\n",
    "\n",
    "    def calc_time_offset(self, exp_name: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the time offset (in seconds) between the behavioral and Kernel Flow data\n",
    "        files. Number of seconds that the Kernel Flow data is ahead of the behavioral data.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "\n",
    "        Returns:\n",
    "            float: Time offset (in seconds).\n",
    "        \"\"\"\n",
    "        exp = self.par_behav.get_exp(exp_name)\n",
    "        exp_start_ts = exp.start_ts\n",
    "        marker_sent_time = float(exp.marker_data[\"start_marker\"][\"sent_time\"])\n",
    "        session = self.par_behav.get_key_from_value(\n",
    "            self.par_behav.session_dict, exp_name\n",
    "        )\n",
    "        marker_df = self.create_abs_marker_df(session)\n",
    "        row = marker_df.loc[marker_df[\"Marker\"].str.startswith(exp_name)].reset_index()\n",
    "        if (\n",
    "            exp_name == \"go_no_go\"\n",
    "        ):  # Go/No-go experiment is missing start timestamp marker\n",
    "            try:\n",
    "                kernel_start_ts = row.loc[0, \"Start timestamp\"]\n",
    "                time_offset = kernel_start_ts - (exp_start_ts + marker_sent_time)\n",
    "            except:\n",
    "                time_offset = \"NaN\"\n",
    "        else:\n",
    "            kernel_start_ts = row.loc[0, \"Start timestamp\"]\n",
    "            time_offset = kernel_start_ts - (exp_start_ts + marker_sent_time)\n",
    "        return float(time_offset)\n",
    "\n",
    "    def create_time_offset_dict(self) -> dict:\n",
    "        \"\"\"\n",
    "        Create a dictionary containing the time offset (in seconds) for each experiment.\n",
    "\n",
    "        Returns:\n",
    "            dict: Time offset dictionary.\n",
    "        \"\"\"\n",
    "        time_offset_dict = {}\n",
    "        for exp_name in self.par_behav.exp_order:\n",
    "            if (\n",
    "                exp_name == \"go_no_go\"\n",
    "            ):  # Go/No-go experiment is missing start timestamp marker\n",
    "                if np.isnan(self.calc_time_offset(exp_name)):\n",
    "                    session = self.par_behav.get_key_from_value(\n",
    "                        self.par_behav.session_dict, exp_name\n",
    "                    )\n",
    "                    session_exp_names = self.par_behav.session_dict[session]\n",
    "                    other_exp_names = [\n",
    "                        temp_exp_name\n",
    "                        for temp_exp_name in session_exp_names\n",
    "                        if temp_exp_name != \"go_no_go\"\n",
    "                    ]\n",
    "                    other_exp_time_offsets = []\n",
    "                    for temp_exp_name in other_exp_names:\n",
    "                        time_offset = self.calc_time_offset(temp_exp_name)\n",
    "                        other_exp_time_offsets.append(time_offset)\n",
    "                    avg_time_offset = np.mean(other_exp_time_offsets)\n",
    "                    time_offset_dict[exp_name] = avg_time_offset\n",
    "            else:\n",
    "                time_offset_dict[exp_name] = self.calc_time_offset(exp_name)\n",
    "        for session, exp_list in self.par_behav.session_dict.items():\n",
    "            session_offset = np.mean(\n",
    "                [time_offset_dict[exp_name] for exp_name in exp_list]\n",
    "            )\n",
    "            time_offset_dict[session] = session_offset\n",
    "        return time_offset_dict\n",
    "\n",
    "    def get_time_offset(self, exp_name: str) -> float:\n",
    "        \"\"\"\n",
    "        Get the time offset for an experiment.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Experiment name.\n",
    "\n",
    "        Returns:\n",
    "            float: Time offset (in seconds).\n",
    "        \"\"\"\n",
    "        return self.time_offset_dict[exp_name]\n",
    "\n",
    "    def offset_time_array(self, exp_name: str, time_array: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Offset a Kernel Flow datetime array for an experiment by the time-offset.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "            time_array (np.ndarray): Datetime array.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Time-offset datetime array.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            time_offset = self.get_time_offset(exp_name)\n",
    "        except KeyError:  # if experiment start time is missing, use avg of other session experiments\n",
    "            time_offset_list = []\n",
    "            for exp_name in self.par_behav.exp_order:\n",
    "                try:\n",
    "                    time_offset = self.get_time_offset(exp_name)\n",
    "                    time_offset_list.append(time_offset)\n",
    "                except KeyError:\n",
    "                    pass\n",
    "            time_offset = mean(time_offset_list)\n",
    "        time_offset_dt = datetime.timedelta(seconds=time_offset)\n",
    "        time_abs_dt_offset = time_array - time_offset_dt\n",
    "        return time_abs_dt_offset\n",
    "\n",
    "    def load_flow_session(\n",
    "        self, session: list[str | int], wrapper: bool = False\n",
    "    ) -> snirf.Snirf:\n",
    "        \"\"\"\n",
    "        Load Kernel Flow data for an experiment session.\n",
    "\n",
    "        Args:\n",
    "            session list[str | int]: Experiment session.\n",
    "            wrapper (bool, optional) Option to return Process_Flow-wrapped SNIRF file.\n",
    "                                     Defaults to false.\n",
    "\n",
    "        Raises:\n",
    "            Exception: Invalid session number argument.\n",
    "\n",
    "        Returns:\n",
    "            snirf.Snirf: SNIRF file object.\n",
    "            -or-\n",
    "            Process_Flow object for each experiment session.\n",
    "        \"\"\"\n",
    "        if isinstance(session, str):\n",
    "            if \"session\" not in session:\n",
    "                session = f\"session_{session}\"\n",
    "        elif isinstance(session, int):\n",
    "            session = f\"session_{session}\"\n",
    "        try:\n",
    "            session_dir = os.path.join(self.flow_raw_data_dir, session)\n",
    "            filename = os.listdir(session_dir)[0]\n",
    "            filepath = os.path.join(session_dir, filename)\n",
    "            if wrapper:\n",
    "                return Process_Flow(filepath)\n",
    "            else:\n",
    "                return Process_Flow(filepath).snirf_file\n",
    "        except:\n",
    "            print(\"Invalid session number.\")\n",
    "            raise\n",
    "\n",
    "    def load_flow_exp(self, exp_name: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load Kernel Flow data for the time frame of a specified experiment.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Kernel Flow data for an experiment.\n",
    "        \"\"\"\n",
    "        session = self.par_behav.get_key_from_value(\n",
    "            self.par_behav.session_dict, exp_name\n",
    "        )\n",
    "        flow_session = self.load_flow_session(session, wrapper=True)\n",
    "\n",
    "        start_dt = self.par_behav.get_start_dt(exp_name, self.adj_ts_markers)\n",
    "        end_dt = self.par_behav.get_end_dt(exp_name, self.adj_ts_markers)\n",
    "        time_abs_dt = flow_session.get_time_abs(\"datetime\")\n",
    "        time_abs_dt_offset = self.offset_time_array(exp_name, time_abs_dt)\n",
    "        start_idx = self.par_behav.get_start_index_dt(time_abs_dt_offset, start_dt)\n",
    "        end_idx = self.par_behav.get_end_index_dt(time_abs_dt_offset, end_dt)\n",
    "\n",
    "        flow_data = flow_session.get_data(\"dataframe\")\n",
    "        flow_data.insert(0, \"datetime\", time_abs_dt_offset)\n",
    "        return flow_data.iloc[start_idx:end_idx, :]\n",
    "\n",
    "    def create_flow_session_dict(self, wrapper: bool = False) -> dict:\n",
    "        \"\"\"\n",
    "        Create a dictionary of Kernel Flow data for all experiment sessions.\n",
    "\n",
    "        wrapper (bool, optional) Option to return Process_Flow-wrapped SNIRF file.\n",
    "                                 Default to false.\n",
    "\n",
    "        Returns:\n",
    "            dict: Kernel Flow data for all experiment sessions.\n",
    "                keys:\n",
    "                    \"session_1001\", \"session_1002\", \"session_1003\"\n",
    "                values:\n",
    "                    SNIRF file object for each experiment session\n",
    "                    -or-\n",
    "                    Process_Flow object for each experiment session\n",
    "        \"\"\"\n",
    "        flow_session_dict = {}\n",
    "        for session in self.par_behav.session_dict.keys():\n",
    "            flow_session_dict[session] = self.load_flow_session(session, wrapper)\n",
    "        return flow_session_dict\n",
    "\n",
    "    def create_abs_marker_df(self, session: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Convert the \"stim\" marker DataFrame into absolute time.\n",
    "\n",
    "        Args:\n",
    "            session (str): Experiment session.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Marker \"stim\" data in absolute time.\n",
    "        \"\"\"\n",
    "        marker_df = self.flow_session_dict[session].get_marker_df()\n",
    "        time_origin_ts = self.flow_session_dict[session].get_time_origin(\"timestamp\")\n",
    "        marker_df[\"Timestamp\"] = marker_df[\"Timestamp\"] + time_origin_ts\n",
    "        marker_df.rename({\"Timestamp\": \"Start timestamp\"}, axis=1, inplace=True)\n",
    "\n",
    "        for idx, row in marker_df.iterrows():\n",
    "            end_ts = row[\"Start timestamp\"] + row[\"Duration\"]\n",
    "            marker_df.at[idx, \"End timestamp\"] = end_ts\n",
    "            exp_num = int(row[\"Experiment\"])\n",
    "            exp_name = self.par_behav.marker_dict[exp_num]\n",
    "            marker_df.at[idx, \"Experiment\"] = exp_name\n",
    "\n",
    "        marker_df.rename({\"Experiment\": \"Marker\"}, axis=1, inplace=True)\n",
    "        marker_df.drop([\"Value\"], axis=1, inplace=True)\n",
    "        marker_df = marker_df[\n",
    "            [\"Marker\", \"Start timestamp\", \"Duration\", \"End timestamp\"]\n",
    "        ]\n",
    "        return marker_df\n",
    "\n",
    "    def create_exp_stim_response_dict(self, exp_name: str) -> dict:\n",
    "        \"\"\"\n",
    "        Create a dictionary that contains the processed Kernel Flow data in response\n",
    "        to a stimulus. It is organized by block (keys) and for each block, the value is\n",
    "        a list of Pandas series. Each series is normalized, averaged, Kernel Flow data\n",
    "        during a presented stimulus duration for each channel. Each block is baselined\n",
    "        to the first 5 seconds, and the stim response is averaged over the stimulus\n",
    "        presentation duration.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "\n",
    "        Returns:\n",
    "            dict:\n",
    "                keys:\n",
    "                    \"block 1\", \"block 2\", ... \"block N\"\n",
    "                values:\n",
    "                    dicts:\n",
    "                        keys:\n",
    "                            \"trial 1\", \"trial 2\", ... \"trial N\"\n",
    "                        values:\n",
    "                            lists of averaged, normalized Kernel Flow data series for each\n",
    "                            channel during the stimulus duration\n",
    "        \"\"\"\n",
    "        exp_results = load_results(\n",
    "            self.par_behav.processed_data_dir, exp_name, self.par_behav.par_num\n",
    "        )\n",
    "        flow_exp = self.load_flow_exp(exp_name)\n",
    "        session = self.par_behav.get_key_from_value(\n",
    "            self.par_behav.session_dict, exp_name\n",
    "        )\n",
    "        ts_list = self.flow_session_dict[session].get_time_abs(\"timestamp\")\n",
    "        exp_time_offset = self.time_offset_dict[exp_name]\n",
    "        exp_by_block = self.par_behav.by_block_ts_dict[exp_name]\n",
    "\n",
    "        if exp_name == \"king_devick\":  # TODO\n",
    "            pass\n",
    "        else:\n",
    "            blocks = list(exp_results[\"block\"].unique())\n",
    "            exp_stim_resp_dict = {\n",
    "                block: {} for block in blocks\n",
    "            }  # initialize with unique blocks\n",
    "            processed_blocks = []\n",
    "            for (\n",
    "                block_start_ts,\n",
    "                block_end_ts,\n",
    "            ) in exp_by_block.keys():  # for each block in the experiment\n",
    "                block_start_ts_offset = block_start_ts + exp_time_offset\n",
    "                block_start_idx, _ = self.data_fun.find_closest_ts(\n",
    "                    block_start_ts_offset, ts_list\n",
    "                )\n",
    "                block_end_ts_offset = block_end_ts + exp_time_offset\n",
    "                block_end_idx, _ = self.data_fun.find_closest_ts(\n",
    "                    block_end_ts_offset, ts_list\n",
    "                )\n",
    "                block_rows = flow_exp.loc[\n",
    "                    block_start_idx:block_end_idx, 0:\n",
    "                ]  # rows from block start to end\n",
    "\n",
    "                baseline_rows = flow_exp.loc[\n",
    "                    block_start_idx : block_start_idx + 35, 0:\n",
    "                ]  # first 5 seconds of a block\n",
    "                baseline = pd.DataFrame(baseline_rows.mean()).T\n",
    "                baseline_df = pd.concat(\n",
    "                    [baseline] * block_rows.shape[0], ignore_index=True\n",
    "                )\n",
    "                baseline_df = baseline_df.set_index(\n",
    "                    pd.Index(range(block_start_idx, block_start_idx + len(baseline_df)))\n",
    "                )\n",
    "\n",
    "                block_rows_norm = block_rows.subtract(\n",
    "                    baseline_df, fill_value=0\n",
    "                )  # normalize the block rows\n",
    "                processed_blocks.append(block_rows_norm)\n",
    "\n",
    "            processed_block_df = pd.concat(\n",
    "                processed_blocks\n",
    "            )  # all processed blocks for an experiment\n",
    "\n",
    "            for _, row in exp_results.iterrows():\n",
    "                stim_start_ts = row[\"stim_start\"]\n",
    "                stim_start_ts_offset = stim_start_ts + exp_time_offset\n",
    "                start_idx, _ = self.data_fun.find_closest_ts(\n",
    "                    stim_start_ts_offset, ts_list\n",
    "                )\n",
    "                stim_end_ts = row[\"stim_end\"]\n",
    "                stim_end_ts_offset = stim_end_ts + exp_time_offset\n",
    "                end_idx, _ = self.data_fun.find_closest_ts(stim_end_ts_offset, ts_list)\n",
    "\n",
    "                stim_rows = processed_block_df.loc[start_idx:end_idx, 0:]\n",
    "                avg_stim_rows = stim_rows.mean()  # all channels for a stim\n",
    "\n",
    "                block = row[\"block\"]\n",
    "                trial = row[\"trial\"]\n",
    "\n",
    "                if trial not in exp_stim_resp_dict[block].keys():\n",
    "                    exp_stim_resp_dict[block][trial] = []\n",
    "                exp_stim_resp_dict[block][trial].append(\n",
    "                    avg_stim_rows\n",
    "                )  # add to a block in dict\n",
    "\n",
    "        return exp_stim_resp_dict\n",
    "\n",
    "    def create_exp_stim_response_df(self, exp_name: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create a DataFrame that contains the processed Kernel Flow data in response\n",
    "        to each stimulus in an experiment. Each channel is normalized and averaged.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Processed Kernel Flow data.\n",
    "        \"\"\"\n",
    "\n",
    "        def _split_col(row: pd.Series) -> pd.Series:\n",
    "            \"\"\"\n",
    "            Split a column containing an array into separate columns for each\n",
    "            element in the array.\n",
    "\n",
    "            Args:\n",
    "                row (pd.Series): DataFrame row.\n",
    "\n",
    "            Returns:\n",
    "                pd.Series: DataFrame row with split column.\n",
    "            \"\"\"\n",
    "            arr = row[\"Channels\"]\n",
    "            num_elements = len(arr)\n",
    "            col_names = [i for i in range(num_elements)]\n",
    "            return pd.Series(arr, index=col_names)\n",
    "\n",
    "        exp_baseline_avg_dict = self.create_exp_stim_response_dict(exp_name)\n",
    "        rows = []\n",
    "        for block, block_data in sorted(exp_baseline_avg_dict.items()):\n",
    "            for trial, stim_resp_data in block_data.items():\n",
    "                trial_avg = np.mean(stim_resp_data, axis=0)\n",
    "                row = {\n",
    "                    \"Participant\": self.par_num,\n",
    "                    \"Block\": block,\n",
    "                    \"Channels\": trial_avg,\n",
    "                }\n",
    "                rows.append(row)\n",
    "\n",
    "        stim_resp_df = pd.DataFrame(rows)\n",
    "        channel_cols = stim_resp_df.apply(_split_col, axis=1)\n",
    "        stim_resp_df = pd.concat(\n",
    "            [stim_resp_df, channel_cols], axis=1\n",
    "        )  # merge with original DataFrame\n",
    "        stim_resp_df = stim_resp_df.drop(\n",
    "            \"Channels\", axis=1\n",
    "        )  # drop the original \"Channels\" column\n",
    "        return stim_resp_df\n",
    "\n",
    "    def create_inter_module_exp_results_df(\n",
    "        self, exp_name: str, fmt: str = None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create a DataFrame with the inter-module channels for an experiment.\n",
    "        This DataFrame can include both HbO and HbR channels in alternating columns\n",
    "        or just \"HbO\", \"HbR\", \"HbTot\", or \"HbDiff\" channels.\n",
    "\n",
    "        Args:\n",
    "            fmt (str, optional): \"HbO\", \"HbR\", \"HbTot\", or \"HbDiff\" channels.\n",
    "                                 Defaults to None (all inter-module channels).\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Inter-module channels for an experiment.\n",
    "        \"\"\"\n",
    "\n",
    "        def _compute_df(fmt: str) -> pd.DataFrame:\n",
    "            \"\"\"\n",
    "            Create the HbTot and HbDiff DataFrames.\n",
    "\n",
    "            Args:\n",
    "                fmt (str): \"HbTot\" or \"HbDiff\".\n",
    "\n",
    "            Returns:\n",
    "                pd.DataFrame: HbTot or HbDiff DataFrame.\n",
    "            \"\"\"\n",
    "            HbO_df = inter_module_df.iloc[\n",
    "                :, np.r_[0, 1, 2 : len(inter_module_df.columns) : 2]\n",
    "            ]\n",
    "            HbO_data_cols = HbO_df.iloc[:, 2:]\n",
    "            HbR_df = inter_module_df.iloc[\n",
    "                :, np.r_[0, 1, 3 : len(inter_module_df.columns) : 2]\n",
    "            ]\n",
    "            HbR_data_cols = HbR_df.iloc[:, 2:]\n",
    "            cols_dict = {}\n",
    "            for i, col_name in enumerate(HbO_data_cols.columns):\n",
    "                if fmt.lower() == \"hbtot\":\n",
    "                    cols_dict[col_name] = (\n",
    "                        HbO_data_cols.iloc[:, i] + HbR_data_cols.iloc[:, i]\n",
    "                    )\n",
    "                elif fmt.lower() == \"hbdiff\":\n",
    "                    cols_dict[col_name] = (\n",
    "                        HbO_data_cols.iloc[:, i] - HbR_data_cols.iloc[:, i]\n",
    "                    )\n",
    "            df = pd.DataFrame(cols_dict)\n",
    "            df.insert(0, \"Block\", HbO_df[\"Block\"])\n",
    "            df.insert(0, \"Participant\", HbO_df[\"Participant\"])\n",
    "            return df\n",
    "\n",
    "        exp_results = load_results(self.flow_processed_data_dir, exp_name)\n",
    "        session = self.par_behav.get_key_from_value(\n",
    "            self.par_behav.session_dict, exp_name\n",
    "        )\n",
    "        measurement_list_df = self.flow_session_dict[\n",
    "            session\n",
    "        ].create_source_detector_df()\n",
    "        channels = (measurement_list_df[\"measurement_list_index\"] - 1).tolist()\n",
    "        cols_to_select = [\"Participant\", \"Block\"] + [str(chan) for chan in channels]\n",
    "        inter_module_df = exp_results.loc[:, cols_to_select]\n",
    "        if fmt:\n",
    "            if fmt.lower() == \"hbo\":  # HbO\n",
    "                HbO_df = inter_module_df.iloc[\n",
    "                    :, np.r_[0, 1, 2 : len(inter_module_df.columns) : 2]\n",
    "                ]\n",
    "                return HbO_df\n",
    "            elif fmt.lower() == \"hbr\":  # HbR\n",
    "                HbR_df = inter_module_df.iloc[\n",
    "                    :, np.r_[0, 1, 3 : len(inter_module_df.columns) : 2]\n",
    "                ]\n",
    "                return HbR_df\n",
    "            elif fmt.lower() == \"hbtot\":  # HbTot\n",
    "                HbTot_df = _compute_df(fmt)\n",
    "                return HbTot_df\n",
    "            elif fmt.lower() == \"hbdiff\":  # HbDiff\n",
    "                HbDiff_df = _compute_df(fmt)\n",
    "                return HbDiff_df\n",
    "        else:\n",
    "            return inter_module_df\n",
    "\n",
    "    def lowpass_filter(\n",
    "        self, data: list[np.ndarray | pd.DataFrame]\n",
    "    ) -> Union[np.ndarray, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Lowpass filter input data.\n",
    "\n",
    "        Args:\n",
    "            data list([np.ndarray | pd.DataFrame]): Data to filter.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Lowpass filtered data.\n",
    "            -or-\n",
    "            pd.DataFrame: Lowpass filtered data.\n",
    "        \"\"\"\n",
    "        order = 20  # filter order\n",
    "        fs = 1.0  # sampling frequency (Hz)\n",
    "        cutoff = 0.1  # cut-off frequency (Hz)\n",
    "        nyq = 0.5 * fs  # nyquist\n",
    "        taps = firwin(order + 1, cutoff / nyq)\n",
    "\n",
    "        if type(data) == pd.DataFrame:\n",
    "            data_out = data.apply(\n",
    "                lambda x: lfilter(taps, 1.0, x)\n",
    "            )  # apply lowpass filter\n",
    "        else:\n",
    "            data_out = lfilter(taps, 1.0, data)  # apply lowpass filter\n",
    "        return data_out\n",
    "\n",
    "    def plot_flow_session(\n",
    "        self, session: str, channels: list[int | list | tuple], filter_type: str = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Plot Kernel flow session data.\n",
    "\n",
    "        Args:\n",
    "            session (str): Session number.\n",
    "            channels (list[int | list | tuple]): Kernel Flow channels to plot.\n",
    "            filter_type (str, optional): Filter type to apply. Defaults to None.\n",
    "        \"\"\"\n",
    "        flow_session = self.flow_session_dict[session]\n",
    "        sel_flow_data = flow_session.get_data(\"dataframe\", channels)  # TODO\n",
    "        if filter_type == \"lowpass\":\n",
    "            sel_flow_data = self.lowpass_filter(sel_flow_data)\n",
    "        session_time_offset = self.time_offset_dict[session]\n",
    "        time_abs_dt = flow_session.get_time_abs(\"datetime\")\n",
    "        time_abs_dt_offset = time_abs_dt - datetime.timedelta(\n",
    "            seconds=session_time_offset\n",
    "        )\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(15, 6))\n",
    "\n",
    "        data_traces = []\n",
    "        data_labels = []\n",
    "        for channel_num in channels:\n",
    "            flow_data = sel_flow_data.iloc[:, channel_num]\n",
    "            data_type_label = self.flow_session_dict[session].get_data_type_label(\n",
    "                channel_num\n",
    "            )\n",
    "            legend_label = f\"Ch {channel_num} ({data_type_label})\"\n",
    "            if data_type_label == \"HbO\":\n",
    "                color = \"red\"\n",
    "            elif data_type_label == \"HbR\":\n",
    "                color = \"blue\"\n",
    "            (data_trace,) = ax.plot(\n",
    "                time_abs_dt_offset, flow_data, color=color, label=legend_label\n",
    "            )\n",
    "            data_traces.append(data_trace)\n",
    "            data_labels.append(legend_label)\n",
    "\n",
    "        exp_spans = []\n",
    "        for exp_name in self.par_behav.session_dict[session]:\n",
    "            exp_start_dt = self.par_behav.get_start_dt(exp_name)\n",
    "            exp_end_dt = self.par_behav.get_end_dt(exp_name)\n",
    "            ax.axvline(exp_start_dt, linestyle=\"dashed\", color=\"k\", alpha=0.75)\n",
    "            ax.axvline(exp_end_dt, linestyle=\"dashed\", color=\"k\", alpha=0.75)\n",
    "            exp_span = ax.axvspan(\n",
    "                exp_start_dt,\n",
    "                exp_end_dt,\n",
    "                color=self.par_behav.exp_color_dict[exp_name],\n",
    "                alpha=0.4,\n",
    "                label=exp_name,\n",
    "            )\n",
    "            exp_spans.append(exp_span)\n",
    "\n",
    "        data_legend = ax.legend(\n",
    "            handles=data_traces,\n",
    "            bbox_to_anchor=(1.0, 1.0),\n",
    "            facecolor=\"white\",\n",
    "            framealpha=1,\n",
    "            title=\"Kernel Flow Data\",\n",
    "        )\n",
    "        handles, labels = plt.gca().get_legend_handles_labels()\n",
    "        uni_labels = dict(zip(labels, handles))\n",
    "        [uni_labels.pop(data_label) for data_label in data_labels]\n",
    "\n",
    "        stim_legend = ax.legend(\n",
    "            uni_labels.values(),\n",
    "            uni_labels.keys(),\n",
    "            bbox_to_anchor=(1.0, 0.75),\n",
    "            facecolor=\"white\",\n",
    "            framealpha=1,\n",
    "            title=\"Experiment\",\n",
    "        )\n",
    "        ax.add_artist(data_legend)\n",
    "        session_split = session.split(\"_\")\n",
    "        exp_title = session_split[0].capitalize() + \" \" + session_split[1]\n",
    "        ax.set_title(exp_title)\n",
    "        datetime_fmt = mdates.DateFormatter(\"%H:%M:%S\")\n",
    "        ax.xaxis.set_major_formatter(datetime_fmt)\n",
    "        ax.set_xlabel(\"Time\", fontsize=16, color=\"k\")\n",
    "\n",
    "    def plot_flow_exp(\n",
    "        self, exp_name: str, channels: list, filter_type: str = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Plot Kernel Flow experiment data.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "            channels (list): Kernel Flow channels to plot.\n",
    "            filter_type (str, optional): Filter type to apply. Defaults to None.\n",
    "        \"\"\"\n",
    "        flow_exp = self.load_flow_exp(exp_name)\n",
    "        session = self.par_behav.get_key_from_value(\n",
    "            self.par_behav.session_dict, exp_name\n",
    "        )\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(15, 6))\n",
    "\n",
    "        data_traces = []\n",
    "        data_labels = []\n",
    "        for channel_num in channels:\n",
    "            timeseries = flow_exp[\"datetime\"]\n",
    "            flow_data = flow_exp.iloc[:, channel_num + 1]\n",
    "            if filter_type == \"lowpass\":\n",
    "                flow_data = self.lowpass_filter(flow_data)\n",
    "            data_type_label = self.flow_session_dict[session].get_data_type_label(\n",
    "                channel_num\n",
    "            )\n",
    "            # legend_label = f\"Ch {channel_num} ({data_type_label})\"\n",
    "            legend_label = f\"{data_type_label}\"\n",
    "            if data_type_label == \"HbO\":\n",
    "                color = \"red\"\n",
    "            elif data_type_label == \"HbR\":\n",
    "                color = \"blue\"\n",
    "            (data_trace,) = ax.plot(\n",
    "                timeseries, flow_data, color=color, label=legend_label\n",
    "            )\n",
    "            data_traces.append(data_trace)\n",
    "            data_labels.append(legend_label)\n",
    "\n",
    "        exp_start_dt = self.par_behav.get_start_dt(exp_name, self.adj_ts_markers)\n",
    "        exp_end_dt = self.par_behav.get_end_dt(exp_name, self.adj_ts_markers)\n",
    "        ax.axvline(exp_start_dt, linestyle=\"dashed\", color=\"k\", alpha=0.75)\n",
    "        ax.axvline(exp_end_dt, linestyle=\"dashed\", color=\"k\", alpha=0.75)\n",
    "        results_dir = r\"C:\\Users\\zackg\\OneDrive\\Ayaz Lab\\KernelFlow_Analysis\\processed_data\\behavioral\"  # NOTE: temporary\n",
    "        exp_results = load_results(results_dir, exp_name, self.par_num)\n",
    "        exp_title = self.par_behav.format_exp_name(exp_name)\n",
    "\n",
    "        stim_spans = []\n",
    "        for _, row in exp_results.iterrows():\n",
    "            try:\n",
    "                uni_stim_dict = self.par_behav.create_unique_stim_dict(\n",
    "                    exp_results, \"stim\"\n",
    "                )\n",
    "                stim = row[\"stim\"]\n",
    "                legend_label = self.par_behav.format_exp_name(row[\"stim\"])\n",
    "            except KeyError:\n",
    "                uni_stim_dict = self.par_behav.create_unique_stim_dict(\n",
    "                    exp_results, \"block\"\n",
    "                )\n",
    "                stim = row[\"block\"]\n",
    "                legend_label = self.par_behav.format_exp_name(row[\"block\"])\n",
    "            color_index = uni_stim_dict[stim]\n",
    "            stim_start = datetime.datetime.fromtimestamp(row[\"stim_start\"])\n",
    "            try:\n",
    "                stim_end = datetime.datetime.fromtimestamp(row[\"stim_end\"])\n",
    "            except ValueError:\n",
    "                if exp_name == \"go_no_go\":\n",
    "                    stim_time = 0.5  # seconds\n",
    "                stim_end = datetime.datetime.fromtimestamp(\n",
    "                    row[\"stim_start\"] + stim_time\n",
    "                )\n",
    "            stim_span = ax.axvspan(\n",
    "                stim_start,\n",
    "                stim_end,\n",
    "                color=self.plot_color_dict[color_index],\n",
    "                alpha=0.4,\n",
    "                label=legend_label,\n",
    "            )\n",
    "            stim_spans.append(stim_span)\n",
    "\n",
    "        data_legend = ax.legend(\n",
    "            handles=data_traces,\n",
    "            bbox_to_anchor=(1.0, 1.0),\n",
    "            facecolor=\"white\",\n",
    "            framealpha=1,\n",
    "            title=\"fNIRS data\",\n",
    "        )\n",
    "        handles, labels = plt.gca().get_legend_handles_labels()\n",
    "        uni_labels = dict(zip(labels, handles))\n",
    "        [uni_labels.pop(data_label) for data_label in data_labels]\n",
    "\n",
    "        stim_legend = ax.legend(\n",
    "            uni_labels.values(),\n",
    "            uni_labels.keys(),\n",
    "            bbox_to_anchor=(1.0, 0.75),\n",
    "            facecolor=\"white\",\n",
    "            framealpha=1,\n",
    "            title=\"Stimulus\",\n",
    "        )\n",
    "        ax.add_artist(data_legend)\n",
    "        ax.set_title(exp_title)\n",
    "        datetime_fmt = mdates.DateFormatter(\"%H:%M:%S\")\n",
    "        ax.xaxis.set_major_formatter(datetime_fmt)\n",
    "        ax.set_xlabel(\"Time\", fontsize=16, color=\"k\")\n",
    "        ax.set_ylabel(\"Concentration (\\u03bcM)\", fontsize=16, color=\"k\")\n",
    "        # plt.savefig(r\"C:\\Users\\zackg\\Downloads\\output.png\", bbox_inches=\"tight\")\n",
    "\n",
    "\n",
    "def create_flow_results_tables(num_pars: int, inter_module_only=False) -> None:\n",
    "    \"\"\"\n",
    "    Generate a CSV file that contains the Kernel Flow stimulus response data\n",
    "    for all experiments and participants.\n",
    "\n",
    "    Args:\n",
    "        num_pars (int): Number of participants in the study.\n",
    "        inter_module_only (bool): Select only inter module channels. Defaults to False.\n",
    "    \"\"\"\n",
    "    exp_order = [\n",
    "        \"audio_narrative\",\n",
    "        \"go_no_go\",\n",
    "        \"king_devick\",\n",
    "        \"n_back\",\n",
    "        \"resting_state\",\n",
    "        \"tower_of_london\",\n",
    "        \"vSAT\",\n",
    "        \"video_narrative_cmiyc\",\n",
    "        \"video_narrative_sherlock\",\n",
    "    ]\n",
    "    hemo_types = [\"HbO\", \"HbR\", \"HbTot\", \"HbDiff\"]\n",
    "    if inter_module_only:\n",
    "        for hemo_type in hemo_types:\n",
    "            all_exp_results_list = []\n",
    "            for par_num in range(1, num_pars + 1):\n",
    "                if hemo_type == \"HbO\":\n",
    "                    print(f\"Processing participant {par_num} ...\")\n",
    "                par = Participant_Flow(par_num)\n",
    "                exp_results_list = []\n",
    "                for exp_name in exp_order:\n",
    "                    stim_resp_df = par.create_inter_module_exp_results_df(\n",
    "                        exp_name, hemo_type\n",
    "                    )\n",
    "                    exp_results_list.append(stim_resp_df)\n",
    "                all_exp_results_list.append(exp_results_list)\n",
    "\n",
    "            filedir = os.path.join(\n",
    "                par.flow_processed_data_dir, \"inter_module_channels\", hemo_type\n",
    "            )\n",
    "            if not os.path.exists(filedir):\n",
    "                os.makedirs(filedir)\n",
    "\n",
    "            for i, exp_name in enumerate(exp_order):\n",
    "                exp_rows = [\n",
    "                    exp_results_list[i] for exp_results_list in all_exp_results_list\n",
    "                ]\n",
    "                exp_df = pd.concat(exp_rows, axis=0, ignore_index=True)\n",
    "                filepath = os.path.join(filedir, f\"{exp_name}_flow_{hemo_type}.csv\")\n",
    "                exp_df.to_csv(filepath, index=False)\n",
    "    else:\n",
    "        for par_num in range(1, num_pars + 1):\n",
    "            print(f\"Processing participant {par_num} ...\")\n",
    "            par = Participant_Flow(par_num)\n",
    "            exp_results_list = []\n",
    "            for exp_name in exp_order:\n",
    "                stim_resp_df = par.create_exp_stim_response_df(exp_name)\n",
    "                exp_results_list.append(stim_resp_df)\n",
    "            all_exp_results_list.append(exp_results_list)\n",
    "\n",
    "        filedir = os.path.join(par.flow_processed_data_dir, \"all_channels\")\n",
    "        if not os.path.exists(filedir):\n",
    "            os.makedirs(filedir)\n",
    "\n",
    "        for i, exp_name in enumerate(exp_order):\n",
    "            exp_rows = [\n",
    "                exp_results_list[i] for exp_results_list in all_exp_results_list\n",
    "            ]\n",
    "            exp_df = pd.concat(exp_rows, axis=0, ignore_index=True)\n",
    "            filepath = os.path.join(filedir, f\"{exp_name}_flow.csv\")\n",
    "            exp_df.to_csv(filepath, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "participant_02\n"
     ]
    }
   ],
   "source": [
    "# SNIRF file loading\n",
    "par_num = 2\n",
    "par = Participant_Flow(par_num)\n",
    "print(par.par_ID)\n",
    "# par.plot_flow_exp(\"n_back\", [0, 1], \"lowpass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "par.create_exp_stim_response_df(exp_name=\"n_back\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>4282</th>\n",
       "      <th>4283</th>\n",
       "      <th>4284</th>\n",
       "      <th>4285</th>\n",
       "      <th>4286</th>\n",
       "      <th>4287</th>\n",
       "      <th>4288</th>\n",
       "      <th>4289</th>\n",
       "      <th>4290</th>\n",
       "      <th>4291</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.035859</td>\n",
       "      <td>-0.183913</td>\n",
       "      <td>-0.222710</td>\n",
       "      <td>0.068421</td>\n",
       "      <td>0.259035</td>\n",
       "      <td>-0.490852</td>\n",
       "      <td>-0.116578</td>\n",
       "      <td>-0.210628</td>\n",
       "      <td>-0.235571</td>\n",
       "      <td>0.062142</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.236890</td>\n",
       "      <td>4.322673</td>\n",
       "      <td>-0.467021</td>\n",
       "      <td>-1.512111</td>\n",
       "      <td>-0.510897</td>\n",
       "      <td>0.198326</td>\n",
       "      <td>0.849623</td>\n",
       "      <td>-0.961662</td>\n",
       "      <td>-1.419555</td>\n",
       "      <td>1.852511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.163402</td>\n",
       "      <td>-0.009616</td>\n",
       "      <td>0.027015</td>\n",
       "      <td>0.182083</td>\n",
       "      <td>-0.003837</td>\n",
       "      <td>0.220420</td>\n",
       "      <td>0.009822</td>\n",
       "      <td>0.027353</td>\n",
       "      <td>0.136682</td>\n",
       "      <td>0.032946</td>\n",
       "      <td>...</td>\n",
       "      <td>2.851061</td>\n",
       "      <td>-2.042768</td>\n",
       "      <td>-0.080494</td>\n",
       "      <td>1.101642</td>\n",
       "      <td>0.161168</td>\n",
       "      <td>-0.261967</td>\n",
       "      <td>-1.436374</td>\n",
       "      <td>0.796895</td>\n",
       "      <td>1.839652</td>\n",
       "      <td>-1.120675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.028139</td>\n",
       "      <td>-0.075019</td>\n",
       "      <td>0.489617</td>\n",
       "      <td>-0.235282</td>\n",
       "      <td>-0.036795</td>\n",
       "      <td>0.176996</td>\n",
       "      <td>-0.223479</td>\n",
       "      <td>0.172834</td>\n",
       "      <td>-0.090313</td>\n",
       "      <td>0.072719</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.519895</td>\n",
       "      <td>0.868650</td>\n",
       "      <td>-4.704751</td>\n",
       "      <td>6.839298</td>\n",
       "      <td>0.160184</td>\n",
       "      <td>-0.254912</td>\n",
       "      <td>-0.732811</td>\n",
       "      <td>1.508536</td>\n",
       "      <td>-1.698786</td>\n",
       "      <td>1.588498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.330871</td>\n",
       "      <td>0.552910</td>\n",
       "      <td>0.113293</td>\n",
       "      <td>0.156244</td>\n",
       "      <td>-0.251738</td>\n",
       "      <td>0.432710</td>\n",
       "      <td>0.140922</td>\n",
       "      <td>-0.099228</td>\n",
       "      <td>-0.273897</td>\n",
       "      <td>0.198859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.107805</td>\n",
       "      <td>-0.513413</td>\n",
       "      <td>1.900438</td>\n",
       "      <td>-2.483459</td>\n",
       "      <td>0.441102</td>\n",
       "      <td>-0.249576</td>\n",
       "      <td>1.700430</td>\n",
       "      <td>-2.115859</td>\n",
       "      <td>1.217071</td>\n",
       "      <td>-1.916715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.110190</td>\n",
       "      <td>0.066061</td>\n",
       "      <td>-0.158138</td>\n",
       "      <td>0.178048</td>\n",
       "      <td>-0.180603</td>\n",
       "      <td>-0.062049</td>\n",
       "      <td>0.103343</td>\n",
       "      <td>-0.323993</td>\n",
       "      <td>0.012882</td>\n",
       "      <td>0.002667</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.815546</td>\n",
       "      <td>0.305019</td>\n",
       "      <td>-1.213209</td>\n",
       "      <td>2.661165</td>\n",
       "      <td>-1.024936</td>\n",
       "      <td>-0.033073</td>\n",
       "      <td>-1.782965</td>\n",
       "      <td>0.027582</td>\n",
       "      <td>1.070039</td>\n",
       "      <td>-1.586852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  4292 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6     \\\n",
       "25 -0.035859 -0.183913 -0.222710  0.068421  0.259035 -0.490852 -0.116578   \n",
       "26 -0.163402 -0.009616  0.027015  0.182083 -0.003837  0.220420  0.009822   \n",
       "27  0.028139 -0.075019  0.489617 -0.235282 -0.036795  0.176996 -0.223479   \n",
       "28 -0.330871  0.552910  0.113293  0.156244 -0.251738  0.432710  0.140922   \n",
       "29  0.110190  0.066061 -0.158138  0.178048 -0.180603 -0.062049  0.103343   \n",
       "\n",
       "        7         8         9     ...      4282      4283      4284      4285  \\\n",
       "25 -0.210628 -0.235571  0.062142  ... -3.236890  4.322673 -0.467021 -1.512111   \n",
       "26  0.027353  0.136682  0.032946  ...  2.851061 -2.042768 -0.080494  1.101642   \n",
       "27  0.172834 -0.090313  0.072719  ... -1.519895  0.868650 -4.704751  6.839298   \n",
       "28 -0.099228 -0.273897  0.198859  ...  0.107805 -0.513413  1.900438 -2.483459   \n",
       "29 -0.323993  0.012882  0.002667  ... -0.815546  0.305019 -1.213209  2.661165   \n",
       "\n",
       "        4286      4287      4288      4289      4290      4291  \n",
       "25 -0.510897  0.198326  0.849623 -0.961662 -1.419555  1.852511  \n",
       "26  0.161168 -0.261967 -1.436374  0.796895  1.839652 -1.120675  \n",
       "27  0.160184 -0.254912 -0.732811  1.508536 -1.698786  1.588498  \n",
       "28  0.441102 -0.249576  1.700430 -2.115859  1.217071 -1.916715  \n",
       "29 -1.024936 -0.033073 -1.782965  0.027582  1.070039 -1.586852  \n",
       "\n",
       "[5 rows x 4292 columns]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_name = \"n_back\"\n",
    "flow_exp = par.load_flow_exp(exp_name)\n",
    "start_idx = 15\n",
    "end_idx = 75\n",
    "baseline_rows = flow_exp.loc[\n",
    "    start_idx : start_idx + 35, 0:\n",
    "]  # first 5 seconds of a block\n",
    "baseline = baseline_rows.mean()\n",
    "block_rows = flow_exp.loc[\n",
    "    start_idx:end_idx, 0:\n",
    "]  # rows from block start to end\n",
    "block_rows_norm = block_rows - baseline  # normalize the block rows\n",
    "block_rows_norm[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>4282</th>\n",
       "      <th>4283</th>\n",
       "      <th>4284</th>\n",
       "      <th>4285</th>\n",
       "      <th>4286</th>\n",
       "      <th>4287</th>\n",
       "      <th>4288</th>\n",
       "      <th>4289</th>\n",
       "      <th>4290</th>\n",
       "      <th>4291</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-0.035859</td>\n",
       "      <td>-0.183913</td>\n",
       "      <td>-0.222710</td>\n",
       "      <td>0.068421</td>\n",
       "      <td>0.259035</td>\n",
       "      <td>-0.490852</td>\n",
       "      <td>-0.116578</td>\n",
       "      <td>-0.210628</td>\n",
       "      <td>-0.235571</td>\n",
       "      <td>0.062142</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.236890</td>\n",
       "      <td>4.322673</td>\n",
       "      <td>-0.467021</td>\n",
       "      <td>-1.512111</td>\n",
       "      <td>-0.510897</td>\n",
       "      <td>0.198326</td>\n",
       "      <td>0.849623</td>\n",
       "      <td>-0.961662</td>\n",
       "      <td>-1.419555</td>\n",
       "      <td>1.852511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>-0.163402</td>\n",
       "      <td>-0.009616</td>\n",
       "      <td>0.027015</td>\n",
       "      <td>0.182083</td>\n",
       "      <td>-0.003837</td>\n",
       "      <td>0.220420</td>\n",
       "      <td>0.009822</td>\n",
       "      <td>0.027353</td>\n",
       "      <td>0.136682</td>\n",
       "      <td>0.032946</td>\n",
       "      <td>...</td>\n",
       "      <td>2.851061</td>\n",
       "      <td>-2.042768</td>\n",
       "      <td>-0.080494</td>\n",
       "      <td>1.101642</td>\n",
       "      <td>0.161168</td>\n",
       "      <td>-0.261967</td>\n",
       "      <td>-1.436374</td>\n",
       "      <td>0.796895</td>\n",
       "      <td>1.839652</td>\n",
       "      <td>-1.120675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.028139</td>\n",
       "      <td>-0.075019</td>\n",
       "      <td>0.489617</td>\n",
       "      <td>-0.235282</td>\n",
       "      <td>-0.036795</td>\n",
       "      <td>0.176996</td>\n",
       "      <td>-0.223479</td>\n",
       "      <td>0.172834</td>\n",
       "      <td>-0.090313</td>\n",
       "      <td>0.072719</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.519895</td>\n",
       "      <td>0.868650</td>\n",
       "      <td>-4.704751</td>\n",
       "      <td>6.839298</td>\n",
       "      <td>0.160184</td>\n",
       "      <td>-0.254912</td>\n",
       "      <td>-0.732811</td>\n",
       "      <td>1.508536</td>\n",
       "      <td>-1.698786</td>\n",
       "      <td>1.588498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>-0.330871</td>\n",
       "      <td>0.552910</td>\n",
       "      <td>0.113293</td>\n",
       "      <td>0.156244</td>\n",
       "      <td>-0.251738</td>\n",
       "      <td>0.432710</td>\n",
       "      <td>0.140922</td>\n",
       "      <td>-0.099228</td>\n",
       "      <td>-0.273897</td>\n",
       "      <td>0.198859</td>\n",
       "      <td>...</td>\n",
       "      <td>0.107805</td>\n",
       "      <td>-0.513413</td>\n",
       "      <td>1.900438</td>\n",
       "      <td>-2.483459</td>\n",
       "      <td>0.441102</td>\n",
       "      <td>-0.249576</td>\n",
       "      <td>1.700430</td>\n",
       "      <td>-2.115859</td>\n",
       "      <td>1.217071</td>\n",
       "      <td>-1.916715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.110190</td>\n",
       "      <td>0.066061</td>\n",
       "      <td>-0.158138</td>\n",
       "      <td>0.178048</td>\n",
       "      <td>-0.180603</td>\n",
       "      <td>-0.062049</td>\n",
       "      <td>0.103343</td>\n",
       "      <td>-0.323993</td>\n",
       "      <td>0.012882</td>\n",
       "      <td>0.002667</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.815546</td>\n",
       "      <td>0.305019</td>\n",
       "      <td>-1.213209</td>\n",
       "      <td>2.661165</td>\n",
       "      <td>-1.024936</td>\n",
       "      <td>-0.033073</td>\n",
       "      <td>-1.782965</td>\n",
       "      <td>0.027582</td>\n",
       "      <td>1.070039</td>\n",
       "      <td>-1.586852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  4292 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4         5         6     \\\n",
       "25 -0.035859 -0.183913 -0.222710  0.068421  0.259035 -0.490852 -0.116578   \n",
       "26 -0.163402 -0.009616  0.027015  0.182083 -0.003837  0.220420  0.009822   \n",
       "27  0.028139 -0.075019  0.489617 -0.235282 -0.036795  0.176996 -0.223479   \n",
       "28 -0.330871  0.552910  0.113293  0.156244 -0.251738  0.432710  0.140922   \n",
       "29  0.110190  0.066061 -0.158138  0.178048 -0.180603 -0.062049  0.103343   \n",
       "\n",
       "        7         8         9     ...      4282      4283      4284      4285  \\\n",
       "25 -0.210628 -0.235571  0.062142  ... -3.236890  4.322673 -0.467021 -1.512111   \n",
       "26  0.027353  0.136682  0.032946  ...  2.851061 -2.042768 -0.080494  1.101642   \n",
       "27  0.172834 -0.090313  0.072719  ... -1.519895  0.868650 -4.704751  6.839298   \n",
       "28 -0.099228 -0.273897  0.198859  ...  0.107805 -0.513413  1.900438 -2.483459   \n",
       "29 -0.323993  0.012882  0.002667  ... -0.815546  0.305019 -1.213209  2.661165   \n",
       "\n",
       "        4286      4287      4288      4289      4290      4291  \n",
       "25 -0.510897  0.198326  0.849623 -0.961662 -1.419555  1.852511  \n",
       "26  0.161168 -0.261967 -1.436374  0.796895  1.839652 -1.120675  \n",
       "27  0.160184 -0.254912 -0.732811  1.508536 -1.698786  1.588498  \n",
       "28  0.441102 -0.249576  1.700430 -2.115859  1.217071 -1.916715  \n",
       "29 -1.024936 -0.033073 -1.782965  0.027582  1.070039 -1.586852  \n",
       "\n",
       "[5 rows x 4292 columns]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_name = \"n_back\"\n",
    "flow_exp = par.load_flow_exp(exp_name)\n",
    "exp_start_idx = flow_exp.index.start\n",
    "block_start_idx = 15\n",
    "block_end_idx = 75\n",
    "baseline_rows = flow_exp.loc[\n",
    "    block_start_idx : block_start_idx + 35, 0:\n",
    "]  # first 5 seconds of a block\n",
    "baseline = pd.DataFrame(baseline_rows.mean()).T\n",
    "block_rows = flow_exp.loc[\n",
    "    block_start_idx:block_end_idx, 0:\n",
    "]  # rows from block start to end\n",
    "baseline_df = pd.concat([baseline] * block_rows.shape[0], ignore_index=True)\n",
    "baseline_df = baseline_df.set_index(pd.Index(range(exp_start_idx, exp_start_idx+len(baseline_df))))\n",
    "# baseline_df = baseline_df.set_index(pd.Index(range(block_start_idx, block_start_idx+len(baseline_df))))  # NOTE correct way in function\n",
    "block_rows_norm = block_rows.subtract(baseline_df, fill_value=0)  # normalize the block rows\n",
    "block_rows_norm[0:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9c1e24752b3065052c27c07c0a22748a6118d05dd89b50e77c79e7ce74e5970"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
