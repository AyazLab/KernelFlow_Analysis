{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import snirf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# %matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import datetime\n",
    "from typing import Union\n",
    "from statistics import mean\n",
    "from behav_analysis import Participant_Behav, load_results\n",
    "from data_functions import Data_Functions\n",
    "\n",
    "\n",
    "class Process_Flow:\n",
    "    \"\"\"\n",
    "    This class contains functions for processing Kernel Flow data.\n",
    "    Wrapper around an snirf.Snirf object.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filepath: str) -> None:\n",
    "        \"\"\"\n",
    "        Initialize by loading SNIRF file.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to SNIRF file.\n",
    "        \"\"\"\n",
    "        self.data_fun = Data_Functions()\n",
    "        self.snirf_file = self.load_snirf(filepath)\n",
    "\n",
    "    def load_snirf(self, filepath: str) -> snirf.Snirf:\n",
    "        \"\"\"\n",
    "        Load SNIRF file.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            snirf.Snirf: SNIRF file object.\n",
    "        \"\"\"\n",
    "        return snirf.Snirf(filepath, \"r+\", dynamic_loading=True)\n",
    "\n",
    "    def get_time_origin(\n",
    "        self, fmt: str = \"datetime\", offset=True\n",
    "    ) -> Union[datetime.datetime, float]:\n",
    "        \"\"\"\n",
    "        Get the time origin (start time) from the SNIRF file.\n",
    "\n",
    "        Args:\n",
    "            fmt (str, optional): Format to get the time origin in: \"datetime\" or \"timestamp\". Defaults to \"datetime\".\n",
    "            offset (bool): Offset the datetime by 4 hours. Defaults to True.\n",
    "\n",
    "        Raises:\n",
    "            Exception: Invalid fmt argument.\n",
    "\n",
    "        Returns:\n",
    "            Union[datetime.datetime, float]:\n",
    "                datetime.datetime: Time origin datetime.\n",
    "                -or-\n",
    "                float: Time origin timestamp.\n",
    "        \"\"\"\n",
    "        start_date = self.snirf_file.nirs[0].metaDataTags.MeasurementDate\n",
    "        start_time = self.snirf_file.nirs[0].metaDataTags.MeasurementTime\n",
    "        start_str = start_date + \" \" + start_time\n",
    "        if offset:\n",
    "            time_origin = datetime.datetime.strptime(\n",
    "                start_str, \"%Y-%m-%d %H:%M:%S\"\n",
    "            ) - datetime.timedelta(\n",
    "                hours=4\n",
    "            )  # 4 hour offset\n",
    "        else:\n",
    "            time_origin = datetime.datetime.strptime(start_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "        if fmt.lower() == \"datetime\":\n",
    "            return time_origin\n",
    "        elif fmt.lower() == \"timestamp\":\n",
    "            return datetime.datetime.timestamp(time_origin)\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"Invalid 'fmt' argument. Must be 'datetime' or 'timestamp'.\"\n",
    "            )\n",
    "\n",
    "    def get_subject_ID(self) -> str:\n",
    "        \"\"\"\n",
    "        Get the subject ID from the SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            str: Subject ID.\n",
    "        \"\"\"\n",
    "        return self.snirf_file.nirs[0].metaDataTags.SubjectID\n",
    "\n",
    "    def get_time_rel(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get the relative time array from the SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Relative time array.\n",
    "        \"\"\"\n",
    "        return self.snirf_file.nirs[0].data[0].time\n",
    "\n",
    "    def get_time_abs(self, fmt: str = \"datetime\") -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Convert relative time array into an absolute time array.\n",
    "\n",
    "        Args:\n",
    "            fmt (str, optional): Format to get the time array in: \"datetime\" or \"timestamp\". Defaults to \"datetime\".\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Absolute time array.\n",
    "        \"\"\"\n",
    "        time_rel = self.get_time_rel()\n",
    "        if fmt.lower() == \"datetime\":\n",
    "            time_origin_dt = self.get_time_origin(\"datetime\")\n",
    "            return np.array(\n",
    "                [\n",
    "                    datetime.timedelta(seconds=time_rel[i]) + time_origin_dt\n",
    "                    for i in range(len(time_rel))\n",
    "                ]\n",
    "            )\n",
    "        elif fmt.lower() == \"timestamp\":\n",
    "            time_origin_ts = self.get_time_origin(\"timestamp\")\n",
    "            return time_rel + time_origin_ts\n",
    "\n",
    "    def get_data(\n",
    "        self, fmt: str = \"array\", cols: list[int | list | tuple] = None\n",
    "    ) -> Union[np.ndarray, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Get timeseries data from the SNIRF file.\n",
    "\n",
    "        Args:\n",
    "            fmt (str): Format of data (np.ndarray or pd.DataFrame). Defaults to \"array\".\n",
    "            cols (list[int | list | tuple]): Data cols to select. Single col, list of cols, or slice of cols.\n",
    "                                             Defaults to None (all columns).\n",
    "\n",
    "        Raises:\n",
    "            Exception: Invalid fmt argument.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Timeseries data array.\n",
    "        \"\"\"\n",
    "        if cols or cols == 0:\n",
    "            if isinstance(cols, tuple):\n",
    "                data = (\n",
    "                    self.snirf_file.nirs[0].data[0].dataTimeSeries[:, cols[0] : cols[1]]\n",
    "                )\n",
    "            else:\n",
    "                data = self.snirf_file.nirs[0].data[0].dataTimeSeries[:, cols]\n",
    "        else:\n",
    "            data = self.snirf_file.nirs[0].data[0].dataTimeSeries\n",
    "\n",
    "        if \"array\" in fmt.lower():\n",
    "            return data\n",
    "        elif \"dataframe\" in fmt.lower():\n",
    "            return pd.DataFrame(data)\n",
    "        else:\n",
    "            raise Exception(\"Invalid fmt argument. Must be 'array' or 'dataframe'.\")\n",
    "\n",
    "    def get_source_pos_2d(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get the 2D source position array.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: 2D source position array\n",
    "        \"\"\"\n",
    "        return self.snirf_file.nirs[0].probe.sourcePos2D\n",
    "\n",
    "    def get_source_pos_3d(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get the 3D source position array.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: 3D source position array\n",
    "        \"\"\"\n",
    "        return self.snirf_file.nirs[0].probe.sourcePos3D\n",
    "\n",
    "    def get_detector_pos_2d(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get the 2D detector position array.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: 2D detector position array\n",
    "        \"\"\"\n",
    "        return self.snirf_file.nirs[0].probe.detectorPos2D\n",
    "\n",
    "    def get_detector_pos_3d(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get the 3D detector position array.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: 3D detector position array\n",
    "        \"\"\"\n",
    "        return self.snirf_file.nirs[0].probe.detectorPos3D\n",
    "\n",
    "    def get_marker_df(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get a DataFrame of marker data from the \"stim\" part of the SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Marker \"stim\" data.\n",
    "        \"\"\"\n",
    "        marker_data = self.snirf_file.nirs[0].stim[0].data\n",
    "        marker_data_cols = self.snirf_file.nirs[0].stim[0].dataLabels\n",
    "        return pd.DataFrame(marker_data, columns=marker_data_cols)\n",
    "\n",
    "    def get_unique_data_types(self) -> list:\n",
    "        \"\"\"\n",
    "        Get unique data types from the SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            list: Unique data types.\n",
    "        \"\"\"\n",
    "        data_types = []\n",
    "        for i in range(len(self.snirf_file.nirs[0].data[0].measurementList)):\n",
    "            data_type = self.snirf_file.nirs[0].data[0].measurementList[i].dataType\n",
    "            if data_type not in data_types:\n",
    "                data_types.append(data_type)\n",
    "        return data_types\n",
    "\n",
    "    def get_data_type_label(self, channel_num: int) -> str:\n",
    "        \"\"\"\n",
    "        Get the data type label for a channel(s).\n",
    "\n",
    "        Args:\n",
    "            channel_num (int): Channel number to get the data type label of.\n",
    "\n",
    "        Returns:\n",
    "            str: Data type label of the channel.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            self.snirf_file.nirs[0].data[0].measurementList[channel_num].dataTypeLabel\n",
    "        )\n",
    "\n",
    "    def get_unique_data_type_labels(self) -> list:\n",
    "        \"\"\"\n",
    "        Get unique data type labels from the SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            list: Unique data type labels.\n",
    "        \"\"\"\n",
    "        data_type_labels = []\n",
    "        for i in range(len(self.snirf_file.nirs[0].data[0].measurementList)):\n",
    "            data_type_label = (\n",
    "                self.snirf_file.nirs[0].data[0].measurementList[i].dataTypeLabel\n",
    "            )\n",
    "            if data_type_label not in data_type_labels:\n",
    "                data_type_labels.append(data_type_label)\n",
    "        return data_type_labels\n",
    "\n",
    "    def create_source_dict(self) -> dict:\n",
    "        \"\"\"\n",
    "        Count the occurrences of each source index.\n",
    "\n",
    "        Returns:\n",
    "            dict: Counts for each source index.\n",
    "        \"\"\"\n",
    "        source_dict = {}\n",
    "        for i in range(len(self.snirf_file.nirs[0].data[0].measurementList)):\n",
    "            source = self.snirf_file.nirs[0].data[0].measurementList[i].sourceIndex\n",
    "            source_dict[source] = source_dict.get(source, 0) + 1\n",
    "        source_dict = self.data_fun.sort_dict(source_dict, \"keys\")\n",
    "        return source_dict\n",
    "\n",
    "    def create_detector_dict(self) -> dict:\n",
    "        \"\"\"\n",
    "        Count the occurrences of each detector index.\n",
    "\n",
    "        Returns:\n",
    "            dict: Counts for each detector index.\n",
    "        \"\"\"\n",
    "        detector_dict = {}\n",
    "        for i in range(len(self.snirf_file.nirs[0].data[0].measurementList)):\n",
    "            detector = self.snirf_file.nirs[0].data[0].measurementList[i].detectorIndex\n",
    "            detector_dict[detector] = detector_dict.get(detector, 0) + 1\n",
    "        detector_dict = self.data_fun.sort_dict(detector_dict, \"keys\")\n",
    "        return detector_dict\n",
    "\n",
    "    def plot_pos_2d(self) -> None:\n",
    "        \"\"\"\n",
    "        Plot the detector/source 2D positions.\n",
    "        \"\"\"\n",
    "        detector_pos_2d = self.get_detector_pos_2d()\n",
    "        x_detector = detector_pos_2d[:, 0]\n",
    "        y_detector = detector_pos_2d[:, 1]\n",
    "\n",
    "        source_pos_2d = self.get_source_pos_2d()\n",
    "        x_source = source_pos_2d[:, 0]\n",
    "        y_source = source_pos_2d[:, 1]\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.scatter(x_detector, y_detector)\n",
    "        ax.scatter(x_source, y_source)\n",
    "        ax.set_title(\"Detector/Source 2D Plot\")\n",
    "        ax.set_xlabel(\"X-Position (mm)\")\n",
    "        ax.set_ylabel(\"Y-Position (mm)\")\n",
    "        ax.legend([\"Detector\", \"Source\"])\n",
    "\n",
    "    def plot_pos_3d(self) -> None:\n",
    "        \"\"\"\n",
    "        Plot the detector/source 3D positions.\n",
    "        \"\"\"\n",
    "        detector_pos_3d = self.get_detector_pos_3d()\n",
    "        x_detector = detector_pos_3d[:, 0]\n",
    "        y_detector = detector_pos_3d[:, 1]\n",
    "        z_detector = detector_pos_3d[:, 2]\n",
    "\n",
    "        source_pos_3d = self.get_source_pos_3d()\n",
    "        x_source = source_pos_3d[:, 0]\n",
    "        y_source = source_pos_3d[:, 1]\n",
    "        z_source = source_pos_3d[:, 2]\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection=\"3d\")\n",
    "        ax.scatter(x_detector, y_detector, z_detector)\n",
    "        ax.scatter(x_source, y_source, z_source)\n",
    "        ax.set_title(\"Detector/Source 3D Plot\")\n",
    "        ax.set_xlabel(\"X-Position (mm)\")\n",
    "        ax.set_ylabel(\"Y-Position (mm)\")\n",
    "        ax.set_zlabel(\"Z-Position (mm)\")\n",
    "        ax.legend([\"Detector\", \"Source\"])\n",
    "\n",
    "\n",
    "class Participant_Flow:\n",
    "    \"\"\"\n",
    "    This class contains functions, data structures, and info necessary for\n",
    "    processing Kernel Flow data from the experiments.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, par_num):\n",
    "        self.data_fun = Data_Functions()\n",
    "        self.par_behav = Participant_Behav(par_num)\n",
    "        self.par_num, self.par_ID = self.data_fun.process_par(par_num)\n",
    "        data_dir = r\"C:\\Kernel\\participants\"\n",
    "        self.flow_data_dir = os.path.join(data_dir, self.par_ID, \"kernel_data\")\n",
    "        self.plot_color_dict = {\n",
    "            0: \"purple\",\n",
    "            1: \"orange\",\n",
    "            2: \"green\",\n",
    "            3: \"yellow\",\n",
    "            4: \"pink\",\n",
    "            5: \"skyblue\",\n",
    "        }\n",
    "        self.flow_session_dict = self.create_flow_session_dict(wrapper=True)\n",
    "\n",
    "    def load_flow_session(\n",
    "        self, session: list[str | int], wrapper: bool = False\n",
    "    ) -> snirf.Snirf:\n",
    "        \"\"\"\n",
    "        Load Kernel Flow data for an experiment session.\n",
    "\n",
    "        Args:\n",
    "            session list[str | int]: Experiment session.\n",
    "            wrapper (bool, optional) Option to return Process_Flow-wrapped SNIRF file.\n",
    "                                     Defaults to false.\n",
    "\n",
    "        Raises:\n",
    "            Exception: Invalid session number argument.\n",
    "\n",
    "        Returns:\n",
    "            snirf.Snirf: SNIRF file object.\n",
    "            -or-\n",
    "            Process_Flow object for each experiment session.\n",
    "        \"\"\"\n",
    "        if isinstance(session, str):\n",
    "            if \"session\" not in session:\n",
    "                session = f\"session_{session}\"\n",
    "        elif isinstance(session, int):\n",
    "            session = f\"session_{session}\"\n",
    "        try:\n",
    "            session_dir = os.path.join(self.flow_data_dir, session)\n",
    "            filename = os.listdir(session_dir)[0]\n",
    "            filepath = os.path.join(session_dir, filename)\n",
    "            if wrapper:\n",
    "                return Process_Flow(filepath)\n",
    "            else:\n",
    "                return Process_Flow(filepath).snirf_file\n",
    "        except:\n",
    "            print(\"Invalid session number.\")\n",
    "            raise\n",
    "\n",
    "    def load_flow_exp(self, exp_name: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load Kernel Flow data for the time frame of a specified experiment.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Kernel Flow data for an experiment.\n",
    "        \"\"\"\n",
    "\n",
    "        def _offset_time_array(exp_name: str, time_array: np.ndarray) -> np.ndarray:\n",
    "            \"\"\"\n",
    "            Offset a Kernel Flow datetime array for an experiment by the time-offset.\n",
    "\n",
    "            Args:\n",
    "                exp_name (str): Name of the experiment.\n",
    "                time_array (np.ndarray): Datetime array.\n",
    "\n",
    "            Returns:\n",
    "                np.ndarray: Time-offset datetime array.\n",
    "            \"\"\"\n",
    "            try:\n",
    "                time_offset = self.get_time_offset(exp_name)\n",
    "            except KeyError:  # if experiment start time missing, use avg of all other experiments\n",
    "                time_offset_list = []\n",
    "                for exp_name in self.par_behav.exp_order:\n",
    "                    try:\n",
    "                        time_offset = self.get_time_offset(exp_name)\n",
    "                        time_offset_list.append(time_offset)\n",
    "                    except KeyError:\n",
    "                        pass\n",
    "                time_offset = mean(time_offset_list)\n",
    "            time_offset_dt = datetime.timedelta(seconds=time_offset)\n",
    "            time_abs_dt_offset = time_array + time_offset_dt\n",
    "            return time_abs_dt_offset\n",
    "\n",
    "        session = self.par_behav.get_key_from_value(\n",
    "            self.par_behav.session_dict, exp_name\n",
    "        )\n",
    "        flow_session = self.load_flow_session(session, wrapper=True)\n",
    "\n",
    "        start_dt = self.par_behav.get_start_dt(exp_name)\n",
    "        end_dt = self.par_behav.get_end_dt(exp_name)\n",
    "        time_abs_dt = flow_session.get_time_abs(\"datetime\")\n",
    "        time_abs_dt_offset = _offset_time_array(exp_name, time_abs_dt)\n",
    "        start_idx = self.par_behav.get_start_index_dt(time_abs_dt_offset, start_dt)\n",
    "        end_idx = self.par_behav.get_end_index_dt(time_abs_dt_offset, end_dt)\n",
    "\n",
    "        flow_data = flow_session.get_data(\"dataframe\")\n",
    "        flow_data.insert(0, \"datetime\", time_abs_dt_offset)\n",
    "        return flow_data.iloc[start_idx:end_idx, :]\n",
    "\n",
    "    def create_flow_session_dict(self, wrapper: bool = False) -> dict:\n",
    "        \"\"\"\n",
    "        Create a dictionary of Kernel Flow data for all experiment sessions.\n",
    "\n",
    "        wrapper (bool, optional) Option to return Process_Flow-wrapped SNIRF file.\n",
    "                                 Default to false.\n",
    "\n",
    "        Returns:\n",
    "            dict: Kernel Flow data for all experiment sessions.\n",
    "                keys:\n",
    "                    \"session_1001\", \"session_1002\", \"session_1003\"\n",
    "                values:\n",
    "                    SNIRF file object for each experiment session\n",
    "                    -or-\n",
    "                    Process_Flow object for each experiment session\n",
    "        \"\"\"\n",
    "        flow_session_dict = {}\n",
    "        for session in self.par_behav.session_dict.keys():\n",
    "            flow_session_dict[session] = self.load_flow_session(session, wrapper)\n",
    "        return flow_session_dict\n",
    "\n",
    "    def create_abs_marker_df(self, session: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Convert the \"stim\" marker DataFrame into absolute time.\n",
    "\n",
    "        Args:\n",
    "            session (str): Experiment session.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Marker \"stim\" data in absolute time.\n",
    "        \"\"\"\n",
    "        marker_df = self.flow_session_dict[session].get_marker_df()\n",
    "        time_origin_ts = self.flow_session_dict[session].get_time_origin(\"timestamp\")\n",
    "        marker_df[\"Timestamp\"] = marker_df[\"Timestamp\"] + time_origin_ts\n",
    "        marker_df.rename({\"Timestamp\": \"Start timestamp\"}, axis=1, inplace=True)\n",
    "\n",
    "        for idx, row in marker_df.iterrows():\n",
    "            end_ts = row[\"Start timestamp\"] + row[\"Duration\"]\n",
    "            marker_df.at[idx, \"End timestamp\"] = end_ts\n",
    "            exp_num = int(row[\"Experiment\"])\n",
    "            exp_name = self.par_behav.marker_dict[exp_num]\n",
    "            marker_df.at[idx, \"Experiment\"] = exp_name\n",
    "\n",
    "        marker_df.rename({\"Experiment\": \"Marker\"}, axis=1, inplace=True)\n",
    "        marker_df.drop([\"Value\"], axis=1, inplace=True)\n",
    "        marker_df = marker_df[\n",
    "            [\"Marker\", \"Start timestamp\", \"Duration\", \"End timestamp\"]\n",
    "        ]\n",
    "        return marker_df\n",
    "\n",
    "    def get_time_offset(self, exp_name: str) -> float:\n",
    "        \"\"\"\n",
    "        Get the time offset (in seconds) between the behavioral and Kernel Flow data files.\n",
    "        Number of seconds that the Kernel Flow data is ahead of the behavioral data.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "\n",
    "        Returns:\n",
    "            float: Time offset (in seconds).\n",
    "        \"\"\"\n",
    "        exp = self.par_behav.get_exp(exp_name)\n",
    "        exp_start_ts = exp.start_ts\n",
    "        marker_sent_time = float(exp.marker_data[\"start_marker\"][\"sent_time\"])\n",
    "        session = self.par_behav.get_key_from_value(\n",
    "            self.par_behav.session_dict, exp_name\n",
    "        )\n",
    "        marker_df = self.create_abs_marker_df(session)\n",
    "        row = marker_df.loc[marker_df[\"Marker\"].str.startswith(exp_name)].reset_index()\n",
    "        kernel_start_ts = row.loc[0, \"Start timestamp\"]\n",
    "        time_offset = kernel_start_ts - (exp_start_ts + marker_sent_time)\n",
    "        return float(time_offset)\n",
    "\n",
    "    def plot_flow_session(self, session: str) -> None:\n",
    "        # NOTE not time offset\n",
    "        flow_session = self.flow_session_dict[session]\n",
    "        time_abs_dt = flow_session.get_time_abs(\"datetime\")\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(15, 6))\n",
    "        ax.plot(\n",
    "            time_abs_dt, flow_session.get_data(cols=0)\n",
    "        )  # NOTE: get_data argument is a placeholder\n",
    "        for exp_name in self.par_behav.session_dict[session]:\n",
    "            exp_start_dt = self.par_behav.get_start_dt(exp_name)\n",
    "            exp_end_dt = self.par_behav.get_end_dt(exp_name)\n",
    "            ax.axvline(exp_start_dt, linestyle=\"dashed\", color=\"k\", alpha=0.75)\n",
    "            ax.axvline(exp_end_dt, linestyle=\"dashed\", color=\"k\", alpha=0.75)\n",
    "            ax.axvspan(\n",
    "                exp_start_dt,\n",
    "                exp_end_dt,\n",
    "                color=self.par_behav.exp_color_dict[exp_name],\n",
    "                alpha=0.4,\n",
    "                label=exp_name,\n",
    "            )\n",
    "        datetime_fmt = mdates.DateFormatter(\"%H:%M:%S\")\n",
    "        ax.xaxis.set_major_formatter(datetime_fmt)\n",
    "        ax.set_xlabel(\"Time\", fontsize=16, color=\"k\")\n",
    "        ax.legend(bbox_to_anchor=(1.0, 0.75), facecolor=\"white\", framealpha=1)\n",
    "\n",
    "    def plot_flow_exp(self, exp_name: str) -> None:\n",
    "        channel_nums = [0, 1]  # NOTE testing\n",
    "        flow_exp = self.load_flow_exp(exp_name)\n",
    "        session = self.par_behav.get_key_from_value(\n",
    "            self.par_behav.session_dict, exp_name\n",
    "        )\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(15, 6))\n",
    "\n",
    "        data_traces = []\n",
    "        data_labels = []\n",
    "        for channel_num in channel_nums:\n",
    "            data_type_label = self.flow_session_dict[session].get_data_type_label(\n",
    "                channel_num\n",
    "            )\n",
    "            legend_label = f\"Ch {channel_num} ({data_type_label})\"\n",
    "            if data_type_label == \"HbO\":\n",
    "                color = \"red\"\n",
    "            elif data_type_label == \"HbR\":\n",
    "                color = \"blue\"\n",
    "            (data_trace,) = ax.plot(\n",
    "                flow_exp[\"datetime\"],\n",
    "                flow_exp.iloc[:, channel_num + 1],\n",
    "                color=color,\n",
    "                label=legend_label,\n",
    "            )\n",
    "            data_traces.append(data_trace)\n",
    "            data_labels.append(legend_label)\n",
    "\n",
    "        exp_start_dt = self.par_behav.get_start_dt(exp_name)\n",
    "        exp_end_dt = self.par_behav.get_end_dt(exp_name)\n",
    "        ax.axvline(exp_start_dt, linestyle=\"dashed\", color=\"k\", alpha=0.75)\n",
    "        ax.axvline(exp_end_dt, linestyle=\"dashed\", color=\"k\", alpha=0.75)\n",
    "        results_dir = r\"C:\\Users\\zackg\\OneDrive\\Ayaz Lab\\KernelFlow_Analysis\\results\\behavioral\"  # NOTE: temporary\n",
    "        exp_results = load_results(results_dir, exp_name, self.par_num)\n",
    "        exp_title = self.par_behav.format_exp_name(exp_name)\n",
    "\n",
    "        stim_spans = []\n",
    "        for _, row in exp_results.iterrows():\n",
    "            try:\n",
    "                uni_stim_dict = self.par_behav.create_unique_stim_dict(\n",
    "                    exp_results, \"stim\"\n",
    "                )\n",
    "                stim = row[\"stim\"]\n",
    "                legend_label = self.par_behav.format_exp_name(row[\"stim\"])\n",
    "            except KeyError:\n",
    "                uni_stim_dict = self.par_behav.create_unique_stim_dict(\n",
    "                    exp_results, \"block\"\n",
    "                )\n",
    "                stim = row[\"block\"]\n",
    "                legend_label = self.par_behav.format_exp_name(row[\"block\"])\n",
    "            color_index = uni_stim_dict[stim]\n",
    "            stim_start = datetime.datetime.fromtimestamp(row[\"stim_start\"])\n",
    "            try:\n",
    "                stim_end = datetime.datetime.fromtimestamp(row[\"stim_end\"])\n",
    "            except ValueError:\n",
    "                if exp_name == \"go_no_go\":\n",
    "                    stim_time = 0.5  # seconds\n",
    "                stim_end = datetime.datetime.fromtimestamp(\n",
    "                    row[\"stim_start\"] + stim_time\n",
    "                )\n",
    "            stim_span = ax.axvspan(\n",
    "                stim_start,\n",
    "                stim_end,\n",
    "                color=self.plot_color_dict[color_index],\n",
    "                alpha=0.4,\n",
    "                label=legend_label,\n",
    "            )\n",
    "            stim_spans.append(stim_span)\n",
    "\n",
    "        data_legend = ax.legend(\n",
    "            handles=data_traces,\n",
    "            bbox_to_anchor=(1.0, 1.0),\n",
    "            facecolor=\"white\",\n",
    "            framealpha=1,\n",
    "            title=\"Kernel Flow Data\",\n",
    "        )\n",
    "\n",
    "        handles, labels = plt.gca().get_legend_handles_labels()\n",
    "        uni_labels = dict(zip(labels, handles))\n",
    "        [uni_labels.pop(data_label) for data_label in data_labels]\n",
    "\n",
    "        stim_legend = ax.legend(\n",
    "            uni_labels.values(),\n",
    "            uni_labels.keys(),\n",
    "            bbox_to_anchor=(1.0, 0.75),\n",
    "            facecolor=\"white\",\n",
    "            framealpha=1,\n",
    "            title=\"Stimulus\",\n",
    "        )\n",
    "\n",
    "        ax.add_artist(data_legend)\n",
    "        ax.set_title(exp_title)\n",
    "        datetime_fmt = mdates.DateFormatter(\"%H:%M:%S\")\n",
    "        ax.xaxis.set_major_formatter(datetime_fmt)\n",
    "        ax.set_xlabel(\"Time\", fontsize=16, color=\"k\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "participant_01\n"
     ]
    }
   ],
   "source": [
    "# SNIRF file loading\n",
    "par_num = 1\n",
    "par = Participant_Flow(par_num)\n",
    "print(par.par_ID)\n",
    "# exp_name = \"tower_of_london\"\n",
    "# par.plot_flow_exp(exp_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment order: ['resting_state', 'go_no_go', 'video_narrative_sherlock', 'king_devick', 'vSAT', 'audio_narrative', 'n_back', 'tower_of_london', 'video_narrative_cmiyc']\n",
      "\n",
      "Experiment time origin: 2022-06-15 14:43:17.132986\n",
      "Start marker sent time: 11.6911\n",
      "Start marker sent time (absolute): 2022-06-15 14:43:28.824086\n",
      "Flow time origin: 2022-06-15 18:43:22\n",
      "\n",
      "Kernel marker data (original):\n",
      "     Timestamp    Duration  Value  Experiment\n",
      "0    19.105127  441.473551    1.0        51.0\n",
      "1  1120.861606  514.570869    1.0        81.0\n",
      "\n",
      "Kernel marker data (absolute):\n",
      "                           Marker  Start timestamp    Duration  End timestamp\n",
      "0             resting_state_start     1.655319e+09  441.473551   1.655319e+09\n",
      "1  video_narrative_sherlock_start     1.655320e+09  514.570869   1.655320e+09\n",
      "\n",
      "Time offset: 12.281\n"
     ]
    }
   ],
   "source": [
    "print(f\"Experiment order: {par.par_behav.exp_order}\\n\")\n",
    "\n",
    "exp_name = \"resting_state\"\n",
    "exp = par.par_behav.get_exp(exp_name)\n",
    "exp_time_origin_ts = exp.start_ts\n",
    "exp_time_origin_dt = datetime.datetime.fromtimestamp(exp_time_origin_ts)\n",
    "print(f\"Experiment time origin: {exp_time_origin_dt}\")\n",
    "start_marker_sent_time = float(exp.marker_data[\"start_marker\"][\"sent_time\"])\n",
    "print(f\"Start marker sent time: {start_marker_sent_time}\")\n",
    "start_marker_sent_time_abs = exp_time_origin_dt + datetime.timedelta(seconds=start_marker_sent_time)\n",
    "print(f\"Start marker sent time (absolute): {start_marker_sent_time_abs}\")\n",
    "\n",
    "# The behavioral start marker sent time (absolute) and \n",
    "# kernel start marker receive time (absolute) should be identical.\n",
    "\n",
    "flow_time_origin = par.flow_session_dict[\"session_1001\"].get_time_origin(offset=False)\n",
    "print(f\"Flow time origin: {flow_time_origin}\\n\")\n",
    "session = par.par_behav.get_key_from_value(par.par_behav.session_dict, exp_name)\n",
    "marker_df = par.flow_session_dict[session].get_marker_df()\n",
    "print(f\"Kernel marker data (original):\\n{marker_df}\\n\")\n",
    "marker_df_abs = par.create_abs_marker_df(session)\n",
    "print(f\"Kernel marker data (absolute):\\n{marker_df_abs}\")\n",
    "\n",
    "row = marker_df_abs.loc[marker_df_abs[\"Marker\"].str.startswith(exp_name)].reset_index()\n",
    "kernel_start_marker_ts = row.loc[0, \"Start timestamp\"]\n",
    "time_offset = kernel_start_marker_ts - (exp_time_origin_ts + start_marker_sent_time)\n",
    "print(f\"\\nTime offset: {round(time_offset, 3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time difference:\n",
      "------------\n",
      "12.281, resting_state\n",
      "12.285, video_narrative_sherlock\n",
      "11.98, king_devick\n",
      "11.981, vSAT\n",
      "11.983, audio_narrative\n",
      "12.577, n_back\n",
      "12.58, tower_of_london\n",
      "12.582, video_narrative_cmiyc\n"
     ]
    }
   ],
   "source": [
    "# Time offset issue\n",
    "# Kernel Flow PC - Behavioral Task PC\n",
    "par_num = 1\n",
    "par = Participant_Flow(par_num)\n",
    "time_offset_list = []\n",
    "print(\"Time difference:\\n------------\")\n",
    "for exp_name in par.par_behav.exp_order:\n",
    "    try:\n",
    "        time_offset = round(par.get_time_offset(exp_name), 3)\n",
    "        time_offset_list.append(time_offset)\n",
    "        print(f\"{time_offset}, {exp_name}\")\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.signal import firwin, lfilter\n",
    "\n",
    "# Define the filter parameters\n",
    "order = 20\n",
    "fs = 1.0  # Sampling frequency (Hz)\n",
    "cutoff = 0.1  # Cut-off frequency (Hz)\n",
    "\n",
    "# Design the filter coefficients using the firwin function\n",
    "nyq = 0.5 * fs\n",
    "taps = firwin(order + 1, cutoff/nyq)\n",
    "\n",
    "# Generate a signal\n",
    "t = par.flow_session_dict[\"session_1001\"].get_time_abs(\"datetime\")[0:11000]\n",
    "x = par.flow_session_dict[\"session_1001\"].get_data(\"array\", 0)[0:11000]\n",
    "\n",
    "# Apply the filter using the lfilter function\n",
    "y = lfilter(taps, 1.0, x)\n",
    "\n",
    "# Plot the original and filtered signals\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(t, x, 'b-', linewidth=1, alpha=0.5, label='Unfiltered signal')\n",
    "plt.plot(t, y, 'r-', linewidth=2, label='Filtered signal')\n",
    "plt.legend()\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9c1e24752b3065052c27c07c0a22748a6118d05dd89b50e77c79e7ce74e5970"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
