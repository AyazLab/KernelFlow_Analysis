{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import snirf\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# %matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from scipy.signal import firwin, lfilter\n",
    "from typing import Union\n",
    "from statistics import mean\n",
    "from behav_analysis import Participant_Behav\n",
    "from data_functions import Data_Functions, load_results\n",
    "\n",
    "\n",
    "class Process_Flow:\n",
    "    \"\"\"\n",
    "    This class contains functions for processing Kernel Flow data.\n",
    "    Wrapper around an snirf.Snirf object.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filepath: str) -> None:\n",
    "        \"\"\"\n",
    "        Initialize by loading SNIRF file.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to SNIRF file.\n",
    "        \"\"\"\n",
    "        self.data_fun = Data_Functions()\n",
    "        self.snirf_file = self.load_snirf(filepath)\n",
    "\n",
    "    def load_snirf(self, filepath: str) -> snirf.Snirf:\n",
    "        \"\"\"\n",
    "        Load SNIRF file.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            snirf.Snirf: SNIRF file object.\n",
    "        \"\"\"\n",
    "        return snirf.Snirf(filepath, \"r+\", dynamic_loading=True)\n",
    "\n",
    "    def get_time_origin(\n",
    "        self, fmt: str = \"datetime\", offset=True\n",
    "    ) -> Union[datetime.datetime, float]:\n",
    "        \"\"\"\n",
    "        Get the time origin (start time) from the SNIRF file.\n",
    "\n",
    "        Args:\n",
    "            fmt (str, optional): Format to get the time origin in: \"datetime\" or \"timestamp\". Defaults to \"datetime\".\n",
    "            offset (bool): Offset the datetime by 4 hours. Defaults to True.\n",
    "\n",
    "        Raises:\n",
    "            Exception: Invalid fmt argument.\n",
    "\n",
    "        Returns:\n",
    "            Union[datetime.datetime, float]:\n",
    "                datetime.datetime: Time origin datetime.\n",
    "                -or-\n",
    "                float: Time origin timestamp.\n",
    "        \"\"\"\n",
    "        start_date = self.snirf_file.nirs[0].metaDataTags.MeasurementDate\n",
    "        start_time = self.snirf_file.nirs[0].metaDataTags.MeasurementTime\n",
    "        start_str = start_date + \" \" + start_time\n",
    "        if offset:\n",
    "            time_origin = datetime.datetime.strptime(\n",
    "                start_str, \"%Y-%m-%d %H:%M:%S\"\n",
    "            ) - datetime.timedelta(\n",
    "                hours=4\n",
    "            )  # 4 hour offset\n",
    "        else:\n",
    "            time_origin = datetime.datetime.strptime(start_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "        if fmt.lower() == \"datetime\":\n",
    "            return time_origin\n",
    "        elif fmt.lower() == \"timestamp\":\n",
    "            return datetime.datetime.timestamp(time_origin)\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"Invalid 'fmt' argument. Must be 'datetime' or 'timestamp'.\"\n",
    "            )\n",
    "\n",
    "    def get_subject_ID(self) -> str:\n",
    "        \"\"\"\n",
    "        Get the subject ID from the SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            str: Subject ID.\n",
    "        \"\"\"\n",
    "        return self.snirf_file.nirs[0].metaDataTags.SubjectID\n",
    "\n",
    "    def get_time_rel(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get the relative time array from the SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Relative time array.\n",
    "        \"\"\"\n",
    "        return self.snirf_file.nirs[0].data[0].time\n",
    "\n",
    "    def get_time_abs(self, fmt: str = \"datetime\") -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Convert relative time array into an absolute time array.\n",
    "\n",
    "        Args:\n",
    "            fmt (str, optional): Format to get the time array in: \"datetime\" or \"timestamp\". Defaults to \"datetime\".\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Absolute time array.\n",
    "        \"\"\"\n",
    "        time_rel = self.get_time_rel()\n",
    "        if fmt.lower() == \"datetime\":\n",
    "            time_origin_dt = self.get_time_origin(\"datetime\")\n",
    "            return np.array(\n",
    "                [\n",
    "                    datetime.timedelta(seconds=time_rel[i]) + time_origin_dt\n",
    "                    for i in range(len(time_rel))\n",
    "                ]\n",
    "            )\n",
    "        elif fmt.lower() == \"timestamp\":\n",
    "            time_origin_ts = self.get_time_origin(\"timestamp\")\n",
    "            return time_rel + time_origin_ts\n",
    "\n",
    "    def get_data(\n",
    "        self, fmt: str = \"array\", cols: list[int | list | tuple] = None\n",
    "    ) -> Union[np.ndarray, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Get timeseries data from the SNIRF file.\n",
    "\n",
    "        Args:\n",
    "            fmt (str): Format of data (np.ndarray or pd.DataFrame). Defaults to \"array\".\n",
    "            cols (list[int | list | tuple]): Data cols to select. Single col, list of cols, or slice of cols.\n",
    "                                             Defaults to None (all columns).\n",
    "\n",
    "        Raises:\n",
    "            Exception: Invalid fmt argument.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Timeseries data array.\n",
    "        \"\"\"\n",
    "        if cols or cols == 0:\n",
    "            if isinstance(cols, tuple):\n",
    "                data = (\n",
    "                    self.snirf_file.nirs[0].data[0].dataTimeSeries[:, cols[0] : cols[1]]\n",
    "                )\n",
    "            else:\n",
    "                data = self.snirf_file.nirs[0].data[0].dataTimeSeries[:, cols]\n",
    "        else:\n",
    "            data = self.snirf_file.nirs[0].data[0].dataTimeSeries\n",
    "\n",
    "        if \"array\" in fmt.lower():\n",
    "            return data\n",
    "        elif \"dataframe\" in fmt.lower():\n",
    "            return pd.DataFrame(data)\n",
    "        else:\n",
    "            raise Exception(\"Invalid fmt argument. Must be 'array' or 'dataframe'.\")\n",
    "\n",
    "    def get_source_pos_2d(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get the 2D source position array.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: 2D source position array.\n",
    "        \"\"\"\n",
    "        return self.snirf_file.nirs[0].probe.sourcePos2D\n",
    "\n",
    "    def get_source_pos_3d(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get the 3D source position array.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: 3D source position array.\n",
    "        \"\"\"\n",
    "        return self.snirf_file.nirs[0].probe.sourcePos3D\n",
    "\n",
    "    def get_detector_pos_2d(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get the 2D detector position array.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: 2D detector position array.\n",
    "        \"\"\"\n",
    "        return self.snirf_file.nirs[0].probe.detectorPos2D\n",
    "\n",
    "    def get_detector_pos_3d(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get the 3D detector position array.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: 3D detector position array.\n",
    "        \"\"\"\n",
    "        return self.snirf_file.nirs[0].probe.detectorPos3D\n",
    "\n",
    "    def get_measurement_list(self) -> np.array:\n",
    "        \"\"\"\n",
    "        Get the data measurement list.\n",
    "\n",
    "        Returns:\n",
    "            np.array: Data measurement list array.\n",
    "        \"\"\"\n",
    "        return self.snirf_file.nirs[0].data[0].measurementList\n",
    "\n",
    "    def get_source_labels(self) -> np.array:\n",
    "        \"\"\"\n",
    "        Get the source labels.\n",
    "\n",
    "        Returns:\n",
    "            np.array: Source label array.\n",
    "        \"\"\"\n",
    "        return self.snirf_file.nirs[0].probe.sourceLabels\n",
    "\n",
    "    def get_detector_labels(self) -> np.array:\n",
    "        \"\"\"\n",
    "        Get the detector labels.\n",
    "\n",
    "        Returns:\n",
    "            np.array: Detector label array.\n",
    "        \"\"\"\n",
    "        return self.snirf_file.nirs[0].probe.detectorLabels\n",
    "\n",
    "    def get_marker_df(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get a DataFrame of marker data from the \"stim\" part of the SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Marker \"stim\" data.\n",
    "        \"\"\"\n",
    "        marker_data = self.snirf_file.nirs[0].stim[0].data\n",
    "        marker_data_cols = self.snirf_file.nirs[0].stim[0].dataLabels\n",
    "        return pd.DataFrame(marker_data, columns=marker_data_cols)\n",
    "\n",
    "    def get_unique_data_types(self) -> list:\n",
    "        \"\"\"\n",
    "        Get unique data types from the SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            list: Unique data types.\n",
    "        \"\"\"\n",
    "        data_types = []\n",
    "        for i in range(len(self.snirf_file.nirs[0].data[0].measurementList)):\n",
    "            data_type = self.snirf_file.nirs[0].data[0].measurementList[i].dataType\n",
    "            if data_type not in data_types:\n",
    "                data_types.append(data_type)\n",
    "        return data_types\n",
    "\n",
    "    def get_data_type_label(self, channel_num: int) -> str:\n",
    "        \"\"\"\n",
    "        Get the data type label for a channel(s).\n",
    "\n",
    "        Args:\n",
    "            channel_num (int): Channel number to get the data type label of.\n",
    "\n",
    "        Returns:\n",
    "            str: Data type label of the channel.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            self.snirf_file.nirs[0].data[0].measurementList[channel_num].dataTypeLabel\n",
    "        )\n",
    "\n",
    "    def get_unique_data_type_labels(self) -> list:\n",
    "        \"\"\"\n",
    "        Get unique data type labels from the SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            list: Unique data type labels.\n",
    "        \"\"\"\n",
    "        data_type_labels = []\n",
    "        for i in range(len(self.snirf_file.nirs[0].data[0].measurementList)):\n",
    "            data_type_label = (\n",
    "                self.snirf_file.nirs[0].data[0].measurementList[i].dataTypeLabel\n",
    "            )\n",
    "            if data_type_label not in data_type_labels:\n",
    "                data_type_labels.append(data_type_label)\n",
    "        return data_type_labels\n",
    "\n",
    "    def create_source_dict(self) -> dict:\n",
    "        \"\"\"\n",
    "        Count the occurrences of each source index.\n",
    "\n",
    "        Returns:\n",
    "            dict: Counts for each source index.\n",
    "        \"\"\"\n",
    "        source_dict = {}\n",
    "        for i in range(len(self.snirf_file.nirs[0].data[0].measurementList)):\n",
    "            source = self.snirf_file.nirs[0].data[0].measurementList[i].sourceIndex\n",
    "            source_dict[source] = source_dict.get(source, 0) + 1\n",
    "        source_dict = self.data_fun.sort_dict(source_dict, \"keys\")\n",
    "        return source_dict\n",
    "\n",
    "    def create_detector_dict(self) -> dict:\n",
    "        \"\"\"\n",
    "        Count the occurrences of each detector index.\n",
    "\n",
    "        Returns:\n",
    "            dict: Counts for each detector index.\n",
    "        \"\"\"\n",
    "        detector_dict = {}\n",
    "        for i in range(len(self.snirf_file.nirs[0].data[0].measurementList)):\n",
    "            detector = self.snirf_file.nirs[0].data[0].measurementList[i].detectorIndex\n",
    "            detector_dict[detector] = detector_dict.get(detector, 0) + 1\n",
    "        detector_dict = self.data_fun.sort_dict(detector_dict, \"keys\")\n",
    "        return detector_dict\n",
    "\n",
    "    def create_measurement_list_df(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create a DataFrame with all the data measurement list information.\n",
    "\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Data measurement list DataFrame.\n",
    "        \"\"\"\n",
    "        measurement_list = self.get_measurement_list()\n",
    "        dict_list = []\n",
    "\n",
    "        for i in range(len(measurement_list)):\n",
    "            measurement_list_i = measurement_list[i]\n",
    "            measurement_dict = {}\n",
    "            measurement_dict[\"measurement_list_index\"] = i + 1\n",
    "            measurement_dict[\"data_type\"] = measurement_list_i.dataType\n",
    "            measurement_dict[\"data_type_index\"] = measurement_list_i.dataTypeLabel\n",
    "            measurement_dict[\"detector_index\"] = measurement_list_i.detectorIndex\n",
    "            measurement_dict[\"source_index\"] = measurement_list_i.sourceIndex\n",
    "            dict_list.append(measurement_dict)\n",
    "\n",
    "        measurement_list_df = pd.DataFrame(dict_list)\n",
    "        return measurement_list_df\n",
    "\n",
    "    def create_source_df(self, dim: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create a DataFrame with the source labels and 2D or 3D source positions.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Source labels and positions.\n",
    "        \"\"\"\n",
    "        source_labels = self.get_source_labels()\n",
    "        if dim.lower() == \"2d\":\n",
    "            source_pos_2d = self.get_source_pos_2d()\n",
    "            source_data = [\n",
    "                (label, *lst) for label, lst in zip(source_labels, source_pos_2d)\n",
    "            ]\n",
    "            source_df = pd.DataFrame(\n",
    "                source_data, columns=[\"source_label\", \"source_x_pos\", \"source_y_pos\"]\n",
    "            )\n",
    "        elif dim.lower() == \"3d\":\n",
    "            source_pos_3d = self.get_source_pos_3d()\n",
    "            source_data = [\n",
    "                (label, *lst) for label, lst in zip(source_labels, source_pos_3d)\n",
    "            ]\n",
    "            source_df = pd.DataFrame(\n",
    "                source_data,\n",
    "                columns=[\n",
    "                    \"source_label\",\n",
    "                    \"source_x_pos\",\n",
    "                    \"source_y_pos\",\n",
    "                    \"source_pos_z\",\n",
    "                ],\n",
    "            )\n",
    "        f = lambda x: int(x.lstrip(\"S\"))\n",
    "        source_df.insert(1, \"source_index\", source_df[\"source_label\"].apply(f))\n",
    "        return source_df\n",
    "\n",
    "    def create_detector_df(self, dim: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create a DataFrame with the detector labels and 2D or 3D detector positions.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Detector labels and positions.\n",
    "        \"\"\"\n",
    "        detector_labels = self.get_detector_labels()\n",
    "        if dim.lower() == \"2d\":\n",
    "            detector_pos_2d = self.get_detector_pos_2d()\n",
    "            detector_data = [\n",
    "                (label, *lst) for label, lst in zip(detector_labels, detector_pos_2d)\n",
    "            ]\n",
    "            detector_df = pd.DataFrame(\n",
    "                detector_data,\n",
    "                columns=[\"detector_label\", \"detector_x_pos\", \"detector_y_pos\"],\n",
    "            )\n",
    "        elif dim.lower() == \"3d\":\n",
    "            detector_pos_3d = self.get_detector_pos_3d()\n",
    "            detector_data = [\n",
    "                (label, *lst) for label, lst in zip(detector_labels, detector_pos_3d)\n",
    "            ]\n",
    "            detector_df = pd.DataFrame(\n",
    "                detector_data,\n",
    "                columns=[\n",
    "                    \"detector_label\",\n",
    "                    \"detector_x_pos\",\n",
    "                    \"detector_y_pos\",\n",
    "                    \"detector_pos_z\",\n",
    "                ],\n",
    "            )\n",
    "        f = lambda x: int(x[1:3])\n",
    "        detector_df.insert(1, \"source_index\", detector_df[\"detector_label\"].apply(f))\n",
    "        detector_df.insert(1, \"detector_index\", range(1, detector_df.shape[0] + 1))\n",
    "        return detector_df\n",
    "\n",
    "    def create_source_detector_df(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create a DataFrame with the source and detector information for the inter-module channels.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Source and detector information for inter-module channels.\n",
    "        \"\"\"\n",
    "        measurement_list_df = self.create_measurement_list_df()\n",
    "        source_df = self.create_source_df(\"3D\")\n",
    "        detector_df = self.create_detector_df(\"3D\")\n",
    "        source_merge = pd.merge(measurement_list_df, source_df, on=\"source_index\")\n",
    "        merged_source_detector_df = pd.merge(\n",
    "            source_merge, detector_df, on=[\"detector_index\", \"source_index\"]\n",
    "        )\n",
    "        return merged_source_detector_df\n",
    "\n",
    "    def plot_pos_2d(self) -> None:\n",
    "        \"\"\"\n",
    "        Plot the detector/source 2D positions.\n",
    "        \"\"\"\n",
    "        detector_pos_2d = self.get_detector_pos_2d()\n",
    "        x_detector = detector_pos_2d[:, 0]\n",
    "        y_detector = detector_pos_2d[:, 1]\n",
    "\n",
    "        source_pos_2d = self.get_source_pos_2d()\n",
    "        x_source = source_pos_2d[:, 0]\n",
    "        y_source = source_pos_2d[:, 1]\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.scatter(x_detector, y_detector)\n",
    "        ax.scatter(x_source, y_source)\n",
    "        ax.set_title(\"Detector/Source 2D Plot\")\n",
    "        ax.set_xlabel(\"X-Position (mm)\")\n",
    "        ax.set_ylabel(\"Y-Position (mm)\")\n",
    "        ax.legend([\"Detector\", \"Source\"])\n",
    "\n",
    "    def plot_pos_3d(self) -> None:\n",
    "        \"\"\"\n",
    "        Plot the detector/source 3D positions.\n",
    "        \"\"\"\n",
    "        detector_pos_3d = self.get_detector_pos_3d()\n",
    "        x_detector = detector_pos_3d[:, 0]\n",
    "        y_detector = detector_pos_3d[:, 1]\n",
    "        z_detector = detector_pos_3d[:, 2]\n",
    "\n",
    "        source_pos_3d = self.get_source_pos_3d()\n",
    "        x_source = source_pos_3d[:, 0]\n",
    "        y_source = source_pos_3d[:, 1]\n",
    "        z_source = source_pos_3d[:, 2]\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection=\"3d\")\n",
    "        ax.scatter(x_detector, y_detector, z_detector)\n",
    "        ax.scatter(x_source, y_source, z_source)\n",
    "        ax.set_title(\"Detector/Source 3D Plot\")\n",
    "        ax.set_xlabel(\"X-Position (mm)\")\n",
    "        ax.set_ylabel(\"Y-Position (mm)\")\n",
    "        ax.set_zlabel(\"Z-Position (mm)\")\n",
    "        ax.legend([\"Detector\", \"Source\"])\n",
    "\n",
    "\n",
    "class Participant_Flow:\n",
    "    \"\"\"\n",
    "    This class contains functions, data structures, and info necessary for\n",
    "    processing Kernel Flow data from the experiments.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, par_num):\n",
    "        self.data_fun = Data_Functions()\n",
    "        self.adj_ts_markers = True\n",
    "        self.par_behav = Participant_Behav(par_num, self.adj_ts_markers)\n",
    "        self.par_num, self.par_ID = self.data_fun.process_par(par_num)\n",
    "        self.flow_raw_data_dir = os.path.join(\n",
    "            self.par_behav.raw_data_dir, self.par_ID, \"kernel_data\"\n",
    "        )\n",
    "        self.flow_processed_data_dir = os.path.join(\n",
    "            os.getcwd(), \"processed_data\", \"flow\"\n",
    "        )\n",
    "        self.flow_session_dict = self.create_flow_session_dict(wrapper=True)\n",
    "        self.time_offset_dict = self.create_time_offset_dict()\n",
    "        self.plot_color_dict = {\n",
    "            0: \"purple\",\n",
    "            1: \"orange\",\n",
    "            2: \"green\",\n",
    "            3: \"yellow\",\n",
    "            4: \"pink\",\n",
    "            5: \"skyblue\",\n",
    "        }\n",
    "\n",
    "    def calc_time_offset(self, exp_name: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the time offset (in seconds) between the behavioral and Kernel Flow data\n",
    "        files. Number of seconds that the Kernel Flow data is ahead of the behavioral data.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "\n",
    "        Returns:\n",
    "            float: Time offset (in seconds).\n",
    "        \"\"\"\n",
    "        exp = self.par_behav.get_exp(exp_name)\n",
    "        exp_start_ts = exp.start_ts\n",
    "        marker_sent_time = float(exp.marker_data[\"start_marker\"][\"sent_time\"])\n",
    "        session = self.par_behav.get_key_from_value(\n",
    "            self.par_behav.session_dict, exp_name\n",
    "        )\n",
    "        marker_df = self.create_abs_marker_df(session)\n",
    "        row = marker_df.loc[marker_df[\"Marker\"].str.startswith(exp_name)].reset_index()\n",
    "        if (\n",
    "            exp_name == \"go_no_go\"\n",
    "        ):  # Go/No-go experiment is missing start timestamp marker\n",
    "            try:\n",
    "                kernel_start_ts = row.loc[0, \"Start timestamp\"]\n",
    "                time_offset = kernel_start_ts - (exp_start_ts + marker_sent_time)\n",
    "            except:\n",
    "                time_offset = \"NaN\"\n",
    "        else:\n",
    "            kernel_start_ts = row.loc[0, \"Start timestamp\"]\n",
    "            time_offset = kernel_start_ts - (exp_start_ts + marker_sent_time)\n",
    "        return float(time_offset)\n",
    "\n",
    "    def create_time_offset_dict(self) -> dict:\n",
    "        \"\"\"\n",
    "        Create a dictionary containing the time offset (in seconds) for each experiment.\n",
    "\n",
    "        Returns:\n",
    "            dict: Time offset dictionary.\n",
    "        \"\"\"\n",
    "        time_offset_dict = {}\n",
    "        for exp_name in self.par_behav.exp_order:\n",
    "            if (\n",
    "                exp_name == \"go_no_go\"\n",
    "            ):  # Go/No-go experiment is missing start timestamp marker\n",
    "                if np.isnan(self.calc_time_offset(exp_name)):\n",
    "                    session = self.par_behav.get_key_from_value(\n",
    "                        self.par_behav.session_dict, exp_name\n",
    "                    )\n",
    "                    session_exp_names = self.par_behav.session_dict[session]\n",
    "                    other_exp_names = [\n",
    "                        temp_exp_name\n",
    "                        for temp_exp_name in session_exp_names\n",
    "                        if temp_exp_name != \"go_no_go\"\n",
    "                    ]\n",
    "                    other_exp_time_offsets = []\n",
    "                    for temp_exp_name in other_exp_names:\n",
    "                        time_offset = self.calc_time_offset(temp_exp_name)\n",
    "                        other_exp_time_offsets.append(time_offset)\n",
    "                    avg_time_offset = np.mean(other_exp_time_offsets)\n",
    "                    time_offset_dict[exp_name] = avg_time_offset\n",
    "            else:\n",
    "                time_offset_dict[exp_name] = self.calc_time_offset(exp_name)\n",
    "        for session, exp_list in self.par_behav.session_dict.items():\n",
    "            session_offset = np.mean(\n",
    "                [time_offset_dict[exp_name] for exp_name in exp_list]\n",
    "            )\n",
    "            time_offset_dict[session] = session_offset\n",
    "        return time_offset_dict\n",
    "\n",
    "    def get_time_offset(self, exp_name: str) -> float:\n",
    "        \"\"\"\n",
    "        Get the time offset for an experiment.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Experiment name.\n",
    "\n",
    "        Returns:\n",
    "            float: Time offset (in seconds).\n",
    "        \"\"\"\n",
    "        return self.time_offset_dict[exp_name]\n",
    "\n",
    "    def offset_time_array(self, exp_name: str, time_array: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Offset a Kernel Flow datetime array for an experiment by the time-offset.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "            time_array (np.ndarray): Datetime array.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Time-offset datetime array.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            time_offset = self.get_time_offset(exp_name)\n",
    "        except KeyError:  # if experiment start time is missing, use avg of other session experiments\n",
    "            time_offset_list = []\n",
    "            for exp_name in self.par_behav.exp_order:\n",
    "                try:\n",
    "                    time_offset = self.get_time_offset(exp_name)\n",
    "                    time_offset_list.append(time_offset)\n",
    "                except KeyError:\n",
    "                    pass\n",
    "            time_offset = mean(time_offset_list)\n",
    "        time_offset_dt = datetime.timedelta(seconds=time_offset)\n",
    "        time_abs_dt_offset = time_array - time_offset_dt\n",
    "        return time_abs_dt_offset\n",
    "\n",
    "    def load_flow_session(\n",
    "        self, session: list[str | int], wrapper: bool = False\n",
    "    ) -> snirf.Snirf:\n",
    "        \"\"\"\n",
    "        Load Kernel Flow data for an experiment session.\n",
    "\n",
    "        Args:\n",
    "            session list[str | int]: Experiment session.\n",
    "            wrapper (bool, optional) Option to return Process_Flow-wrapped SNIRF file.\n",
    "                                     Defaults to false.\n",
    "\n",
    "        Raises:\n",
    "            Exception: Invalid session number argument.\n",
    "\n",
    "        Returns:\n",
    "            snirf.Snirf: SNIRF file object.\n",
    "            -or-\n",
    "            Process_Flow object for each experiment session.\n",
    "        \"\"\"\n",
    "        if isinstance(session, str):\n",
    "            if \"session\" not in session:\n",
    "                session = f\"session_{session}\"\n",
    "        elif isinstance(session, int):\n",
    "            session = f\"session_{session}\"\n",
    "        try:\n",
    "            session_dir = os.path.join(self.flow_raw_data_dir, session)\n",
    "            filename = os.listdir(session_dir)[0]\n",
    "            filepath = os.path.join(session_dir, filename)\n",
    "            if wrapper:\n",
    "                return Process_Flow(filepath)\n",
    "            else:\n",
    "                return Process_Flow(filepath).snirf_file\n",
    "        except:\n",
    "            print(\"Invalid session number.\")\n",
    "            raise\n",
    "\n",
    "    def load_flow_exp(self, exp_name: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load Kernel Flow data for the time frame of a specified experiment.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Kernel Flow data for an experiment.\n",
    "        \"\"\"\n",
    "        session = self.par_behav.get_key_from_value(\n",
    "            self.par_behav.session_dict, exp_name\n",
    "        )\n",
    "        flow_session = self.load_flow_session(session, wrapper=True)\n",
    "\n",
    "        start_dt = self.par_behav.get_start_dt(exp_name, self.adj_ts_markers)\n",
    "        end_dt = self.par_behav.get_end_dt(exp_name, self.adj_ts_markers)\n",
    "        time_abs_dt = flow_session.get_time_abs(\"datetime\")\n",
    "        time_abs_dt_offset = self.offset_time_array(exp_name, time_abs_dt)\n",
    "        start_idx = self.par_behav.get_start_index_dt(time_abs_dt_offset, start_dt)\n",
    "        end_idx = self.par_behav.get_end_index_dt(time_abs_dt_offset, end_dt)\n",
    "\n",
    "        flow_data = flow_session.get_data(\"dataframe\")\n",
    "        flow_data.insert(0, \"datetime\", time_abs_dt_offset)\n",
    "        return flow_data.iloc[start_idx:end_idx, :]\n",
    "\n",
    "    def create_flow_session_dict(self, wrapper: bool = False) -> dict:\n",
    "        \"\"\"\n",
    "        Create a dictionary of Kernel Flow data for all experiment sessions.\n",
    "\n",
    "        wrapper (bool, optional) Option to return Process_Flow-wrapped SNIRF file.\n",
    "                                 Default to false.\n",
    "\n",
    "        Returns:\n",
    "            dict: Kernel Flow data for all experiment sessions.\n",
    "                keys:\n",
    "                    \"session_1001\", \"session_1002\", \"session_1003\"\n",
    "                values:\n",
    "                    SNIRF file object for each experiment session\n",
    "                    -or-\n",
    "                    Process_Flow object for each experiment session\n",
    "        \"\"\"\n",
    "        flow_session_dict = {}\n",
    "        for session in self.par_behav.session_dict.keys():\n",
    "            flow_session_dict[session] = self.load_flow_session(session, wrapper)\n",
    "        return flow_session_dict\n",
    "\n",
    "    def create_abs_marker_df(self, session: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Convert the \"stim\" marker DataFrame into absolute time.\n",
    "\n",
    "        Args:\n",
    "            session (str): Experiment session.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Marker \"stim\" data in absolute time.\n",
    "        \"\"\"\n",
    "        marker_df = self.flow_session_dict[session].get_marker_df()\n",
    "        time_origin_ts = self.flow_session_dict[session].get_time_origin(\"timestamp\")\n",
    "        marker_df[\"Timestamp\"] = marker_df[\"Timestamp\"] + time_origin_ts\n",
    "        marker_df.rename({\"Timestamp\": \"Start timestamp\"}, axis=1, inplace=True)\n",
    "\n",
    "        for idx, row in marker_df.iterrows():\n",
    "            end_ts = row[\"Start timestamp\"] + row[\"Duration\"]\n",
    "            marker_df.at[idx, \"End timestamp\"] = end_ts\n",
    "            exp_num = int(row[\"Experiment\"])\n",
    "            exp_name = self.par_behav.marker_dict[exp_num]\n",
    "            marker_df.at[idx, \"Experiment\"] = exp_name\n",
    "\n",
    "        marker_df.rename({\"Experiment\": \"Marker\"}, axis=1, inplace=True)\n",
    "        marker_df.drop([\"Value\"], axis=1, inplace=True)\n",
    "        marker_df = marker_df[\n",
    "            [\"Marker\", \"Start timestamp\", \"Duration\", \"End timestamp\"]\n",
    "        ]\n",
    "        return marker_df\n",
    "\n",
    "    def create_exp_stim_response_dict(self, exp_name: str) -> dict:\n",
    "        \"\"\"\n",
    "        Create a dictionary that contains the processed Kernel Flow data in response\n",
    "        to a stimulus. It is organized by block (keys) and for each block, the value is\n",
    "        a list of Pandas series.  Each series is normalized, averaged, Kernel Flow data\n",
    "        during a presented stimulus duration for each channel.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "\n",
    "        Returns:\n",
    "            dict:\n",
    "                keys:\n",
    "                    \"block 1\", \"block 2\", ... \"block N\"\n",
    "                values:\n",
    "                    dicts:\n",
    "                        keys:\n",
    "                            \"trial 1\", \"trial 2\", ... \"trial N\"\n",
    "                        values:\n",
    "                            lists of averaged, normalized Kernel Flow data series for each\n",
    "                            channel during the stimulus duration\n",
    "        \"\"\"\n",
    "        exp_results = load_results(\n",
    "            self.par_behav.processed_data_dir, exp_name, self.par_behav.par_num\n",
    "        )\n",
    "        flow_exp = self.load_flow_exp(exp_name)\n",
    "        session = self.par_behav.get_key_from_value(\n",
    "            self.par_behav.session_dict, exp_name\n",
    "        )\n",
    "        ts_list = self.flow_session_dict[session].get_time_abs(\"timestamp\")\n",
    "        exp_time_offset = self.time_offset_dict[exp_name]\n",
    "        blocks = list(exp_results[\"block\"].unique())\n",
    "        exp_stim_resp_dict = {\n",
    "            block: {} for block in blocks\n",
    "        }  # initialize with unique blocks\n",
    "        for _, row in exp_results.iterrows():\n",
    "            stim_start_ts = row[\"stim_start\"]\n",
    "            stim_start_ts_offset = stim_start_ts + exp_time_offset\n",
    "            start_idx, _ = self.data_fun.find_closest_ts(stim_start_ts_offset, ts_list)\n",
    "            stim_end_ts = row[\"stim_end\"]\n",
    "            stim_end_ts_offset = stim_end_ts + exp_time_offset\n",
    "            end_idx, _ = self.data_fun.find_closest_ts(stim_end_ts_offset, ts_list)\n",
    "\n",
    "            baseline_row = flow_exp.loc[start_idx, 0:]\n",
    "            stim_rows = flow_exp.loc[start_idx:end_idx, 0:]\n",
    "            avg_norm_rows = (stim_rows - baseline_row).mean()  # all channels for a stim\n",
    "\n",
    "            block = row[\"block\"]\n",
    "            trial = row[\"trial\"]\n",
    "            if trial not in exp_stim_resp_dict[block].keys():\n",
    "                exp_stim_resp_dict[block][trial] = []\n",
    "            exp_stim_resp_dict[block][trial].append(\n",
    "                avg_norm_rows\n",
    "            )  # add to a block in dict\n",
    "        return exp_stim_resp_dict\n",
    "\n",
    "    def create_exp_stim_response_df(self, exp_name: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create a DataFrame that contains the processed Kernel Flow data in response\n",
    "        to each stimulus in an experiment. Each channel is normalized and averaged.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Processed Kernel Flow data.\n",
    "        \"\"\"\n",
    "\n",
    "        def split_col(row: pd.Series) -> pd.Series:\n",
    "            \"\"\"\n",
    "            Split a column containing an array into separate columns for each\n",
    "            element in the array.\n",
    "\n",
    "            Args:\n",
    "                row (pd.Series): DataFrame row.\n",
    "\n",
    "            Returns:\n",
    "                pd.Series: DataFrame row with split column.\n",
    "            \"\"\"\n",
    "            arr = row[\"Channels\"]\n",
    "            num_elements = len(arr)\n",
    "            col_names = [i for i in range(num_elements)]\n",
    "            return pd.Series(arr, index=col_names)\n",
    "\n",
    "        exp_baseline_avg_dict = self.create_exp_stim_response_dict(exp_name)\n",
    "        rows = []\n",
    "        for block, block_data in sorted(exp_baseline_avg_dict.items()):\n",
    "            for trial, stim_resp_data in block_data.items():\n",
    "                trial_avg = np.mean(stim_resp_data, axis=0)\n",
    "                row = {\n",
    "                    \"Participant\": self.par_num,\n",
    "                    \"Block\": block,\n",
    "                    \"Channels\": trial_avg,\n",
    "                }\n",
    "                rows.append(row)\n",
    "\n",
    "        stim_resp_df = pd.DataFrame(rows)\n",
    "        channel_cols = stim_resp_df.apply(split_col, axis=1)\n",
    "        stim_resp_df = pd.concat(\n",
    "            [stim_resp_df, channel_cols], axis=1\n",
    "        )  # merge with original DataFrame\n",
    "        stim_resp_df = stim_resp_df.drop(\n",
    "            \"Channels\", axis=1\n",
    "        )  # drop the original \"Channels\" column\n",
    "        return stim_resp_df\n",
    "\n",
    "    def create_inter_module_exp_results_df(\n",
    "        self, exp_name: str, fmt: str = None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create a DataFrame with the inter-module channels for an experiment.\n",
    "        This DataFrame can include both HbO and HbR channels in alternating columns\n",
    "        or just \"HbO\" or \"HbR\" channels.\n",
    "\n",
    "        Args:\n",
    "            fmt (str, optional): \"HbO\" or \"HbR\" channels. Defaults to None (all inter-module channels).\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Inter-module channels for an experiment.\n",
    "        \"\"\"\n",
    "        exp_results = load_results(self.flow_processed_data_dir, exp_name)\n",
    "        session = self.par_behav.get_key_from_value(\n",
    "            self.par_behav.session_dict, exp_name\n",
    "        )\n",
    "        measurement_list_df = self.flow_session_dict[\n",
    "            session\n",
    "        ].create_source_detector_df()\n",
    "        channels = (measurement_list_df[\"measurement_list_index\"] - 1).tolist()\n",
    "        cols_to_select = [\"Participant\", \"Block\"] + [str(chan) for chan in channels]\n",
    "        inter_module_df = exp_results.loc[:, cols_to_select]\n",
    "        if fmt:\n",
    "            if fmt.lower() == \"hbo\":  # HbO\n",
    "                HbO_df = inter_module_df.iloc[\n",
    "                    :, np.r_[0, 1, 2 : len(inter_module_df.columns) : 2]\n",
    "                ]\n",
    "                return HbO_df\n",
    "            elif fmt.lower() == \"hbr\":  # HbR\n",
    "                HbR_df = inter_module_df.iloc[\n",
    "                    :, np.r_[0, 1, 3 : len(inter_module_df.columns) : 2]\n",
    "                ]\n",
    "                return HbR_df\n",
    "        else:\n",
    "            return inter_module_df\n",
    "\n",
    "    def lowpass_filter(\n",
    "        self, data: list[np.ndarray | pd.DataFrame]\n",
    "    ) -> Union[np.ndarray, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Lowpass filter input data.\n",
    "\n",
    "        Args:\n",
    "            data list([np.ndarray | pd.DataFrame]): Data to filter.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Lowpass filtered data.\n",
    "            -or-\n",
    "            pd.DataFrame: Lowpass filtered data.\n",
    "        \"\"\"\n",
    "        order = 20  # filter order\n",
    "        fs = 1.0  # sampling frequency (Hz)\n",
    "        cutoff = 0.1  # cut-off frequency (Hz)\n",
    "        nyq = 0.5 * fs  # nyquist\n",
    "        taps = firwin(order + 1, cutoff / nyq)\n",
    "\n",
    "        if type(data) == pd.DataFrame:\n",
    "            data_out = data.apply(\n",
    "                lambda x: lfilter(taps, 1.0, x)\n",
    "            )  # apply lowpass filter\n",
    "        else:\n",
    "            data_out = lfilter(taps, 1.0, data)  # apply lowpass filter\n",
    "        return data_out\n",
    "\n",
    "    def plot_flow_session(\n",
    "        self, session: str, channels: list[int | list | tuple], filter_type: str = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Plot Kernel flow session data.\n",
    "\n",
    "        Args:\n",
    "            session (str): Session number.\n",
    "            channels (list[int | list | tuple]): Kernel Flow channels to plot.\n",
    "            filter_type (str, optional): Filter type to apply. Defaults to None.\n",
    "        \"\"\"\n",
    "        flow_session = self.flow_session_dict[session]\n",
    "        sel_flow_data = flow_session.get_data(\"dataframe\", channels)  # TODO\n",
    "        if filter_type == \"lowpass\":\n",
    "            sel_flow_data = self.lowpass_filter(sel_flow_data)\n",
    "        session_time_offset = self.time_offset_dict[session]\n",
    "        time_abs_dt = flow_session.get_time_abs(\"datetime\")\n",
    "        time_abs_dt_offset = time_abs_dt - datetime.timedelta(\n",
    "            seconds=session_time_offset\n",
    "        )\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(15, 6))\n",
    "\n",
    "        data_traces = []\n",
    "        data_labels = []\n",
    "        for channel_num in channels:\n",
    "            flow_data = sel_flow_data.iloc[:, channel_num]\n",
    "            data_type_label = self.flow_session_dict[session].get_data_type_label(\n",
    "                channel_num\n",
    "            )\n",
    "            legend_label = f\"Ch {channel_num} ({data_type_label})\"\n",
    "            if data_type_label == \"HbO\":\n",
    "                color = \"red\"\n",
    "            elif data_type_label == \"HbR\":\n",
    "                color = \"blue\"\n",
    "            (data_trace,) = ax.plot(\n",
    "                time_abs_dt_offset, flow_data, color=color, label=legend_label\n",
    "            )\n",
    "            data_traces.append(data_trace)\n",
    "            data_labels.append(legend_label)\n",
    "\n",
    "        exp_spans = []\n",
    "        for exp_name in self.par_behav.session_dict[session]:\n",
    "            exp_start_dt = self.par_behav.get_start_dt(exp_name)\n",
    "            exp_end_dt = self.par_behav.get_end_dt(exp_name)\n",
    "            ax.axvline(exp_start_dt, linestyle=\"dashed\", color=\"k\", alpha=0.75)\n",
    "            ax.axvline(exp_end_dt, linestyle=\"dashed\", color=\"k\", alpha=0.75)\n",
    "            exp_span = ax.axvspan(\n",
    "                exp_start_dt,\n",
    "                exp_end_dt,\n",
    "                color=self.par_behav.exp_color_dict[exp_name],\n",
    "                alpha=0.4,\n",
    "                label=exp_name,\n",
    "            )\n",
    "            exp_spans.append(exp_span)\n",
    "\n",
    "        data_legend = ax.legend(\n",
    "            handles=data_traces,\n",
    "            bbox_to_anchor=(1.0, 1.0),\n",
    "            facecolor=\"white\",\n",
    "            framealpha=1,\n",
    "            title=\"Kernel Flow Data\",\n",
    "        )\n",
    "        handles, labels = plt.gca().get_legend_handles_labels()\n",
    "        uni_labels = dict(zip(labels, handles))\n",
    "        [uni_labels.pop(data_label) for data_label in data_labels]\n",
    "\n",
    "        stim_legend = ax.legend(\n",
    "            uni_labels.values(),\n",
    "            uni_labels.keys(),\n",
    "            bbox_to_anchor=(1.0, 0.75),\n",
    "            facecolor=\"white\",\n",
    "            framealpha=1,\n",
    "            title=\"Experiment\",\n",
    "        )\n",
    "        ax.add_artist(data_legend)\n",
    "        session_split = session.split(\"_\")\n",
    "        exp_title = session_split[0].capitalize() + \" \" + session_split[1]\n",
    "        ax.set_title(exp_title)\n",
    "        datetime_fmt = mdates.DateFormatter(\"%H:%M:%S\")\n",
    "        ax.xaxis.set_major_formatter(datetime_fmt)\n",
    "        ax.set_xlabel(\"Time\", fontsize=16, color=\"k\")\n",
    "\n",
    "    def plot_flow_exp(\n",
    "        self, exp_name: str, channels: list, filter_type: str = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Plot Kernel Flow experiment data.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "            channels (list): Kernel Flow channels to plot.\n",
    "            filter_type (str, optional): Filter type to apply. Defaults to None.\n",
    "        \"\"\"\n",
    "        flow_exp = self.load_flow_exp(exp_name)\n",
    "        session = self.par_behav.get_key_from_value(\n",
    "            self.par_behav.session_dict, exp_name\n",
    "        )\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(15, 6))\n",
    "\n",
    "        data_traces = []\n",
    "        data_labels = []\n",
    "        for channel_num in channels:\n",
    "            timeseries = flow_exp[\"datetime\"]\n",
    "            flow_data = flow_exp.iloc[:, channel_num + 1]\n",
    "            if filter_type == \"lowpass\":\n",
    "                flow_data = self.lowpass_filter(flow_data)\n",
    "            data_type_label = self.flow_session_dict[session].get_data_type_label(\n",
    "                channel_num\n",
    "            )\n",
    "            legend_label = f\"Ch {channel_num} ({data_type_label})\"\n",
    "            if data_type_label == \"HbO\":\n",
    "                color = \"red\"\n",
    "            elif data_type_label == \"HbR\":\n",
    "                color = \"blue\"\n",
    "            (data_trace,) = ax.plot(\n",
    "                timeseries, flow_data, color=color, label=legend_label\n",
    "            )\n",
    "            data_traces.append(data_trace)\n",
    "            data_labels.append(legend_label)\n",
    "\n",
    "        exp_start_dt = self.par_behav.get_start_dt(exp_name, self.adj_ts_markers)\n",
    "        exp_end_dt = self.par_behav.get_end_dt(exp_name, self.adj_ts_markers)\n",
    "        ax.axvline(exp_start_dt, linestyle=\"dashed\", color=\"k\", alpha=0.75)\n",
    "        ax.axvline(exp_end_dt, linestyle=\"dashed\", color=\"k\", alpha=0.75)\n",
    "        results_dir = r\"C:\\Users\\zackg\\OneDrive\\Ayaz Lab\\KernelFlow_Analysis\\processed_data\\behavioral\"  # NOTE: temporary\n",
    "        exp_results = load_results(results_dir, exp_name, self.par_num)\n",
    "        exp_title = self.par_behav.format_exp_name(exp_name)\n",
    "\n",
    "        stim_spans = []\n",
    "        for _, row in exp_results.iterrows():\n",
    "            try:\n",
    "                uni_stim_dict = self.par_behav.create_unique_stim_dict(\n",
    "                    exp_results, \"stim\"\n",
    "                )\n",
    "                stim = row[\"stim\"]\n",
    "                legend_label = self.par_behav.format_exp_name(row[\"stim\"])\n",
    "            except KeyError:\n",
    "                uni_stim_dict = self.par_behav.create_unique_stim_dict(\n",
    "                    exp_results, \"block\"\n",
    "                )\n",
    "                stim = row[\"block\"]\n",
    "                legend_label = self.par_behav.format_exp_name(row[\"block\"])\n",
    "            color_index = uni_stim_dict[stim]\n",
    "            stim_start = datetime.datetime.fromtimestamp(row[\"stim_start\"])\n",
    "            try:\n",
    "                stim_end = datetime.datetime.fromtimestamp(row[\"stim_end\"])\n",
    "            except ValueError:\n",
    "                if exp_name == \"go_no_go\":\n",
    "                    stim_time = 0.5  # seconds\n",
    "                stim_end = datetime.datetime.fromtimestamp(\n",
    "                    row[\"stim_start\"] + stim_time\n",
    "                )\n",
    "            stim_span = ax.axvspan(\n",
    "                stim_start,\n",
    "                stim_end,\n",
    "                color=self.plot_color_dict[color_index],\n",
    "                alpha=0.4,\n",
    "                label=legend_label,\n",
    "            )\n",
    "            stim_spans.append(stim_span)\n",
    "\n",
    "        data_legend = ax.legend(\n",
    "            handles=data_traces,\n",
    "            bbox_to_anchor=(1.0, 1.0),\n",
    "            facecolor=\"white\",\n",
    "            framealpha=1,\n",
    "            title=\"Kernel Flow Data\",\n",
    "        )\n",
    "        handles, labels = plt.gca().get_legend_handles_labels()\n",
    "        uni_labels = dict(zip(labels, handles))\n",
    "        [uni_labels.pop(data_label) for data_label in data_labels]\n",
    "\n",
    "        stim_legend = ax.legend(\n",
    "            uni_labels.values(),\n",
    "            uni_labels.keys(),\n",
    "            bbox_to_anchor=(1.0, 0.75),\n",
    "            facecolor=\"white\",\n",
    "            framealpha=1,\n",
    "            title=\"Stimulus\",\n",
    "        )\n",
    "        ax.add_artist(data_legend)\n",
    "        ax.set_title(exp_title)\n",
    "        datetime_fmt = mdates.DateFormatter(\"%H:%M:%S\")\n",
    "        ax.xaxis.set_major_formatter(datetime_fmt)\n",
    "        ax.set_xlabel(\"Time\", fontsize=16, color=\"k\")\n",
    "\n",
    "\n",
    "def create_flow_results_tables(num_pars: int) -> None:\n",
    "    \"\"\"\n",
    "    Generate an Excel file that contains the Kernel Flow stimulus response data\n",
    "    for all experiments and participants.\n",
    "\n",
    "    Args:\n",
    "        num_pars (int): Number of participants in the study.\n",
    "    \"\"\"\n",
    "    exp_order = [\n",
    "        \"audio_narrative\",\n",
    "        \"go_no_go\",\n",
    "        \"king_devick\",\n",
    "        \"n_back\",\n",
    "        \"resting_state\",\n",
    "        \"tower_of_london\",\n",
    "        \"vSAT\",\n",
    "        \"video_narrative_cmiyc\",\n",
    "        \"video_narrative_sherlock\",\n",
    "    ]\n",
    "    all_exp_results_list = []\n",
    "    for par_num in range(1, num_pars + 1):\n",
    "        print(f\"Processing participant {par_num} ...\")\n",
    "        par = Participant_Flow(par_num)\n",
    "        exp_results_list = []\n",
    "        for exp_name in exp_order:\n",
    "            stim_resp_df = par.create_exp_stim_response_df(exp_name)\n",
    "            exp_results_list.append(stim_resp_df)\n",
    "        all_exp_results_list.append(exp_results_list)\n",
    "\n",
    "    filedir = par.flow_processed_data_dir\n",
    "    if not os.path.exists(os.path.dirname(filedir)):\n",
    "        os.mkdir(os.path.dirname(filedir))\n",
    "\n",
    "    for i, exp_name in enumerate(exp_order):\n",
    "        exp_rows = [exp_results_list[i] for exp_results_list in all_exp_results_list]\n",
    "        exp_df = pd.concat(exp_rows, axis=0, ignore_index=True)\n",
    "        filepath = os.path.join(filedir, f\"{exp_name}_flow.csv\")\n",
    "        exp_df.to_csv(filepath, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "participant_02\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Participant</th>\n",
       "      <th>Block</th>\n",
       "      <th>1</th>\n",
       "      <th>3</th>\n",
       "      <th>5</th>\n",
       "      <th>7</th>\n",
       "      <th>9</th>\n",
       "      <th>11</th>\n",
       "      <th>61</th>\n",
       "      <th>63</th>\n",
       "      <th>...</th>\n",
       "      <th>4219</th>\n",
       "      <th>4221</th>\n",
       "      <th>4223</th>\n",
       "      <th>4225</th>\n",
       "      <th>4281</th>\n",
       "      <th>4283</th>\n",
       "      <th>4285</th>\n",
       "      <th>4287</th>\n",
       "      <th>4289</th>\n",
       "      <th>4291</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0_back</td>\n",
       "      <td>-0.061740</td>\n",
       "      <td>-0.045396</td>\n",
       "      <td>-0.014794</td>\n",
       "      <td>-0.019565</td>\n",
       "      <td>-0.113982</td>\n",
       "      <td>0.020400</td>\n",
       "      <td>-0.091770</td>\n",
       "      <td>-0.001125</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016451</td>\n",
       "      <td>-0.002667</td>\n",
       "      <td>-0.063466</td>\n",
       "      <td>-0.077119</td>\n",
       "      <td>-0.021397</td>\n",
       "      <td>0.003093</td>\n",
       "      <td>-0.130032</td>\n",
       "      <td>-0.098489</td>\n",
       "      <td>0.120857</td>\n",
       "      <td>-0.089635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0_back</td>\n",
       "      <td>0.037491</td>\n",
       "      <td>0.037959</td>\n",
       "      <td>0.033026</td>\n",
       "      <td>0.032214</td>\n",
       "      <td>-0.055479</td>\n",
       "      <td>-0.008248</td>\n",
       "      <td>-0.006472</td>\n",
       "      <td>-0.001565</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100989</td>\n",
       "      <td>0.048041</td>\n",
       "      <td>0.013167</td>\n",
       "      <td>0.054327</td>\n",
       "      <td>0.007747</td>\n",
       "      <td>-0.118054</td>\n",
       "      <td>0.053620</td>\n",
       "      <td>-0.016646</td>\n",
       "      <td>-0.025307</td>\n",
       "      <td>0.080856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0_back</td>\n",
       "      <td>-0.081892</td>\n",
       "      <td>-0.038416</td>\n",
       "      <td>-0.039464</td>\n",
       "      <td>-0.011566</td>\n",
       "      <td>-0.035225</td>\n",
       "      <td>-0.024752</td>\n",
       "      <td>-0.037667</td>\n",
       "      <td>-0.051450</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047214</td>\n",
       "      <td>-0.016213</td>\n",
       "      <td>0.085961</td>\n",
       "      <td>-0.205686</td>\n",
       "      <td>-0.028179</td>\n",
       "      <td>-0.084198</td>\n",
       "      <td>-0.021898</td>\n",
       "      <td>-0.011186</td>\n",
       "      <td>-0.023106</td>\n",
       "      <td>-0.033720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1_back</td>\n",
       "      <td>0.020251</td>\n",
       "      <td>-0.029147</td>\n",
       "      <td>-0.026428</td>\n",
       "      <td>0.003749</td>\n",
       "      <td>0.082904</td>\n",
       "      <td>-0.013057</td>\n",
       "      <td>-0.006590</td>\n",
       "      <td>0.060276</td>\n",
       "      <td>...</td>\n",
       "      <td>0.074927</td>\n",
       "      <td>-0.047389</td>\n",
       "      <td>0.044372</td>\n",
       "      <td>-0.022984</td>\n",
       "      <td>-0.096989</td>\n",
       "      <td>0.018543</td>\n",
       "      <td>-0.139527</td>\n",
       "      <td>-0.058265</td>\n",
       "      <td>-0.073187</td>\n",
       "      <td>0.068137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1_back</td>\n",
       "      <td>-0.009724</td>\n",
       "      <td>-0.021163</td>\n",
       "      <td>-0.015846</td>\n",
       "      <td>0.014788</td>\n",
       "      <td>-0.003454</td>\n",
       "      <td>-0.051465</td>\n",
       "      <td>-0.055697</td>\n",
       "      <td>0.016875</td>\n",
       "      <td>...</td>\n",
       "      <td>0.020599</td>\n",
       "      <td>-0.068672</td>\n",
       "      <td>0.005145</td>\n",
       "      <td>0.038495</td>\n",
       "      <td>0.040013</td>\n",
       "      <td>0.142902</td>\n",
       "      <td>0.095687</td>\n",
       "      <td>-0.020499</td>\n",
       "      <td>-0.040581</td>\n",
       "      <td>-0.122854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>15</td>\n",
       "      <td>1_back</td>\n",
       "      <td>0.015106</td>\n",
       "      <td>0.010910</td>\n",
       "      <td>-0.008337</td>\n",
       "      <td>-0.018495</td>\n",
       "      <td>-0.010675</td>\n",
       "      <td>0.017623</td>\n",
       "      <td>0.023815</td>\n",
       "      <td>0.009218</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>15</td>\n",
       "      <td>1_back</td>\n",
       "      <td>-0.013921</td>\n",
       "      <td>-0.034779</td>\n",
       "      <td>-0.051731</td>\n",
       "      <td>-0.030712</td>\n",
       "      <td>-0.022956</td>\n",
       "      <td>-0.049035</td>\n",
       "      <td>-0.013359</td>\n",
       "      <td>-0.046708</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>15</td>\n",
       "      <td>2_back</td>\n",
       "      <td>0.017791</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>0.030502</td>\n",
       "      <td>0.025366</td>\n",
       "      <td>0.006680</td>\n",
       "      <td>0.022695</td>\n",
       "      <td>0.029593</td>\n",
       "      <td>0.057516</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>15</td>\n",
       "      <td>2_back</td>\n",
       "      <td>0.039527</td>\n",
       "      <td>0.032654</td>\n",
       "      <td>0.043869</td>\n",
       "      <td>0.045958</td>\n",
       "      <td>0.052935</td>\n",
       "      <td>0.041592</td>\n",
       "      <td>0.012904</td>\n",
       "      <td>0.007820</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>15</td>\n",
       "      <td>2_back</td>\n",
       "      <td>0.012979</td>\n",
       "      <td>-0.005255</td>\n",
       "      <td>-0.006950</td>\n",
       "      <td>-0.028033</td>\n",
       "      <td>-0.010641</td>\n",
       "      <td>-0.015822</td>\n",
       "      <td>-0.009604</td>\n",
       "      <td>0.011667</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>135 rows × 308 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Participant   Block         1         3         5         7         9  \\\n",
       "0              1  0_back -0.061740 -0.045396 -0.014794 -0.019565 -0.113982   \n",
       "1              1  0_back  0.037491  0.037959  0.033026  0.032214 -0.055479   \n",
       "2              1  0_back -0.081892 -0.038416 -0.039464 -0.011566 -0.035225   \n",
       "3              1  1_back  0.020251 -0.029147 -0.026428  0.003749  0.082904   \n",
       "4              1  1_back -0.009724 -0.021163 -0.015846  0.014788 -0.003454   \n",
       "..           ...     ...       ...       ...       ...       ...       ...   \n",
       "130           15  1_back  0.015106  0.010910 -0.008337 -0.018495 -0.010675   \n",
       "131           15  1_back -0.013921 -0.034779 -0.051731 -0.030712 -0.022956   \n",
       "132           15  2_back  0.017791  0.020000  0.030502  0.025366  0.006680   \n",
       "133           15  2_back  0.039527  0.032654  0.043869  0.045958  0.052935   \n",
       "134           15  2_back  0.012979 -0.005255 -0.006950 -0.028033 -0.010641   \n",
       "\n",
       "           11        61        63  ...      4219      4221      4223  \\\n",
       "0    0.020400 -0.091770 -0.001125  ...  0.016451 -0.002667 -0.063466   \n",
       "1   -0.008248 -0.006472 -0.001565  ...  0.100989  0.048041  0.013167   \n",
       "2   -0.024752 -0.037667 -0.051450  ...  0.047214 -0.016213  0.085961   \n",
       "3   -0.013057 -0.006590  0.060276  ...  0.074927 -0.047389  0.044372   \n",
       "4   -0.051465 -0.055697  0.016875  ...  0.020599 -0.068672  0.005145   \n",
       "..        ...       ...       ...  ...       ...       ...       ...   \n",
       "130  0.017623  0.023815  0.009218  ...       NaN       NaN       NaN   \n",
       "131 -0.049035 -0.013359 -0.046708  ...       NaN       NaN       NaN   \n",
       "132  0.022695  0.029593  0.057516  ...       NaN       NaN       NaN   \n",
       "133  0.041592  0.012904  0.007820  ...       NaN       NaN       NaN   \n",
       "134 -0.015822 -0.009604  0.011667  ...       NaN       NaN       NaN   \n",
       "\n",
       "         4225      4281      4283      4285      4287      4289      4291  \n",
       "0   -0.077119 -0.021397  0.003093 -0.130032 -0.098489  0.120857 -0.089635  \n",
       "1    0.054327  0.007747 -0.118054  0.053620 -0.016646 -0.025307  0.080856  \n",
       "2   -0.205686 -0.028179 -0.084198 -0.021898 -0.011186 -0.023106 -0.033720  \n",
       "3   -0.022984 -0.096989  0.018543 -0.139527 -0.058265 -0.073187  0.068137  \n",
       "4    0.038495  0.040013  0.142902  0.095687 -0.020499 -0.040581 -0.122854  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "130       NaN       NaN       NaN       NaN       NaN       NaN       NaN  \n",
       "131       NaN       NaN       NaN       NaN       NaN       NaN       NaN  \n",
       "132       NaN       NaN       NaN       NaN       NaN       NaN       NaN  \n",
       "133       NaN       NaN       NaN       NaN       NaN       NaN       NaN  \n",
       "134       NaN       NaN       NaN       NaN       NaN       NaN       NaN  \n",
       "\n",
       "[135 rows x 308 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SNIRF file loading\n",
    "par_num = 2\n",
    "par = Participant_Flow(par_num)\n",
    "print(par.par_ID)\n",
    "\n",
    "par.create_inter_module_exp_results_df(\"n_back\", \"HbR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment order: ['vSAT', 'king_devick', 'audio_narrative', 'resting_state', 'go_no_go', 'video_narrative_sherlock', 'n_back', 'tower_of_london', 'video_narrative_cmiyc']\n",
      "\n",
      "Experiment time origin: 2022-06-17 14:07:11.993452\n",
      "Start marker sent time: 14.4406\n",
      "Start marker sent time (absolute): 2022-06-17 14:07:26.434052\n",
      "Flow time origin: 2022-06-17 17:40:13\n",
      "\n",
      "Kernel marker data (original):\n",
      "    Timestamp    Duration  Value  Experiment\n",
      "0    18.45619  427.252366    1.0        51.0\n",
      "1  1110.70192  487.842409    1.0        81.0\n",
      "\n",
      "Kernel marker data (absolute):\n",
      "                           Marker  Start timestamp    Duration  End timestamp\n",
      "0             resting_state_start     1.655489e+09  427.252366   1.655490e+09\n",
      "1  video_narrative_sherlock_start     1.655490e+09  487.842409   1.655491e+09\n",
      "\n",
      "Time offset: 13.022\n",
      "\n",
      "Time difference:\n",
      "------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'resting_state': 12.281041622161865,\n",
       " 'go_no_go': 12.282827615737915,\n",
       " 'video_narrative_sherlock': 12.284613609313965,\n",
       " 'king_devick': 11.980223417282104,\n",
       " 'vSAT': 11.980556726455688,\n",
       " 'audio_narrative': 11.982890844345093,\n",
       " 'n_back': 12.57655382156372,\n",
       " 'tower_of_london': 12.580034732818604,\n",
       " 'video_narrative_cmiyc': 12.58246636390686,\n",
       " 'session_1001': 12.282827615737915,\n",
       " 'session_1002': 11.981223662694296,\n",
       " 'session_1003': 12.579684972763062}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time offset issue\n",
    "# Kernel Flow PC - Behavioral Task PC\n",
    "\n",
    "print(f\"Experiment order: {par.par_behav.exp_order}\\n\")\n",
    "\n",
    "exp_name = \"resting_state\"\n",
    "exp = par.par_behav.get_exp(exp_name)\n",
    "exp_time_origin_ts = exp.start_ts\n",
    "exp_time_origin_dt = datetime.datetime.fromtimestamp(exp_time_origin_ts)\n",
    "print(f\"Experiment time origin: {exp_time_origin_dt}\")\n",
    "start_marker_sent_time = float(exp.marker_data[\"start_marker\"][\"sent_time\"])\n",
    "print(f\"Start marker sent time: {start_marker_sent_time}\")\n",
    "start_marker_sent_time_abs = exp_time_origin_dt + datetime.timedelta(seconds=start_marker_sent_time)\n",
    "print(f\"Start marker sent time (absolute): {start_marker_sent_time_abs}\")\n",
    "\n",
    "# The behavioral start marker sent time (absolute) and \n",
    "# kernel start marker receive time (absolute) should be identical.\n",
    "\n",
    "flow_time_origin = par.flow_session_dict[\"session_1001\"].get_time_origin(offset=False)\n",
    "print(f\"Flow time origin: {flow_time_origin}\\n\")\n",
    "session = par.par_behav.get_key_from_value(par.par_behav.session_dict, exp_name)\n",
    "marker_df = par.flow_session_dict[session].get_marker_df()\n",
    "print(f\"Kernel marker data (original):\\n{marker_df}\\n\")\n",
    "marker_df_abs = par.create_abs_marker_df(session)\n",
    "print(f\"Kernel marker data (absolute):\\n{marker_df_abs}\")\n",
    "\n",
    "row = marker_df_abs.loc[marker_df_abs[\"Marker\"].str.startswith(exp_name)].reset_index()\n",
    "kernel_start_marker_ts = row.loc[0, \"Start timestamp\"]\n",
    "time_offset = kernel_start_marker_ts - (exp_time_origin_ts + start_marker_sent_time)\n",
    "print(f\"\\nTime offset: {round(time_offset, 3)}\\n\")\n",
    "\n",
    "par_num = 1\n",
    "par = Participant_Flow(par_num)\n",
    "time_offset_list = []\n",
    "print(\"Time difference:\\n------------\")\n",
    "par.time_offset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9c1e24752b3065052c27c07c0a22748a6118d05dd89b50e77c79e7ce74e5970"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
