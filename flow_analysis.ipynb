{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from flow_analysis import Participant_Flow, Flow_Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import snirf\n",
    "import ctypes\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pingouin as pg\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.axes import Axes\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from adjustText import adjust_text\n",
    "from PIL import Image\n",
    "from statistics import mean\n",
    "from scipy.signal import butter, filtfilt, sosfiltfilt\n",
    "from typing import Union, Tuple, List, Literal\n",
    "from behav_analysis import Participant_Behav\n",
    "from data_functions import Data_Functions, load_results, exp_name_to_title\n",
    "\n",
    "hllDll = ctypes.WinDLL(\n",
    "    r\"C:\\Program Files\\R\\R-4.2.3\\bin\\x64\\R.dll\"\n",
    ")  # path to R DLL file\n",
    "import rpy2.robjects as robjects\n",
    "\n",
    "\n",
    "class Flow_Coordinates:\n",
    "    \"\"\"\n",
    "    Convert Kernel Flow XYZ coordinates into the MRIcron AAL template XYZ and MNI coordinates. All units in mm.\n",
    "    Brain anatomical directions:\n",
    "        x-pos = right\n",
    "        x-neg = left\n",
    "        y-pos = anterior\n",
    "        y-neg = posterior\n",
    "        z-pos = superior\n",
    "        z-neg = inferior\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, source_detector_df: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            source_detector_df (pd.DataFrame): DataFrame with inter-module source and detector XYZ coordinates.\n",
    "        \"\"\"\n",
    "        self.sd_df = source_detector_df\n",
    "\n",
    "        # measurements from: https://doi.org/10.1002/jbio.201900175\n",
    "        self.SCALP_THICKNESS = 3\n",
    "        self.SKULL_THICKNESS = 7\n",
    "        self.CSF_THICKNESS = 2\n",
    "        self.BRAIN_SURFACE_DEPTH = (\n",
    "            self.SCALP_THICKNESS + self.SKULL_THICKNESS + self.CSF_THICKNESS\n",
    "        )\n",
    "\n",
    "        # XYZ and MNI measurements from MRIcron AAL template: https://www.nitrc.org/projects/mricron\n",
    "        self.XYZ_ORIGIN = (91, 126, 72)\n",
    "        self.MNI_ORIGIN = (0, 0, 0)\n",
    "\n",
    "        self.X_MIN = 18\n",
    "        self.Y_MIN = 21\n",
    "        self.Z_MIN = 11\n",
    "\n",
    "        self.X_MAX = 163\n",
    "        self.Y_MAX = 200\n",
    "        self.Z_MAX = 156\n",
    "\n",
    "        self.MNI_X_MIN = -73\n",
    "        self.MNI_Y_MIN = -105\n",
    "        self.MNI_Z_MIN = -61\n",
    "\n",
    "        self.MNI_X_MAX = 72\n",
    "        self.MNI_Y_MAX = 74\n",
    "        self.MNI_Z_MAX = 84\n",
    "\n",
    "        self.X_MIN_ADJ = self.X_MIN - self.BRAIN_SURFACE_DEPTH\n",
    "        self.Y_MIN_ADJ = self.Y_MIN - self.BRAIN_SURFACE_DEPTH\n",
    "        self.Z_MIN_ADJ = self.Z_MIN - self.BRAIN_SURFACE_DEPTH\n",
    "        self.X_MAX_ADJ = self.X_MAX + self.BRAIN_SURFACE_DEPTH\n",
    "        self.Y_MAX_ADJ = self.Y_MAX + self.BRAIN_SURFACE_DEPTH\n",
    "        self.Z_MAX_ADJ = self.Z_MAX + self.BRAIN_SURFACE_DEPTH\n",
    "\n",
    "        self.MNI_X_MIN_ADJ = self.MNI_X_MIN - self.BRAIN_SURFACE_DEPTH\n",
    "        self.MNI_Y_MIN_ADJ = self.MNI_Y_MIN - self.BRAIN_SURFACE_DEPTH\n",
    "        self.MNI_Z_MIN_ADJ = self.MNI_Z_MIN - self.BRAIN_SURFACE_DEPTH\n",
    "        self.MNI_X_MAX_ADJ = self.MNI_X_MAX + self.BRAIN_SURFACE_DEPTH\n",
    "        self.MNI_Y_MAX_ADJ = self.MNI_Y_MAX + self.BRAIN_SURFACE_DEPTH\n",
    "        self.MNI_Z_MAX_ADJ = self.MNI_Z_MAX + self.BRAIN_SURFACE_DEPTH\n",
    "\n",
    "        # Kernel Flow source/detector midpoint min and max for each coordinate dimension\n",
    "        self.FLOW_X_MIN = self._get_flow_pos(\"x\", \"min\")  # -87.26267567763952\n",
    "        self.FLOW_Y_MIN = self._get_flow_pos(\"y\", \"min\")  # -121.1857849385022\n",
    "        self.FLOW_Z_MIN = self._get_flow_pos(\"z\", \"min\")  # -18.382019226737423\n",
    "        self.FLOW_X_MAX = self._get_flow_pos(\"x\", \"max\")  # 87.32753031777712\n",
    "        self.FLOW_Y_MAX = self._get_flow_pos(\"y\", \"max\")  # 86.61967154031478\n",
    "        self.FLOW_Z_MAX = self._get_flow_pos(\"z\", \"max\")  # 104.75086014260755\n",
    "\n",
    "        self._run_basic_checks()\n",
    "\n",
    "    def _get_flow_pos(\n",
    "        self, dim: Literal[\"x\", \"y\", \"z\"], pos_type: Literal[\"min\", \"max\"]\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Get the min or max Kernel Flow XYZ dimension coordinate.\n",
    "\n",
    "        Args:\n",
    "            dim (Literal[\"x\", \"y\", \"z\"]): Coordinate dimension.\n",
    "            pos_type (Literal[\"min\", \"max\"]): Min or max coordinate in that dimension.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: Invalid dimension.\n",
    "            ValueError: Invalid position type.\n",
    "\n",
    "        Returns:\n",
    "            float: Min or max coordinate value.\n",
    "        \"\"\"\n",
    "        if dim.lower() == \"x\":\n",
    "            pos_cols = self.sd_df[\"midpoint_x_pos\"]\n",
    "        elif dim.lower() == \"y\":\n",
    "            pos_cols = self.sd_df[\"midpoint_y_pos\"]\n",
    "        elif dim.lower() == \"z\":\n",
    "            pos_cols = self.sd_df[\"midpoint_z_pos\"]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid dimension. Must be 'x', 'y', or 'z'.\")\n",
    "\n",
    "        if pos_type.lower() == \"max\":\n",
    "            pos = pos_cols.max()\n",
    "        elif pos_type.lower() == \"min\":\n",
    "            pos = pos_cols.min()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid position type. Must be 'min' or 'max'.\")\n",
    "        return pos\n",
    "\n",
    "    def _get_range(\n",
    "        self, dim: Literal[\"x\", \"y\", \"z\"], source: Literal[\"template\", \"flow\"]\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Get the dimension range for the template AAL XYZ coordinates or Kernel Flow XYZ coordinates.\n",
    "\n",
    "        Args:\n",
    "            dim (Literal[\"x\", \"y\", \"z\"]): Coordinate dimension.\n",
    "            source (Literal[\"template\", \"flow\"]): Coordinate system measurement source.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: Invalid measurement source.\n",
    "            ValueError: Invalid dimension.\n",
    "\n",
    "        Returns:\n",
    "            float: Range of a coordinate system dimension.\n",
    "        \"\"\"\n",
    "        if dim.lower() == \"x\":\n",
    "            if source.lower() == \"template\":\n",
    "                range_out = self.X_MAX_ADJ - self.X_MIN_ADJ\n",
    "            elif source.lower() == \"flow\":\n",
    "                range_out = self.FLOW_X_MAX - self.FLOW_X_MIN\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Invalid measurement source. Must be 'template' or 'flow'.\"\n",
    "                )\n",
    "        elif dim.lower() == \"y\":\n",
    "            if source.lower() == \"template\":\n",
    "                range_out = self.Y_MAX_ADJ - self.Y_MIN_ADJ\n",
    "            elif source.lower() == \"flow\":\n",
    "                range_out = self.FLOW_Y_MAX - self.FLOW_Y_MIN\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Invalid measurement source. Must be 'template' or 'flow'.\"\n",
    "                )\n",
    "        elif dim.lower() == \"z\":\n",
    "            if source.lower() == \"template\":\n",
    "                range_out = self.Z_MAX_ADJ - self.Z_MIN_ADJ\n",
    "            elif source.lower() == \"flow\":\n",
    "                range_out = self.FLOW_Z_MAX - self.FLOW_Z_MIN\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Invalid measurement source. Must be 'template' or 'flow'.\"\n",
    "                )\n",
    "        else:\n",
    "            raise ValueError(\"Invalid dimension. Must be 'x', 'y', or 'z'.\")\n",
    "        return range_out\n",
    "\n",
    "    def _get_scaling_factor(self, dim: Literal[\"x\", \"y\", \"z\"]) -> float:\n",
    "        \"\"\"\n",
    "        Get the scaling factor to equalize a Kernel Flow XYZ coordinate dimension range to an\n",
    "        AAL template XYZ coordinate dimension range.\n",
    "\n",
    "        Args:\n",
    "             dim (Literal[\"x\", \"y\", \"z\"]): Coordinate dimension.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: Invalid dimension.\n",
    "\n",
    "        Returns:\n",
    "            float: Scaling factor.\n",
    "        \"\"\"\n",
    "        if dim.lower() == \"x\":\n",
    "            scaling_factor = self._get_range(\"x\", \"template\") / self._get_range(\n",
    "                \"x\", \"flow\"\n",
    "            )\n",
    "        elif dim.lower() == \"y\":\n",
    "            scaling_factor = self._get_range(\"y\", \"template\") / self._get_range(\n",
    "                \"y\", \"flow\"\n",
    "            )\n",
    "        elif dim.lower() == \"z\":\n",
    "            scaling_factor = self._get_range(\"z\", \"template\") / self._get_range(\n",
    "                \"z\", \"flow\"\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Invalid dimension. Must be 'x', 'y', or 'z'.\")\n",
    "        return scaling_factor\n",
    "\n",
    "    def _scale_kernel_XYZ(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Scale Kernel Flow XYZ coordinate dimensions by the scaling factor such that the range of each\n",
    "        dimension equals the AAL template XYZ coordinate dimension.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Scaled Kernel Flow XYZ coordinate dimensions.\n",
    "        \"\"\"\n",
    "        sd_df_scaled = pd.DataFrame()\n",
    "        sd_df_scaled[\"midpoint_x_pos\"] = self.sd_df[\n",
    "            \"midpoint_x_pos\"\n",
    "        ] * self._get_scaling_factor(\"x\")\n",
    "        sd_df_scaled[\"midpoint_y_pos\"] = self.sd_df[\n",
    "            \"midpoint_y_pos\"\n",
    "        ] * self._get_scaling_factor(\"y\")\n",
    "        sd_df_scaled[\"midpoint_z_pos\"] = self.sd_df[\n",
    "            \"midpoint_z_pos\"\n",
    "        ] * self._get_scaling_factor(\"z\")\n",
    "        return sd_df_scaled\n",
    "\n",
    "    def _align_kernel_XYZ(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Align Kernel Flow XYZ coordinates with the AAL template XYZ coordinates.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Kernel Flow XYZ coordinate dimensions.\n",
    "        \"\"\"\n",
    "        sd_df_scaled = self._scale_kernel_XYZ()\n",
    "        sd_df_aligned = pd.DataFrame()\n",
    "        x_align = sd_df_scaled[\"midpoint_x_pos\"].max() - self.X_MAX_ADJ\n",
    "        y_align = sd_df_scaled[\"midpoint_y_pos\"].max() - self.Y_MAX_ADJ\n",
    "        z_align = sd_df_scaled[\"midpoint_z_pos\"].max() - self.Z_MAX_ADJ\n",
    "        sd_df_aligned[\"midpoint_x_XYZ\"] = sd_df_scaled[\"midpoint_x_pos\"] - x_align\n",
    "        sd_df_aligned[\"midpoint_y_XYZ\"] = sd_df_scaled[\"midpoint_y_pos\"] - y_align\n",
    "        sd_df_aligned[\"midpoint_z_XYZ\"] = sd_df_scaled[\"midpoint_z_pos\"] - z_align\n",
    "        return sd_df_aligned\n",
    "\n",
    "    def _get_depth(self, depth: Union[int, float] = None) -> Union[int, float]:\n",
    "        \"\"\"\n",
    "        Get the Kernel Flow coordinate translation depth (in mm).\n",
    "\n",
    "        Args:\n",
    "            depth (Union[int, float], optional): Depth into the brain. Defaults to None (brain surface).\n",
    "\n",
    "        Returns:\n",
    "            depth (Union[int, float], optional): Depth into the brain.\n",
    "        \"\"\"\n",
    "        if depth is None:\n",
    "            depth_out = self.BRAIN_SURFACE_DEPTH\n",
    "        else:\n",
    "            depth_out = self.BRAIN_SURFACE_DEPTH + depth\n",
    "        return depth_out\n",
    "\n",
    "    def _translate_kernel_XYZ(self, depth: Union[int, float] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Translate Kernel Flow XYZ coordinate dimensions to the brain surface or to a specified depth.\n",
    "\n",
    "        Args:\n",
    "            depth (Union[int, float], optional): Depth into the brain. Defaults to None (brain surface).\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Translated Kernel Flow DataFrame.\n",
    "        \"\"\"\n",
    "        depth = self._get_depth(depth)\n",
    "        sd_df_aligned = self._align_kernel_XYZ()\n",
    "        sd_df_translated = pd.DataFrame()\n",
    "        if depth:\n",
    "            for col in sd_df_aligned.columns:\n",
    "                min_val = sd_df_aligned[col].min()\n",
    "                max_val = sd_df_aligned[col].max()\n",
    "                new_min = min_val + depth\n",
    "                new_max = max_val - depth\n",
    "                sd_df_translated[col] = (sd_df_aligned[col] - min_val) * (\n",
    "                    new_max - new_min\n",
    "                ) / (max_val - min_val) + new_min\n",
    "            self._run_translation_check(sd_df_translated, depth)\n",
    "        else:\n",
    "            sd_df_translated = sd_df_aligned\n",
    "        return sd_df_translated\n",
    "\n",
    "    def get_kernel_XYZ(self, depth: Union[int, float] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get a DataFrame with Kernel Flow XYZ coordinates scaled and aligned with the AAL template XYZ coordinates.\n",
    "\n",
    "        Args:\n",
    "            depth (Union[int, float], optional): Depth into the brain. Defaults to None (brain surface).\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Kernel Flow XYZ coordinate dimensions.\n",
    "        \"\"\"\n",
    "        kernel_XYZ = self._translate_kernel_XYZ(depth)\n",
    "        return kernel_XYZ\n",
    "\n",
    "    def get_kernel_MNI(self, depth: Union[int, float] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get a DataFrame with Kernel Flow MNI coordinates scaled and aligned with the AAL template MNI coordinates.\n",
    "\n",
    "        Args:\n",
    "            depth (Union[int, float], optional): Depth into the brain. Defaults to None (brain surface).\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Kernel Flow MNI coordinate dimensions.\n",
    "        \"\"\"\n",
    "        kernel_XYZ = self.get_kernel_XYZ(depth)\n",
    "        kernel_MNI = pd.DataFrame()\n",
    "        kernel_MNI[\"midpoint_x_MNI\"] = kernel_XYZ[\"midpoint_x_XYZ\"] - self.XYZ_ORIGIN[0]\n",
    "        kernel_MNI[\"midpoint_y_MNI\"] = kernel_XYZ[\"midpoint_y_XYZ\"] - self.XYZ_ORIGIN[1]\n",
    "        kernel_MNI[\"midpoint_z_MNI\"] = kernel_XYZ[\"midpoint_z_XYZ\"] - self.XYZ_ORIGIN[2]\n",
    "        return kernel_MNI\n",
    "\n",
    "    def create_source_detector_adj(\n",
    "        self, depth: Union[int, float] = None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Add the new Kernel Flow XYZ and MNI coordinates to the source/detector DataFrame.\n",
    "\n",
    "        Args:\n",
    "            depth (Union[int, float], optional): Depth into the brain. Defaults to None (brain surface).\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Adjusted source/detector DataFrame.\n",
    "        \"\"\"\n",
    "        kernel_XYZ = self.get_kernel_XYZ(depth)\n",
    "        kernel_MNI = self.get_kernel_MNI(depth)\n",
    "        sd_df_adj = self.sd_df\n",
    "        sd_df_adj[\"midpoint_x_XYZ\"] = kernel_XYZ[\"midpoint_x_XYZ\"]\n",
    "        sd_df_adj[\"midpoint_y_XYZ\"] = kernel_XYZ[\"midpoint_y_XYZ\"]\n",
    "        sd_df_adj[\"midpoint_z_XYZ\"] = kernel_XYZ[\"midpoint_z_XYZ\"]\n",
    "        sd_df_adj[\"midpoint_x_MNI\"] = kernel_MNI[\"midpoint_x_MNI\"]\n",
    "        sd_df_adj[\"midpoint_y_MNI\"] = kernel_MNI[\"midpoint_y_MNI\"]\n",
    "        sd_df_adj[\"midpoint_z_MNI\"] = kernel_MNI[\"midpoint_z_MNI\"]\n",
    "        return sd_df_adj\n",
    "\n",
    "    def plot_coordinates(\n",
    "        self, coord_sys: Literal[\"XYZ\", \"MNI\"], depth: Union[int, float] = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Plot the Kernel Flow XYZ or MNI coordinate system scaled to and aligned with the AAL template.\n",
    "        The bounding surfaces indicate the min and max x, y, and z values of the respective AAL template coordinate system.\n",
    "        A \"depth\" argument is accepted which projects the Kernel Flow coordinates that number of mm into the cortex.\n",
    "\n",
    "        Args:\n",
    "            coord_sys (Literal[\"XYZ\", \"MNI\"]): Coordinate system. \"XYZ\" or \"MNI\".\n",
    "            depth (Union[int, float], optional): Depth into the brain\n",
    "                                                Defaults to None (brain surface).\n",
    "        \"\"\"\n",
    "\n",
    "        def _create_surface(\n",
    "            ax: Axes, val: Union[int, float], dim: Literal[\"x\", \"y\", \"z\"], color: str\n",
    "        ) -> None:\n",
    "            \"\"\"\n",
    "            Create a surface at the specified dimension value.\n",
    "\n",
    "            Args:\n",
    "                ax (pd.Axes): Plot axis.\n",
    "                val (Union[int, float]): Surface value.\n",
    "                dim (Literal[\"x\", \"y\", \"z\"]): Coordinate dimension.\n",
    "                color (str): Surface color.\n",
    "\n",
    "            Raises:\n",
    "                ValueError: Invalid coordinate dimension.\n",
    "            \"\"\"\n",
    "            if coord_sys.lower() == \"xyz\":\n",
    "                x_vals = np.linspace(self.X_MIN - 10, self.X_MAX + 10, 110)\n",
    "                y_vals = np.linspace(self.Y_MIN - 10, self.Y_MAX + 10, 110)\n",
    "                z_vals = np.linspace(self.Z_MIN - 10, self.Z_MAX + 10, 110)\n",
    "            elif coord_sys.lower() == \"mni\":\n",
    "                x_vals = np.linspace(self.MNI_X_MIN - 10, self.MNI_X_MAX + 10, 110)\n",
    "                y_vals = np.linspace(self.MNI_Y_MIN - 10, self.MNI_Y_MAX + 10, 110)\n",
    "                z_vals = np.linspace(self.MNI_Z_MIN - 10, self.MNI_Z_MAX + 10, 110)\n",
    "\n",
    "            if dim == \"x\":\n",
    "                y_grid, z_grid = np.meshgrid(y_vals, z_vals)\n",
    "                x_grid = np.ones_like(y_grid) * val\n",
    "            elif dim == \"y\":\n",
    "                x_grid, z_grid = np.meshgrid(x_vals, z_vals)\n",
    "                y_grid = np.ones_like(x_grid) * val\n",
    "            elif dim == \"z\":\n",
    "                x_grid, y_grid = np.meshgrid(x_vals, y_vals)\n",
    "                z_grid = np.ones_like(x_grid) * val\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Invalid coordinate dimension. Choose 'x', 'y', or 'z'.\"\n",
    "                )\n",
    "\n",
    "            ax.plot_surface(x_grid, y_grid, z_grid, alpha=0.4, color=color)\n",
    "\n",
    "        gs = gridspec.GridSpec(1, 2, width_ratios=[1, 1.3])\n",
    "        fig = plt.figure(figsize=(18, 12))\n",
    "        ax1 = fig.add_subplot(gs[0], projection=\"3d\")\n",
    "        ax1.view_init(azim=0, elev=270)\n",
    "        ax2 = fig.add_subplot(gs[1], projection=\"3d\")\n",
    "        ax2.view_init(azim=0, elev=0)\n",
    "\n",
    "        if coord_sys.lower() == \"xyz\":\n",
    "            plot_df = self.get_kernel_XYZ(depth)\n",
    "            _create_surface(ax1, self.X_MIN, \"x\", \"r\")\n",
    "            _create_surface(ax1, self.X_MAX, \"x\", \"r\")\n",
    "            _create_surface(ax1, self.Y_MIN, \"y\", \"g\")\n",
    "            _create_surface(ax1, self.Y_MAX, \"y\", \"g\")\n",
    "            _create_surface(ax2, self.Y_MIN, \"y\", \"g\")\n",
    "            _create_surface(ax2, self.Y_MAX, \"y\", \"g\")\n",
    "            _create_surface(ax2, self.Z_MIN, \"z\", \"b\")\n",
    "            _create_surface(ax2, self.Z_MAX, \"z\", \"b\")\n",
    "        elif coord_sys.lower() == \"mni\":\n",
    "            plot_df = self.get_kernel_MNI(depth)\n",
    "            _create_surface(ax1, self.MNI_X_MIN, \"x\", \"r\")\n",
    "            _create_surface(ax1, self.MNI_X_MAX, \"x\", \"r\")\n",
    "            _create_surface(ax1, self.MNI_Y_MIN, \"y\", \"g\")\n",
    "            _create_surface(ax1, self.MNI_Y_MAX, \"y\", \"g\")\n",
    "            _create_surface(ax2, self.MNI_Y_MIN, \"y\", \"g\")\n",
    "            _create_surface(ax2, self.MNI_Y_MAX, \"y\", \"g\")\n",
    "            _create_surface(ax2, self.MNI_Z_MIN, \"z\", \"b\")\n",
    "            _create_surface(ax2, self.MNI_Z_MAX, \"z\", \"b\")\n",
    "        else:\n",
    "            raise ValueError(\"Invalid coordinate system type. Must be 'XYZ' or 'MNI'.\")\n",
    "\n",
    "        ax1.scatter(\n",
    "            list(plot_df[f\"midpoint_x_{coord_sys.upper()}\"].iloc[::2]),\n",
    "            list(plot_df[f\"midpoint_y_{coord_sys.upper()}\"].iloc[::2]),\n",
    "            list(plot_df[f\"midpoint_z_{coord_sys.upper()}\"].iloc[::2]),\n",
    "            color=\"blue\",\n",
    "            s=20,\n",
    "            alpha=1,\n",
    "        )\n",
    "        ax2.scatter(\n",
    "            list(plot_df[f\"midpoint_x_{coord_sys.upper()}\"].iloc[::2]),\n",
    "            list(plot_df[f\"midpoint_y_{coord_sys.upper()}\"].iloc[::2]),\n",
    "            list(plot_df[f\"midpoint_z_{coord_sys.upper()}\"].iloc[::2]),\n",
    "            color=\"blue\",\n",
    "            s=20,\n",
    "        )\n",
    "        ax1.set_xlabel(\n",
    "            f\"{coord_sys.upper()} x-axis (mm)\",\n",
    "            fontweight=\"bold\",\n",
    "            fontsize=14,\n",
    "            labelpad=10,\n",
    "        )\n",
    "        ax1.set_ylabel(\n",
    "            f\"{coord_sys.upper()} y-axis (mm)\",\n",
    "            fontweight=\"bold\",\n",
    "            fontsize=14,\n",
    "            labelpad=15,\n",
    "        )\n",
    "        ax2.set_ylabel(\n",
    "            f\"{coord_sys.upper()} y-axis (mm)\",\n",
    "            fontweight=\"bold\",\n",
    "            fontsize=14,\n",
    "            labelpad=15,\n",
    "        )\n",
    "        ax2.set_zlabel(\n",
    "            f\"{coord_sys.upper()} z-axis (mm)\",\n",
    "            fontweight=\"bold\",\n",
    "            fontsize=14,\n",
    "            labelpad=5,\n",
    "        )\n",
    "        ax1.set_zticks([])\n",
    "        ax1.set_zticklabels([])\n",
    "        ax2.set_xticks([])\n",
    "        ax2.set_xticklabels([])\n",
    "        ax1.set_title(\"Top View\", fontweight=\"bold\", fontsize=18, y=0.96)\n",
    "        ax2.set_title(\"Right View\", fontweight=\"bold\", fontsize=18, y=0.85)\n",
    "        fig.text(\n",
    "            0.43,\n",
    "            0.5,\n",
    "            \"Anterior\",\n",
    "            fontweight=\"bold\",\n",
    "            fontsize=14,\n",
    "            rotation=90,\n",
    "            va=\"center\",\n",
    "        )\n",
    "        fig.text(\n",
    "            0.84,\n",
    "            0.5,\n",
    "            \"Anterior\",\n",
    "            fontweight=\"bold\",\n",
    "            fontsize=14,\n",
    "            rotation=90,\n",
    "            va=\"center\",\n",
    "        )\n",
    "        fig.suptitle(\n",
    "            f\"Kernel Flow {coord_sys.upper()} Coordinates (cortical depth: {depth} mm)\",\n",
    "            fontweight=\"bold\",\n",
    "            fontsize=20,\n",
    "            x=0.465,\n",
    "            y=0.78,\n",
    "        )\n",
    "        fig.subplots_adjust(wspace=0)\n",
    "        plt.show()\n",
    "\n",
    "    def _run_basic_checks(self) -> None:\n",
    "        \"\"\"\n",
    "        Run assertion tests to ensure all steps were successful.\n",
    "        Tolerance = 0.00001 mm (round 5).\n",
    "        \"\"\"\n",
    "        # kernel position scaling\n",
    "        assert (\n",
    "            round(\n",
    "                (self.FLOW_X_MAX - self.FLOW_X_MIN) * self._get_scaling_factor(\"x\"), 5\n",
    "            )\n",
    "        ) == self.X_MAX_ADJ - self.X_MIN_ADJ, (\n",
    "            \"Test failed: FLOW_X did not scale correctly.\"\n",
    "        )\n",
    "        assert (\n",
    "            round(\n",
    "                (self.FLOW_Y_MAX - self.FLOW_Y_MIN) * self._get_scaling_factor(\"y\"), 5\n",
    "            )\n",
    "        ) == self.Y_MAX_ADJ - self.Y_MIN_ADJ, (\n",
    "            \"Test failed: FLOW_Y did not scale correctly.\"\n",
    "        )\n",
    "        assert (\n",
    "            round(\n",
    "                (self.FLOW_Z_MAX - self.FLOW_Z_MIN) * self._get_scaling_factor(\"z\"), 5\n",
    "            )\n",
    "        ) == self.Z_MAX_ADJ - self.Z_MIN_ADJ, (\n",
    "            \"Test failed: FLOW_Z did not scale correctly.\"\n",
    "        )\n",
    "\n",
    "        # kernel DataFrame scaling\n",
    "        sd_df_scaled = self._scale_kernel_XYZ()\n",
    "        assert (\n",
    "            round(\n",
    "                sd_df_scaled[\"midpoint_x_pos\"].max()\n",
    "                - sd_df_scaled[\"midpoint_x_pos\"].min(),\n",
    "                5,\n",
    "            )\n",
    "            == self.X_MAX_ADJ - self.X_MIN_ADJ\n",
    "        ), \"Test failed. Flow DataFrame x-pos did not scale correctly.\"\n",
    "        assert (\n",
    "            round(\n",
    "                sd_df_scaled[\"midpoint_y_pos\"].max()\n",
    "                - sd_df_scaled[\"midpoint_y_pos\"].min(),\n",
    "                5,\n",
    "            )\n",
    "            == self.Y_MAX_ADJ - self.Y_MIN_ADJ\n",
    "        ), \"Test failed. Flow DataFrame y-pos did not scale correctly.\"\n",
    "        assert (\n",
    "            round(\n",
    "                sd_df_scaled[\"midpoint_z_pos\"].max()\n",
    "                - sd_df_scaled[\"midpoint_z_pos\"].min(),\n",
    "                5,\n",
    "            )\n",
    "            == self.Z_MAX_ADJ - self.Z_MIN_ADJ\n",
    "        ), \"Test failed. Flow DataFrame z-pos did not scale correctly.\"\n",
    "\n",
    "        # kernel DataFrame alignment\n",
    "        sd_df_aligned = self._align_kernel_XYZ()\n",
    "        assert (\n",
    "            round(sd_df_aligned[\"midpoint_x_XYZ\"].min(), 5) == self.X_MIN_ADJ\n",
    "        ), \"Test failed. Flow DataFrame x-pos min did not align correctly.\"\n",
    "        assert (\n",
    "            round(sd_df_aligned[\"midpoint_y_XYZ\"].min(), 5) == self.Y_MIN_ADJ\n",
    "        ), \"Test failed. Flow DataFrame y-pos min did not align correctly.\"\n",
    "        assert (\n",
    "            round(sd_df_aligned[\"midpoint_z_XYZ\"].min(), 5) == self.Z_MIN_ADJ\n",
    "        ), \"Test failed. Flow DataFrame z-pos min did not align correctly.\"\n",
    "        assert (\n",
    "            round(sd_df_aligned[\"midpoint_x_XYZ\"].max(), 5) == self.X_MAX_ADJ\n",
    "        ), \"Test failed. Flow DataFrame x-pos max did not align correctly.\"\n",
    "        assert (\n",
    "            round(sd_df_aligned[\"midpoint_y_XYZ\"].max(), 5) == self.Y_MAX_ADJ\n",
    "        ), \"Test failed. Flow DataFrame y-pos max did not align correctly.\"\n",
    "        assert (\n",
    "            round(sd_df_aligned[\"midpoint_z_XYZ\"].max(), 5) == self.Z_MAX_ADJ\n",
    "        ), \"Test failed. Flow DataFrame z-pos max did not align correctly.\"\n",
    "\n",
    "    def _run_translation_check(\n",
    "        self, df: pd.DataFrame, depth: Tuple[int, float] = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Run an assertion test to check Kernel XYZ translation.\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            round(df[\"midpoint_x_XYZ\"].min(), 5) == self.X_MIN_ADJ + depth\n",
    "        ), \"Test failed. Flow DataFrame x-pos min did not translate correctly.\"\n",
    "        assert (\n",
    "            round(df[\"midpoint_y_XYZ\"].min(), 5) == self.Y_MIN_ADJ + depth\n",
    "        ), \"Test failed. Flow DataFrame y-pos min did not translate correctly.\"\n",
    "        assert (\n",
    "            round(df[\"midpoint_z_XYZ\"].min(), 5) == self.Z_MIN_ADJ + depth\n",
    "        ), \"Test failed. Flow DataFrame z-pos min did not translate correctly.\"\n",
    "        assert (\n",
    "            round(df[\"midpoint_x_XYZ\"].max(), 5) == self.X_MAX_ADJ - depth\n",
    "        ), \"Test failed. Flow DataFrame x-pos max did not translate correctly.\"\n",
    "        assert (\n",
    "            round(df[\"midpoint_y_XYZ\"].max(), 5) == self.Y_MAX_ADJ - depth\n",
    "        ), \"Test failed. Flow DataFrame y-pos max did not translate correctly.\"\n",
    "        assert (\n",
    "            round(df[\"midpoint_z_XYZ\"].max(), 5) == self.Z_MAX_ADJ - depth\n",
    "        ), \"Test failed. Flow DataFrame z-pos max did not translate correctly.\"\n",
    "\n",
    "\n",
    "class Process_Flow:\n",
    "    \"\"\"\n",
    "    This class contains functions for processing Kernel Flow data.\n",
    "    Wrapper around an snirf.Snirf object.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filepath: str) -> None:\n",
    "        \"\"\"\n",
    "        Initialize by loading SNIRF file.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to SNIRF file.\n",
    "        \"\"\"\n",
    "        self.data_fun = Data_Functions()\n",
    "        self.snirf_file = self.load_snirf(filepath)\n",
    "\n",
    "        self.missing_detector_pos_2d = [\n",
    "            [0.2700519522879849, 0.92534462173171],\n",
    "            [0.2100404073350992, 0.9599923033647436],\n",
    "            [0.1500288623822143, 0.92534462173171],\n",
    "            [0.1500288623822143, 0.856049258465643],\n",
    "            [0.2100404073350992, 0.8214015768326095],\n",
    "            [0.2700519522879849, 0.856049258465643],\n",
    "        ]\n",
    "        self.missing_source_pos_2d = [0.2100404073350983, 0.8906969400986755]\n",
    "        self.missing_detector_pos_3d = [\n",
    "            [34.18373257128052, 83.84749436111261, -3.421772079425661],\n",
    "            [24.89193921324638, 87.59280827807989, -3.877662542873584],\n",
    "            [19.49960518952535, 88.52633022589306, 4.53462776618961],\n",
    "            [23.69484819349888, 86.5963118571706, 13.38774165295894],\n",
    "            [32.93421777049451, 82.87888296072012, 13.83928277924401],\n",
    "            [37.86338484008788, 80.87503761567585, 5.394829563438814],\n",
    "        ]\n",
    "        self.missing_source_pos_3d = [\n",
    "            28.65886271209007,\n",
    "            84.52123706248807,\n",
    "            4.746746612880643,\n",
    "        ]\n",
    "        self.missing_measurement_list_data = {\n",
    "            \"measurement_list_index\": [float(\"NaN\")] * 12,\n",
    "            \"data_type\": [99999] * 12,\n",
    "            \"data_type_index\": [\"HbO\", \"HbR\"] * 6,\n",
    "            \"detector_index\": [\n",
    "                307,\n",
    "                307,\n",
    "                308,\n",
    "                308,\n",
    "                309,\n",
    "                309,\n",
    "                310,\n",
    "                310,\n",
    "                311,\n",
    "                311,\n",
    "                312,\n",
    "                312,\n",
    "            ],\n",
    "            \"source_index\": [0] * 12,\n",
    "        }\n",
    "\n",
    "    def load_snirf(self, filepath: str) -> snirf.Snirf:\n",
    "        \"\"\"\n",
    "        Load SNIRF file.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            snirf.Snirf: SNIRF file object.\n",
    "        \"\"\"\n",
    "        return snirf.Snirf(filepath, \"r+\", dynamic_loading=True)\n",
    "\n",
    "    def get_time_origin(\n",
    "        self, fmt: str = \"datetime\", offset=True\n",
    "    ) -> Union[datetime.datetime, float]:\n",
    "        \"\"\"\n",
    "        Get the time origin (start time) from the SNIRF file.\n",
    "\n",
    "        Args:\n",
    "            fmt (str, optional): Format to get the time origin in: \"datetime\" or \"timestamp\". Defaults to \"datetime\".\n",
    "            offset (bool): Offset the datetime by 4 hours. Defaults to True.\n",
    "\n",
    "        Raises:\n",
    "            Exception: Invalid fmt argument.\n",
    "\n",
    "        Returns:\n",
    "            Union[datetime.datetime, float]:\n",
    "                datetime.datetime: Time origin datetime.\n",
    "                -or-\n",
    "                float: Time origin timestamp.\n",
    "        \"\"\"\n",
    "        start_date = self.snirf_file.nirs[0].metaDataTags.MeasurementDate\n",
    "        start_time = self.snirf_file.nirs[0].metaDataTags.MeasurementTime\n",
    "        start_str = start_date + \" \" + start_time\n",
    "        if offset:\n",
    "            time_origin = datetime.datetime.strptime(\n",
    "                start_str, \"%Y-%m-%d %H:%M:%S\"\n",
    "            ) - datetime.timedelta(\n",
    "                hours=4\n",
    "            )  # 4 hour offset\n",
    "        else:\n",
    "            time_origin = datetime.datetime.strptime(start_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "        if fmt.lower() == \"datetime\":\n",
    "            return time_origin\n",
    "        elif fmt.lower() == \"timestamp\":\n",
    "            return datetime.datetime.timestamp(time_origin)\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"Invalid 'fmt' argument. Must be 'datetime' or 'timestamp'.\"\n",
    "            )\n",
    "\n",
    "    def get_subject_ID(self) -> str:\n",
    "        \"\"\"\n",
    "        Get the subject ID from the SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            str: Subject ID.\n",
    "        \"\"\"\n",
    "        return self.snirf_file.nirs[0].metaDataTags.SubjectID\n",
    "\n",
    "    def get_time_rel(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get the relative time array from the SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Relative time array.\n",
    "        \"\"\"\n",
    "        return self.snirf_file.nirs[0].data[0].time\n",
    "\n",
    "    def get_time_abs(self, fmt: str = \"datetime\") -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Convert relative time array into an absolute time array.\n",
    "\n",
    "        Args:\n",
    "            fmt (str, optional): Format to get the time array in: \"datetime\" or \"timestamp\". Defaults to \"datetime\".\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Absolute time array.\n",
    "        \"\"\"\n",
    "        time_rel = self.get_time_rel()\n",
    "        if fmt.lower() == \"datetime\":\n",
    "            time_origin_dt = self.get_time_origin(\"datetime\")\n",
    "            return np.array(\n",
    "                [\n",
    "                    datetime.timedelta(seconds=time_rel[i]) + time_origin_dt\n",
    "                    for i in range(len(time_rel))\n",
    "                ]\n",
    "            )\n",
    "        elif fmt.lower() == \"timestamp\":\n",
    "            time_origin_ts = self.get_time_origin(\"timestamp\")\n",
    "            return time_rel + time_origin_ts\n",
    "\n",
    "    def get_data(\n",
    "        self, fmt: str = \"array\", cols: list[int | list | tuple] = None\n",
    "    ) -> Union[np.ndarray, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Get timeseries data from the SNIRF file.\n",
    "\n",
    "        Args:\n",
    "            fmt (str): Format of data (np.ndarray or pd.DataFrame). Defaults to \"array\".\n",
    "            cols (list[int | list | tuple]): Data cols to select. Single col, list of cols, or slice of cols.\n",
    "                                             Defaults to None (all columns).\n",
    "\n",
    "        Raises:\n",
    "            Exception: Invalid fmt argument.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Timeseries data array.\n",
    "        \"\"\"\n",
    "        if cols or cols == 0:\n",
    "            if isinstance(cols, tuple):\n",
    "                data = (\n",
    "                    self.snirf_file.nirs[0].data[0].dataTimeSeries[:, cols[0] : cols[1]]\n",
    "                )\n",
    "            else:\n",
    "                data = self.snirf_file.nirs[0].data[0].dataTimeSeries[:, cols]\n",
    "        else:\n",
    "            data = self.snirf_file.nirs[0].data[0].dataTimeSeries\n",
    "\n",
    "        if \"array\" in fmt.lower():\n",
    "            return data\n",
    "        elif \"dataframe\" in fmt.lower():\n",
    "            return pd.DataFrame(data)\n",
    "        else:\n",
    "            raise Exception(\"Invalid fmt argument. Must be 'array' or 'dataframe'.\")\n",
    "\n",
    "    def get_source_pos(self, dim: str, add_missing: bool = False) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get the 2D or 3D source position array.\n",
    "\n",
    "        Args:\n",
    "            dim (str): Position data dimension \"2D\" or \"3D\".\n",
    "            add_missing (bool): Add missing source data. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: 2D or 3D source position array.\n",
    "        \"\"\"\n",
    "        if dim.lower() == \"2d\":\n",
    "            source_pos_array_og = self.snirf_file.nirs[0].probe.sourcePos2D\n",
    "            if add_missing:\n",
    "                source_pos_array = np.vstack(\n",
    "                    [np.array(self.missing_source_pos_2d), source_pos_array_og]\n",
    "                )\n",
    "                return source_pos_array\n",
    "            else:\n",
    "                return source_pos_array_og\n",
    "        elif dim.lower() == \"3d\":\n",
    "            source_pos_array_og = self.snirf_file.nirs[0].probe.sourcePos3D\n",
    "            if add_missing:\n",
    "                source_pos_array = np.vstack(\n",
    "                    [np.array(self.missing_source_pos_3d), source_pos_array_og]\n",
    "                )\n",
    "                return source_pos_array\n",
    "            else:\n",
    "                return source_pos_array_og\n",
    "\n",
    "    def get_detector_pos(self, dim: str, add_missing: bool = False) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get the 2D or 3D detector position array.\n",
    "\n",
    "        Args:\n",
    "            dim (str): Position data dimension \"2D\" or \"3D\".\n",
    "            add_missing (bool): Add missing detector data. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: 2D or 3D detector position array.\n",
    "        \"\"\"\n",
    "        if dim.lower() == \"2d\":\n",
    "            detector_pos_array_og = self.snirf_file.nirs[0].probe.detectorPos2D\n",
    "            if add_missing:\n",
    "                detector_pos_array = np.vstack(\n",
    "                    [np.array(self.missing_detector_pos_2d), detector_pos_array_og]\n",
    "                )\n",
    "                return detector_pos_array\n",
    "            else:\n",
    "                return detector_pos_array_og\n",
    "        elif dim.lower() == \"3d\":\n",
    "            detector_pos_array_og = self.snirf_file.nirs[0].probe.detectorPos3D\n",
    "            if add_missing:\n",
    "                detector_pos_array_og = self.snirf_file.nirs[0].probe.detectorPos3D\n",
    "                detector_pos_array = np.vstack(\n",
    "                    [np.array(self.missing_detector_pos_3d), detector_pos_array_og]\n",
    "                )\n",
    "                return detector_pos_array\n",
    "            else:\n",
    "                return detector_pos_array_og\n",
    "\n",
    "    def get_measurement_list(self) -> np.array:\n",
    "        \"\"\"\n",
    "        Get the data measurement list.\n",
    "\n",
    "        Returns:\n",
    "            np.array: Data measurement list array.\n",
    "        \"\"\"\n",
    "        return self.snirf_file.nirs[0].data[0].measurementList\n",
    "\n",
    "    def get_source_labels(self, add_missing: bool = False) -> np.array:\n",
    "        \"\"\"\n",
    "        Get the source labels.\n",
    "\n",
    "        Args:\n",
    "            add_missing (bool): Add missing source label. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            np.array: Source label array.\n",
    "        \"\"\"\n",
    "        source_labels_og = self.snirf_file.nirs[0].probe.sourceLabels\n",
    "        if add_missing:\n",
    "            missing_source_label = \"S00\"\n",
    "            source_labels = np.insert(source_labels_og, 0, missing_source_label)\n",
    "            return source_labels\n",
    "        else:\n",
    "            return source_labels_og\n",
    "\n",
    "    def get_detector_labels(self, add_missing: bool = False) -> np.array:\n",
    "        \"\"\"\n",
    "        Get the detector labels.\n",
    "\n",
    "        Args:\n",
    "            add_missing (bool): Add missing detector labels. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            np.array: Detector label array.\n",
    "        \"\"\"\n",
    "        detector_labels_og = self.snirf_file.nirs[0].probe.detectorLabels\n",
    "        if add_missing:\n",
    "            missing_detector_labels = [\n",
    "                \"D00d0\",\n",
    "                \"D00d1\",\n",
    "                \"D00d2\",\n",
    "                \"D00d3\",\n",
    "                \"D00d4\",\n",
    "                \"D00d5\",\n",
    "            ]\n",
    "            detector_labels = np.insert(detector_labels_og, 0, missing_detector_labels)\n",
    "            return detector_labels\n",
    "        else:\n",
    "            return detector_labels_og\n",
    "\n",
    "    def get_marker_df(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get a DataFrame of marker data from the \"stim\" part of the SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Marker \"stim\" data.\n",
    "        \"\"\"\n",
    "        marker_data = self.snirf_file.nirs[0].stim[0].data\n",
    "        marker_data_cols = self.snirf_file.nirs[0].stim[0].dataLabels\n",
    "        return pd.DataFrame(marker_data, columns=marker_data_cols)\n",
    "\n",
    "    def get_unique_data_types(self) -> list:\n",
    "        \"\"\"\n",
    "        Get unique data types from the SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            list: Unique data types.\n",
    "        \"\"\"\n",
    "        data_types = []\n",
    "        for i in range(len(self.snirf_file.nirs[0].data[0].measurementList)):\n",
    "            data_type = self.snirf_file.nirs[0].data[0].measurementList[i].dataType\n",
    "            if data_type not in data_types:\n",
    "                data_types.append(data_type)\n",
    "        return data_types\n",
    "\n",
    "    def get_data_type_label(self, channel_num: int) -> str:\n",
    "        \"\"\"\n",
    "        Get the data type label for a channel(s).\n",
    "\n",
    "        Args:\n",
    "            channel_num (int): Channel number to get the data type label of.\n",
    "\n",
    "        Returns:\n",
    "            str: Data type label of the channel.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            self.snirf_file.nirs[0].data[0].measurementList[channel_num].dataTypeLabel\n",
    "        )\n",
    "\n",
    "    def get_unique_data_type_labels(self) -> list:\n",
    "        \"\"\"\n",
    "        Get unique data type labels from the SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            list: Unique data type labels.\n",
    "        \"\"\"\n",
    "        data_type_labels = []\n",
    "        for i in range(len(self.snirf_file.nirs[0].data[0].measurementList)):\n",
    "            data_type_label = (\n",
    "                self.snirf_file.nirs[0].data[0].measurementList[i].dataTypeLabel\n",
    "            )\n",
    "            if data_type_label not in data_type_labels:\n",
    "                data_type_labels.append(data_type_label)\n",
    "        return data_type_labels\n",
    "\n",
    "    def create_source_dict(self) -> dict:\n",
    "        \"\"\"\n",
    "        Count the occurrences of each source index.\n",
    "\n",
    "        Returns:\n",
    "            dict: Counts for each source index.\n",
    "        \"\"\"\n",
    "        source_dict = {}\n",
    "        for i in range(len(self.snirf_file.nirs[0].data[0].measurementList)):\n",
    "            source = self.snirf_file.nirs[0].data[0].measurementList[i].sourceIndex\n",
    "            source_dict[source] = source_dict.get(source, 0) + 1\n",
    "        source_dict = self.data_fun.sort_dict(source_dict, \"keys\")\n",
    "        return source_dict\n",
    "\n",
    "    def create_detector_dict(self) -> dict:\n",
    "        \"\"\"\n",
    "        Count the occurrences of each detector index.\n",
    "\n",
    "        Returns:\n",
    "            dict: Counts for each detector index.\n",
    "        \"\"\"\n",
    "        detector_dict = {}\n",
    "        for i in range(len(self.snirf_file.nirs[0].data[0].measurementList)):\n",
    "            detector = self.snirf_file.nirs[0].data[0].measurementList[i].detectorIndex\n",
    "            detector_dict[detector] = detector_dict.get(detector, 0) + 1\n",
    "        detector_dict = self.data_fun.sort_dict(detector_dict, \"keys\")\n",
    "        return detector_dict\n",
    "\n",
    "    def create_measurement_list_df(self, add_missing: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create a DataFrame with all the data measurement list information.\n",
    "\n",
    "        Args:\n",
    "            add_missing (bool): Add missing detector data. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Data measurement list DataFrame.\n",
    "        \"\"\"\n",
    "        measurement_list = self.get_measurement_list()\n",
    "        dict_list = []\n",
    "\n",
    "        for i in range(len(measurement_list)):\n",
    "            measurement_list_i = measurement_list[i]\n",
    "            measurement_dict = {}\n",
    "            measurement_dict[\"measurement_list_index\"] = (\n",
    "                i + 1\n",
    "            )  # TODO if missing, start at detector_index 7\n",
    "            measurement_dict[\"data_type\"] = measurement_list_i.dataType\n",
    "            measurement_dict[\"data_type_index\"] = measurement_list_i.dataTypeLabel\n",
    "            measurement_dict[\"detector_index\"] = measurement_list_i.detectorIndex\n",
    "            measurement_dict[\"source_index\"] = measurement_list_i.sourceIndex\n",
    "            dict_list.append(measurement_dict)\n",
    "\n",
    "        measurement_list_df = pd.DataFrame(dict_list)\n",
    "\n",
    "        if add_missing:\n",
    "            missing_data_df = pd.DataFrame(self.missing_measurement_list_data)\n",
    "            measurement_list_df = pd.concat(\n",
    "                [missing_data_df, measurement_list_df], ignore_index=True\n",
    "            )\n",
    "            measurement_list_df[\"measurement_list_index\"] = measurement_list_df[\n",
    "                \"measurement_list_index\"\n",
    "            ].astype(pd.Int64Dtype())\n",
    "        return measurement_list_df\n",
    "\n",
    "    def create_source_df(self, dim: str, add_missing: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create a DataFrame with the source labels and 2D or 3D source positions.\n",
    "\n",
    "        Args:\n",
    "            dim (str): Position data dimension \"2D\" or \"3D\".\n",
    "            add_missing (bool): Add missing detector data. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Source labels and positions.\n",
    "        \"\"\"\n",
    "        source_labels = self.get_source_labels(add_missing)\n",
    "        if dim.lower() == \"2d\":\n",
    "            source_pos_2d = self.get_source_pos(dim, add_missing)\n",
    "            source_data = [\n",
    "                (label, *lst) for label, lst in zip(source_labels, source_pos_2d)\n",
    "            ]\n",
    "            source_df = pd.DataFrame(\n",
    "                source_data, columns=[\"source_label\", \"source_x_pos\", \"source_y_pos\"]\n",
    "            )\n",
    "        elif dim.lower() == \"3d\":\n",
    "            source_pos_3d = self.get_source_pos(dim, add_missing)\n",
    "            source_data = [\n",
    "                (label, *lst) for label, lst in zip(source_labels, source_pos_3d)\n",
    "            ]\n",
    "            source_df = pd.DataFrame(\n",
    "                source_data,\n",
    "                columns=[\n",
    "                    \"source_label\",\n",
    "                    \"source_x_pos\",\n",
    "                    \"source_y_pos\",\n",
    "                    \"source_z_pos\",\n",
    "                ],\n",
    "            )\n",
    "        # NOTE: Kernel changed source and detector label formats after a certain date\n",
    "        try:\n",
    "            f = lambda x: int(x.lstrip(\"S\"))\n",
    "            source_df.insert(1, \"source_index\", source_df[\"source_label\"].apply(f))\n",
    "        except ValueError:  # Format changed for participants 12+\n",
    "            f = lambda x: int(x[1:4].lstrip(\"0\"))\n",
    "            source_df.insert(1, \"source_index\", source_df[\"source_label\"].apply(f))\n",
    "        return source_df\n",
    "\n",
    "    def create_detector_df(self, dim: str, add_missing: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create a DataFrame with the detector labels and 2D or 3D detector positions.\n",
    "\n",
    "        Args:\n",
    "            dim (str): Position data dimension \"2D\" or \"3D\".\n",
    "            add_missing (bool): Add missing detector data. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Detector labels and positions.\n",
    "        \"\"\"\n",
    "        detector_labels = self.get_detector_labels(add_missing)\n",
    "        if dim.lower() == \"2d\":\n",
    "            detector_pos_2d = self.get_detector_pos(dim, add_missing)\n",
    "            detector_data = [\n",
    "                (label, *lst) for label, lst in zip(detector_labels, detector_pos_2d)\n",
    "            ]\n",
    "            detector_df = pd.DataFrame(\n",
    "                detector_data,\n",
    "                columns=[\"detector_label\", \"detector_x_pos\", \"detector_y_pos\"],\n",
    "            )\n",
    "        elif dim.lower() == \"3d\":\n",
    "            detector_pos_3d = self.get_detector_pos(dim, add_missing)\n",
    "            detector_data = [\n",
    "                (label, *lst) for label, lst in zip(detector_labels, detector_pos_3d)\n",
    "            ]\n",
    "            detector_df = pd.DataFrame(\n",
    "                detector_data,\n",
    "                columns=[\n",
    "                    \"detector_label\",\n",
    "                    \"detector_x_pos\",\n",
    "                    \"detector_y_pos\",\n",
    "                    \"detector_z_pos\",\n",
    "                ],\n",
    "            )\n",
    "        # NOTE: Kernel changed source and detector label formats after a certain date\n",
    "        if len(detector_df[\"detector_label\"][7]) == 5:\n",
    "            f = lambda x: int(x[1:3])\n",
    "        elif (\n",
    "            len(detector_df[\"detector_label\"][7]) == 7\n",
    "        ):  # Format changed for participants 12+\n",
    "            f = lambda x: int(x[2:4])\n",
    "\n",
    "        detector_df.insert(1, \"source_index\", detector_df[\"detector_label\"].apply(f))\n",
    "        if add_missing:\n",
    "            detector_index_col = []\n",
    "            for i in range(307, 313):\n",
    "                detector_index_col.append(i)\n",
    "            for i in range(1, detector_df.shape[0] - 5):\n",
    "                detector_index_col.append(i)\n",
    "            detector_df.insert(1, \"detector_index\", detector_index_col)\n",
    "        else:\n",
    "            detector_df.insert(1, \"detector_index\", range(1, detector_df.shape[0] + 1))\n",
    "        return detector_df\n",
    "\n",
    "    def create_source_detector_df(\n",
    "        self,\n",
    "        dim: str,\n",
    "        add_missing: bool = False,\n",
    "        midpoint_only: bool = False,\n",
    "        MNI: bool = False,\n",
    "        brain_regions: bool = False,\n",
    "        channels: Union[List[int], int] = None,\n",
    "        depth: Union[int, float] = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create a DataFrame with the source and detector information for the inter-module channels.\n",
    "\n",
    "        Args:\n",
    "            dim (str): Position data dimension \"2D\" or \"3D\".\n",
    "            add_missing (bool): Add missing detector data. Defaults to False.\n",
    "            midpoint_only (bool): Include only source/detector midpoint coordinate dimensions. Default to False.\n",
    "            MNI (bool): Include MNI coordinate system columns. Defaults to False.\n",
    "            brain_regions (bool): Include AAL and BA brain region columns. Defaults to False.\n",
    "            channels (Union[List[int], int]): Return only specific channel(s). Defaults to None.\n",
    "            depth (Union[int, float], optional): Depth into the brain. Defaults to None (brain surface).\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Source and detector information for inter-module channels.\n",
    "        \"\"\"\n",
    "        measurement_list_df = self.create_measurement_list_df(add_missing)\n",
    "        if dim.lower() == \"2d\":\n",
    "            source_df = self.create_source_df(\"2D\", add_missing)\n",
    "            detector_df = self.create_detector_df(\"2D\", add_missing)\n",
    "        elif dim.lower() == \"3d\":\n",
    "            source_df = self.create_source_df(\"3D\", add_missing)\n",
    "            detector_df = self.create_detector_df(\"3D\", add_missing)\n",
    "        source_merge = pd.merge(measurement_list_df, source_df, on=\"source_index\")\n",
    "        merged_source_detector_df = pd.merge(\n",
    "            source_merge, detector_df, on=[\"detector_index\", \"source_index\"]\n",
    "        )\n",
    "        source_detector_df = merged_source_detector_df.copy()\n",
    "        source_detector_df.insert(\n",
    "            0, \"channel_num\", source_detector_df[\"measurement_list_index\"] - 1\n",
    "        )\n",
    "\n",
    "        if dim.lower() == \"2d\":\n",
    "            if isinstance(channels, int):\n",
    "                source_detector_df = source_detector_df[\n",
    "                    source_detector_df[\"channel_num\"] == channels\n",
    "                ].copy()\n",
    "            elif isinstance(channels, list):\n",
    "                source_detector_df = source_detector_df[\n",
    "                    source_detector_df[\"channel_num\"].isin(channels)\n",
    "                ].copy()\n",
    "        elif dim.lower() == \"3d\":\n",
    "            source_detector_df[\n",
    "                [\"midpoint_x_pos\", \"midpoint_y_pos\", \"midpoint_z_pos\"]\n",
    "            ] = source_detector_df.apply(\n",
    "                lambda row: self.get_midpoint(\n",
    "                    (row[\"source_x_pos\"], row[\"source_y_pos\"], row[\"source_z_pos\"]),\n",
    "                    (\n",
    "                        row[\"detector_x_pos\"],\n",
    "                        row[\"detector_y_pos\"],\n",
    "                        row[\"detector_z_pos\"],\n",
    "                    ),\n",
    "                ),\n",
    "                axis=1,\n",
    "                result_type=\"expand\",\n",
    "            )\n",
    "\n",
    "            if MNI or brain_regions:\n",
    "                # add source/detector MNI coordinates\n",
    "                FC = Flow_Coordinates(source_detector_df)\n",
    "                source_detector_df = FC.create_source_detector_adj(depth)\n",
    "            if isinstance(channels, int):\n",
    "                source_detector_df = source_detector_df[\n",
    "                    source_detector_df[\"channel_num\"] == channels\n",
    "                ].copy()\n",
    "            elif isinstance(channels, list):\n",
    "                source_detector_df = source_detector_df[\n",
    "                    source_detector_df[\"channel_num\"].isin(channels)\n",
    "                ].copy()\n",
    "            if brain_regions:\n",
    "                # load R script files here to improve performance\n",
    "                with open(\n",
    "                    os.path.join(\n",
    "                        os.getcwd(), \"label4MRI\", \"R\", \"mni_to_region_index.R\"\n",
    "                    ),\n",
    "                    \"r\",\n",
    "                ) as file:\n",
    "                    mni_to_region_index_code = \"\".join(file.readlines())\n",
    "                with open(\n",
    "                    os.path.join(os.getcwd(), \"label4MRI\", \"R\", \"mni_to_region_name.R\"),\n",
    "                    \"r\",\n",
    "                ) as file:\n",
    "                    mni_to_region_name_code = \"\".join(file.readlines())\n",
    "                # evaluate R code\n",
    "                metadata_path = os.path.join(\n",
    "                    os.getcwd(), \"label4MRI\", \"data\", \"metadata.RData\"\n",
    "                )\n",
    "                load_rdata = robjects.r[\"load\"]\n",
    "                load_rdata(metadata_path)\n",
    "                robjects.r(mni_to_region_index_code)\n",
    "                robjects.r(mni_to_region_name_code)\n",
    "                # R function as Python callable\n",
    "                self.mni_to_region_name = robjects.globalenv[\"mni_to_region_name\"]\n",
    "\n",
    "                source_detector_df[\n",
    "                    [\"AAL_distance\", \"AAL_region\", \"BA_distance\", \"BA_region\"]\n",
    "                ] = source_detector_df.apply(\n",
    "                    lambda row: self.MNI_to_region(\n",
    "                        row[\"midpoint_x_MNI\"],\n",
    "                        row[\"midpoint_y_MNI\"],\n",
    "                        row[\"midpoint_z_MNI\"],\n",
    "                    ),\n",
    "                    axis=1,\n",
    "                    result_type=\"expand\",\n",
    "                )\n",
    "            if midpoint_only:\n",
    "                source_detector_df = source_detector_df.drop(\n",
    "                    columns=[\n",
    "                        \"source_x_pos\",\n",
    "                        \"source_y_pos\",\n",
    "                        \"source_z_pos\",\n",
    "                        \"detector_x_pos\",\n",
    "                        \"detector_y_pos\",\n",
    "                        \"detector_z_pos\",\n",
    "                    ]\n",
    "                )\n",
    "        return source_detector_df\n",
    "\n",
    "    def get_midpoint(\n",
    "        self, point1: Tuple[float, float, float], point2: Tuple[float, float, float]\n",
    "    ) -> Tuple[float, float, float]:\n",
    "        \"\"\"\n",
    "        Get the midpoint between two x, y, z coordinate points (source and detector).\n",
    "\n",
    "        Args:\n",
    "            point1 (Tuple[float, float, float]): x, y, z coordinates of the source.\n",
    "            point2 (Tuple[float, float, float]): x, y, z coordinates of the detector.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[float, float, float]: x, y, z coordinates of the source/detector midpoint.\n",
    "        \"\"\"\n",
    "        x_mid = (point1[0] + point2[0]) / 2\n",
    "        y_mid = (point1[1] + point2[1]) / 2\n",
    "        z_mid = (point1[2] + point2[2]) / 2\n",
    "        return x_mid, y_mid, z_mid\n",
    "\n",
    "    def MNI_to_region(\n",
    "        self, mni_x: float, mni_y: float, mni_z: float, print_results: bool = False\n",
    "    ) -> Tuple[float, str, float, str]:\n",
    "        \"\"\"\n",
    "        Convert MNI coordinates to the corresponding Automated Anatomical Labeling (AAL) and\n",
    "        Brodmann area (BA) including the distance from the nearest brain region.\n",
    "        Adapted from https://github.com/yunshiuan/label4MRI.\n",
    "\n",
    "        Args:\n",
    "            mni_x (float): x MNI coordinate.\n",
    "            mni_y (float): y MNI coordinate.\n",
    "            mni_z (float): z MNI coordinate.\n",
    "            print_results (bool): Print the results. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[float, str, float, str]: Distance from AAL brain region, AAL brain region,\n",
    "                                           distance from BA brain region, and BA region.\n",
    "        \"\"\"\n",
    "        if hasattr(self.__class__, \"mni_to_region_name\"):\n",
    "            mni_to_region_name = self.mni_to_region_name\n",
    "        else:\n",
    "            # load R script files\n",
    "            with open(\n",
    "                os.path.join(os.getcwd(), \"label4MRI\", \"R\", \"mni_to_region_index.R\"),\n",
    "                \"r\",\n",
    "            ) as file:\n",
    "                mni_to_region_index_code = \"\".join(file.readlines())\n",
    "            with open(\n",
    "                os.path.join(os.getcwd(), \"label4MRI\", \"R\", \"mni_to_region_name.R\"), \"r\"\n",
    "            ) as file:\n",
    "                mni_to_region_name_code = \"\".join(file.readlines())\n",
    "            # evaluate R code\n",
    "            metadata_path = os.path.join(\n",
    "                os.getcwd(), \"label4MRI\", \"data\", \"metadata.RData\"\n",
    "            )\n",
    "            load_rdata = robjects.r[\"load\"]\n",
    "            load_rdata(metadata_path)\n",
    "            robjects.r(mni_to_region_index_code)\n",
    "            robjects.r(mni_to_region_name_code)\n",
    "            # R function as Python callable\n",
    "            mni_to_region_name = robjects.globalenv[\"mni_to_region_name\"]\n",
    "\n",
    "        result = mni_to_region_name(float(mni_x), float(mni_y), float(mni_z))\n",
    "\n",
    "        aal_distance = result.rx2(\"aal.distance\")\n",
    "        aal_label = result.rx2(\"aal.label\")\n",
    "        ba_distance = result.rx2(\"ba.distance\")\n",
    "        ba_label = result.rx2(\"ba.label\")\n",
    "\n",
    "        # convert R vector objects\n",
    "        aal_distance = round(list(aal_distance)[0], 2)\n",
    "        aal_label = list(aal_label)[0]\n",
    "        ba_distance = round(list(ba_distance)[0], 2)\n",
    "        ba_label = list(ba_label)[0]\n",
    "\n",
    "        if print_results:\n",
    "            print(f\"AAL distance: {aal_distance}\")\n",
    "            print(f\"AAL region: {aal_label}\")\n",
    "            print(f\"BA distance: {ba_distance}\")\n",
    "            print(f\"BA region: {ba_label}\")\n",
    "\n",
    "        return aal_distance, aal_label, ba_distance, ba_label\n",
    "\n",
    "    def create_flow_atlas(self, depth: Union[int, float] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create an atlas of Kernel Flow source/detector locations with corresponding brain regions.\n",
    "\n",
    "        Args:\n",
    "            depth (Union[int, float], optional): Depth into the brain. Defaults to None (brain surface).\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame with brain regions for all sources and detectors\n",
    "        \"\"\"\n",
    "        if depth is None:\n",
    "            depth = 0\n",
    "        atlas_df = self.create_source_detector_df(\n",
    "            \"3D\", add_missing=True, brain_regions=True, depth=depth\n",
    "        )\n",
    "        filename = f\"kernel_flow_atlas_depth_{depth}.csv\"\n",
    "        filedir = os.path.join(os.getcwd(), \"processed_data\", \"flow\")\n",
    "        filepath = os.path.join(filedir, filename)\n",
    "        atlas_df.to_csv(filepath, index=False)\n",
    "        return atlas_df\n",
    "\n",
    "    def load_flow_atlas(\n",
    "        self, depth: Union[int, float] = None, minimal: bool = False\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load an atlas of Kernel Flow source/detector locations with corresponding brain regions.\n",
    "\n",
    "        Args:\n",
    "            depth (Union[int, float], optional): Depth into the brain. Defaults to None (brain surface).\n",
    "            minimal (bool): Load a minimal version with just channels and brain regions. Defaults to False (load all data).\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame with brain regions for all sources and detectors\n",
    "        \"\"\"\n",
    "        if depth is None:\n",
    "            depth = 0\n",
    "        filename = f\"kernel_flow_atlas_depth_{depth}.csv\"\n",
    "        filedir = os.path.join(os.getcwd(), \"processed_data\", \"flow\")\n",
    "        filepath = os.path.join(filedir, filename)\n",
    "        atlas_df = pd.read_csv(filepath, dtype={\"channel_num\": \"Int64\"})\n",
    "        if minimal:\n",
    "            atlas_df = atlas_df[\n",
    "                [\n",
    "                    \"channel_num\",\n",
    "                    \"AAL_distance\",\n",
    "                    \"AAL_region\",\n",
    "                    \"BA_distance\",\n",
    "                    \"BA_region\",\n",
    "                ]\n",
    "            ]\n",
    "        return atlas_df\n",
    "\n",
    "    def plot_pos(\n",
    "        self,\n",
    "        dim: str,\n",
    "        add_labels: bool = False,\n",
    "        minimal: bool = True,\n",
    "        hemo_type: str = \"HbO\",\n",
    "        add_missing: bool = True,\n",
    "        azim: int = 120,\n",
    "        view: str = None,\n",
    "        channels: Union[List[int], int] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Plot the detector and source 2D or 3D positions.\n",
    "\n",
    "        Args:\n",
    "            dim (str): Position data dimension \"2D\" or \"3D\".\n",
    "            add_labels (bool): Add a channel number label at each source position. Defaults to False.\n",
    "            minimal (bool): Show minimal plot elements. Defaults to False.\n",
    "            hemo_type (str): Hemodynamic type. \"HbO\" or \"HbR\". Defaults to \"HbO\".\n",
    "            add_missing (bool): Add missing detector/source positions. Defaults to True.\n",
    "            azim (int): 3D plot azimuth. Defaults to 120 degrees.\n",
    "            view: 3D plot view. \"Anterior\", \"Posterior\", \"Left\" or \"Right\". Defaults to None.\n",
    "            channels (Union[List[int], int]): Highlight specific channel(s). Defaults to None.\n",
    "        \"\"\"\n",
    "\n",
    "        def _get_highlight_channels(\n",
    "            plot_df: pd.DataFrame, channels: Union[List[int], int]\n",
    "        ) -> pd.DataFrame:\n",
    "            if isinstance(channels, int):\n",
    "                return plot_df[plot_df[\"channel_num\"] == channels]\n",
    "            elif isinstance(channels, list):\n",
    "                return plot_df[plot_df[\"channel_num\"].isin(channels)]\n",
    "\n",
    "        def _add_labels(\n",
    "            plot_df: pd.DataFrame,\n",
    "            dim: int,\n",
    "            opt_type: str = \"source\",\n",
    "            label_x_offset: int = 0,\n",
    "            label_y_offset: int = 0,\n",
    "            label_z_offset: int = 0,\n",
    "        ):\n",
    "            if dim.lower() == \"2d\":\n",
    "                labels = plot_df[\"channel_num\"]\n",
    "                if opt_type == \"source\":\n",
    "                    x_pos = list(plot_df[\"source_x_pos\"])\n",
    "                    y_pos = list(plot_df[\"source_y_pos\"])\n",
    "                elif opt_type == \"detector\":\n",
    "                    x_pos = list(plot_df[\"detector_x_pos\"])\n",
    "                    y_pos = list(plot_df[\"detector_y_pos\"])\n",
    "                for i, label in enumerate(labels):\n",
    "                    try:\n",
    "                        ax.annotate(\n",
    "                            label,\n",
    "                            (x_pos[i] - 0.007, y_pos[i] - 0.007),\n",
    "                            xytext=(label_x_offset, label_y_offset),\n",
    "                            textcoords=\"offset points\",\n",
    "                            fontsize=8,\n",
    "                            ha=\"left\",\n",
    "                            va=\"center\",\n",
    "                            bbox=dict(\n",
    "                                boxstyle=\"round,pad=0.15\",\n",
    "                                edgecolor=\"black\",\n",
    "                                facecolor=\"white\",\n",
    "                                alpha=1,\n",
    "                            ),\n",
    "                            arrowprops=dict(\n",
    "                                arrowstyle=\"-|>\",\n",
    "                                facecolor=\"black\",\n",
    "                                linewidth=2,\n",
    "                                shrinkA=0,\n",
    "                                shrinkB=0,\n",
    "                            ),\n",
    "                        )\n",
    "                    except TypeError:\n",
    "                        ax.annotate(\n",
    "                            \"NaN\",\n",
    "                            (x_pos[i] - 0.007, y_pos[i] - 0.007),\n",
    "                            xytext=(label_x_offset, label_y_offset),\n",
    "                            textcoords=\"offset points\",\n",
    "                            fontsize=8,\n",
    "                            ha=\"left\",\n",
    "                            va=\"center\",\n",
    "                            bbox=dict(\n",
    "                                boxstyle=\"round,pad=0.15\",\n",
    "                                edgecolor=\"black\",\n",
    "                                facecolor=\"white\",\n",
    "                                alpha=1,\n",
    "                            ),\n",
    "                            arrowprops=dict(\n",
    "                                arrowstyle=\"-|>\",\n",
    "                                facecolor=\"black\",\n",
    "                                linewidth=2,\n",
    "                                shrinkA=0,\n",
    "                                shrinkB=0,\n",
    "                            ),\n",
    "                        )\n",
    "            elif dim.lower() == \"3d\":\n",
    "                labels = plot_df[\"channel_num\"]\n",
    "                if opt_type == \"source\":\n",
    "                    x_pos = list(plot_df[\"source_x_pos\"])\n",
    "                    y_pos = list(plot_df[\"source_y_pos\"])\n",
    "                    z_pos = list(plot_df[\"source_z_pos\"])\n",
    "                elif opt_type == \"detector\":\n",
    "                    x_pos = list(plot_df[\"detector_x_pos\"])\n",
    "                    y_pos = list(plot_df[\"detector_y_pos\"])\n",
    "                    z_pos = list(plot_df[\"detector_z_pos\"])\n",
    "                for i, label in enumerate(labels):\n",
    "                    label_x = x_pos[i] + label_x_offset\n",
    "                    label_y = y_pos[i] + label_y_offset\n",
    "                    label_z = z_pos[i] + label_z_offset\n",
    "                    arrow_length = np.array(\n",
    "                        [label_x_offset, label_y_offset, label_z_offset]\n",
    "                    )\n",
    "                    ax.quiver(\n",
    "                        x_pos[i] + arrow_length[0],\n",
    "                        y_pos[i] + arrow_length[1],\n",
    "                        z_pos[i] + arrow_length[2],\n",
    "                        -arrow_length[0],\n",
    "                        -arrow_length[1],\n",
    "                        -arrow_length[2],\n",
    "                        color=\"black\",\n",
    "                        linewidth=1,\n",
    "                        arrow_length_ratio=0.3,\n",
    "                    )\n",
    "                    try:\n",
    "                        ax.text(\n",
    "                            label_x,\n",
    "                            label_y,\n",
    "                            label_z,\n",
    "                            label,\n",
    "                            fontsize=8,\n",
    "                            ha=\"left\",\n",
    "                            va=\"center\",\n",
    "                            bbox=dict(\n",
    "                                boxstyle=\"round,pad=0.15\",\n",
    "                                edgecolor=\"black\",\n",
    "                                facecolor=\"white\",\n",
    "                                alpha=1,\n",
    "                            ),\n",
    "                        )\n",
    "                    except TypeError:\n",
    "                        ax.text(\n",
    "                            label_x,\n",
    "                            label_y,\n",
    "                            label_z,\n",
    "                            \"NaN\",\n",
    "                            fontsize=8,\n",
    "                            ha=\"left\",\n",
    "                            va=\"center\",\n",
    "                            bbox=dict(\n",
    "                                boxstyle=\"round,pad=0.15\",\n",
    "                                edgecolor=\"black\",\n",
    "                                facecolor=\"white\",\n",
    "                                alpha=1,\n",
    "                            ),\n",
    "                        )\n",
    "\n",
    "        source_detector_df = self.create_source_detector_df(dim, add_missing)\n",
    "        source_detector_hemo = source_detector_df[\n",
    "            source_detector_df[\"data_type_index\"] == hemo_type\n",
    "        ]\n",
    "        uni_source_label_df = source_detector_hemo.drop_duplicates(\n",
    "            subset=\"source_index\"\n",
    "        )\n",
    "\n",
    "        if dim.lower() == \"2d\":\n",
    "            x_detector = list(source_detector_hemo[\"detector_x_pos\"])\n",
    "            y_detector = list(source_detector_hemo[\"detector_y_pos\"])\n",
    "            x_source = list(uni_source_label_df[\"source_x_pos\"])\n",
    "            y_source = list(uni_source_label_df[\"source_y_pos\"])\n",
    "            fig = plt.figure(figsize=(8, 8))\n",
    "            ax = fig.add_subplot(111)\n",
    "            ax.scatter(x_detector, y_detector, s=40)\n",
    "            ax.scatter(x_source, y_source, s=70)\n",
    "            if add_labels and not channels:\n",
    "                label_x_offset = 10\n",
    "                label_y_offset = 15\n",
    "                _add_labels(\n",
    "                    uni_source_label_df, dim, \"source\", label_x_offset, label_y_offset\n",
    "                )\n",
    "            if minimal:\n",
    "                ax.set_title(\"Anterior\", fontweight=\"bold\", fontsize=14)\n",
    "                ax.set_xticklabels([])\n",
    "                ax.set_yticklabels([])\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "                for spine in ax.spines.values():\n",
    "                    spine.set_visible(False)\n",
    "                ax.text(\n",
    "                    0.5,\n",
    "                    -0.06,\n",
    "                    \"Posterior\",\n",
    "                    fontweight=\"bold\",\n",
    "                    fontsize=14,\n",
    "                    ha=\"center\",\n",
    "                    va=\"bottom\",\n",
    "                    transform=ax.transAxes,\n",
    "                )\n",
    "                ax.text(\n",
    "                    -0.02,\n",
    "                    0.5,\n",
    "                    \"Left\",\n",
    "                    fontsize=14,\n",
    "                    fontweight=\"bold\",\n",
    "                    rotation=90,\n",
    "                    va=\"center\",\n",
    "                    ha=\"center\",\n",
    "                    transform=ax.transAxes,\n",
    "                )\n",
    "                ax.text(\n",
    "                    1.02,\n",
    "                    0.5,\n",
    "                    \"Right\",\n",
    "                    fontsize=14,\n",
    "                    fontweight=\"bold\",\n",
    "                    rotation=90,\n",
    "                    va=\"center\",\n",
    "                    ha=\"center\",\n",
    "                    transform=ax.transAxes,\n",
    "                )\n",
    "            else:\n",
    "                ax.set_title(\"Detector/Source 2D Plot\")\n",
    "                ax.set_xlabel(\"X-Position (mm)\")\n",
    "                ax.set_ylabel(\"Y-Position (mm)\")\n",
    "                ax.legend([\"Detector\", \"Source\"])\n",
    "            if channels:\n",
    "                label_x_offset = 12\n",
    "                label_y_offset = 12\n",
    "                highlight_rows = _get_highlight_channels(source_detector_hemo, channels)\n",
    "                _add_labels(\n",
    "                    highlight_rows, dim, \"detector\", label_x_offset, label_y_offset\n",
    "                )\n",
    "\n",
    "        elif dim.lower() == \"3d\":\n",
    "            fig = plt.figure(figsize=(8, 8))\n",
    "            ax = fig.add_subplot(111, projection=\"3d\", computed_zorder=False)\n",
    "            label_x_offset = 10\n",
    "            label_y_offset = 10\n",
    "            label_z_offset = 10\n",
    "            if not view:\n",
    "                x_detector = list(source_detector_hemo[\"detector_x_pos\"])\n",
    "                y_detector = list(source_detector_hemo[\"detector_y_pos\"])\n",
    "                z_detector = list(source_detector_hemo[\"detector_z_pos\"])\n",
    "                x_source = list(uni_source_label_df[\"source_x_pos\"])\n",
    "                y_source = list(uni_source_label_df[\"source_y_pos\"])\n",
    "                z_source = list(uni_source_label_df[\"source_z_pos\"])\n",
    "                ax.scatter(x_detector, y_detector, z_detector, s=30)\n",
    "                ax.scatter(x_source, y_source, z_source, s=55)\n",
    "                ax.view_init(azim=azim)\n",
    "                if add_labels and not channels:\n",
    "                    _add_labels(\n",
    "                        uni_source_label_df,\n",
    "                        dim,\n",
    "                        \"source\",\n",
    "                        label_x_offset,\n",
    "                        label_y_offset,\n",
    "                        label_z_offset,\n",
    "                    )\n",
    "                if channels:\n",
    "                    highlight_rows = _get_highlight_channels(\n",
    "                        source_detector_hemo, channels\n",
    "                    )\n",
    "                    _add_labels(\n",
    "                        highlight_rows,\n",
    "                        dim,\n",
    "                        \"detector\",\n",
    "                        label_x_offset,\n",
    "                        label_y_offset,\n",
    "                        label_z_offset,\n",
    "                    )\n",
    "            else:\n",
    "                views = {\n",
    "                    \"right\": 0,\n",
    "                    \"left\": 180,\n",
    "                    \"anterior\": 90,\n",
    "                    \"posterior\": 270,\n",
    "                }\n",
    "                ax.view_init(elev=0, azim=views[view])\n",
    "                if view == \"right\":\n",
    "                    source_plot_df = uni_source_label_df[\n",
    "                        uni_source_label_df[\"source_x_pos\"] >= 0\n",
    "                    ]\n",
    "                    detector_plot_df = source_detector_hemo[\n",
    "                        source_detector_hemo[\"detector_x_pos\"] >= 0\n",
    "                    ]\n",
    "                    ax.set_title(\"Right View\", fontweight=\"bold\", fontsize=14, y=0.85)\n",
    "                elif view == \"left\":\n",
    "                    source_plot_df = uni_source_label_df[\n",
    "                        uni_source_label_df[\"source_x_pos\"] <= 0\n",
    "                    ]\n",
    "                    detector_plot_df = source_detector_hemo[\n",
    "                        source_detector_hemo[\"detector_x_pos\"] <= 0\n",
    "                    ]\n",
    "                    ax.set_title(\"Left View\", fontweight=\"bold\", fontsize=14, y=0.85)\n",
    "                elif view == \"anterior\":\n",
    "                    source_plot_df = uni_source_label_df[\n",
    "                        uni_source_label_df[\"source_y_pos\"] > 0\n",
    "                    ]\n",
    "                    detector_plot_df = source_detector_hemo[\n",
    "                        source_detector_hemo[\"detector_y_pos\"] > 0\n",
    "                    ]\n",
    "                    ax.set_title(\n",
    "                        \"Anterior View\", fontweight=\"bold\", fontsize=14, y=0.85\n",
    "                    )\n",
    "                elif view == \"posterior\":\n",
    "                    source_plot_df = uni_source_label_df[\n",
    "                        uni_source_label_df[\"source_y_pos\"] <= 0\n",
    "                    ]\n",
    "                    detector_plot_df = source_detector_hemo[\n",
    "                        source_detector_hemo[\"detector_y_pos\"] <= 0\n",
    "                    ]\n",
    "                    ax.set_title(\n",
    "                        \"Posterior View\", fontweight=\"bold\", fontsize=14, y=0.85\n",
    "                    )\n",
    "                if add_labels and not channels:\n",
    "                    try:\n",
    "                        _add_labels(\n",
    "                            source_plot_df,\n",
    "                            dim,\n",
    "                            \"source\",\n",
    "                            label_x_offset,\n",
    "                            label_y_offset,\n",
    "                            label_z_offset,\n",
    "                        )\n",
    "                    except NameError:\n",
    "                        _add_labels(\n",
    "                            source_plot_df,\n",
    "                            dim,\n",
    "                            \"source\",\n",
    "                            label_x_offset,\n",
    "                            label_y_offset,\n",
    "                            label_z_offset,\n",
    "                        )\n",
    "                ax.scatter(\n",
    "                    detector_plot_df[\"detector_x_pos\"],\n",
    "                    detector_plot_df[\"detector_y_pos\"],\n",
    "                    detector_plot_df[\"detector_z_pos\"],\n",
    "                    s=30,\n",
    "                    alpha=1,\n",
    "                    zorder=2,\n",
    "                )\n",
    "                ax.scatter(\n",
    "                    source_plot_df[\"source_x_pos\"],\n",
    "                    source_plot_df[\"source_y_pos\"],\n",
    "                    source_plot_df[\"source_z_pos\"],\n",
    "                    s=55,\n",
    "                    alpha=1,\n",
    "                    zorder=1,\n",
    "                )\n",
    "                if channels:\n",
    "                    highlight_rows = _get_highlight_channels(detector_plot_df, channels)\n",
    "                    _add_labels(\n",
    "                        highlight_rows,\n",
    "                        dim,\n",
    "                        \"detector\",\n",
    "                        label_x_offset,\n",
    "                        label_y_offset,\n",
    "                        label_z_offset,\n",
    "                    )\n",
    "\n",
    "            if minimal:\n",
    "                ax.patch.set_alpha(0.0)\n",
    "                ax.xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "                ax.xaxis.line.set_color(\"none\")\n",
    "                ax.yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "                ax.yaxis.line.set_color(\"none\")\n",
    "                ax.zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "                ax.zaxis.line.set_color(\"none\")\n",
    "                ax.set_xticklabels([])\n",
    "                ax.set_yticklabels([])\n",
    "                ax.set_zticklabels([])\n",
    "                if not view:\n",
    "                    if azim > 180:\n",
    "                        ax.set_xlabel(\"Posterior\", fontweight=\"bold\", fontsize=14)\n",
    "                    else:\n",
    "                        ax.set_xlabel(\"Anterior\", fontweight=\"bold\", fontsize=14)\n",
    "                    if azim >= 270 or (azim >= 0 and azim <= 90):\n",
    "                        ax.set_ylabel(\"Right\", fontweight=\"bold\", fontsize=14)\n",
    "                    else:\n",
    "                        ax.set_ylabel(\"Left\", fontweight=\"bold\", fontsize=14)\n",
    "            else:\n",
    "                ax.set_title(\"Detector/Source 3D Plot\")\n",
    "                ax.set_xlabel(\"X-Position (mm)\")\n",
    "                ax.set_ylabel(\"Y-Position (mm)\")\n",
    "                ax.set_zlabel(\"Z-Position (mm)\")\n",
    "                ax.legend([\"Detector\", \"Source\"])\n",
    "\n",
    "\n",
    "class Participant_Flow:\n",
    "    \"\"\"\n",
    "    This class contains functions, data structures, and info necessary for\n",
    "    processing Kernel Flow data from the experiments.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, par_num: int = None):\n",
    "        if not par_num:\n",
    "            par_num = 1\n",
    "        self.data_fun = Data_Functions()\n",
    "        self.adj_ts_markers = True\n",
    "        self.par_behav = Participant_Behav(par_num, self.adj_ts_markers)\n",
    "        self.par_num, self.par_ID = self.data_fun.process_par(par_num)\n",
    "        self.flow_raw_data_dir = os.path.join(\n",
    "            self.par_behav.raw_data_dir, self.par_ID, \"kernel_data\"\n",
    "        )\n",
    "        self.flow_processed_data_dir = os.path.join(\n",
    "            os.getcwd(), \"processed_data\", \"flow\"\n",
    "        )\n",
    "        self.flow = self.load_flow_session(\"1001\", wrapper=True)\n",
    "        self.flow_session_dict = self.create_flow_session_dict(wrapper=True)\n",
    "        self.time_offset_dict = self.create_time_offset_dict()\n",
    "        self.plot_color_dict = {\n",
    "            0: \"purple\",\n",
    "            1: \"orange\",\n",
    "            2: \"green\",\n",
    "            3: \"yellow\",\n",
    "            4: \"pink\",\n",
    "            5: \"skyblue\",\n",
    "        }\n",
    "\n",
    "    def calc_time_offset(self, exp_name: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the time offset (in seconds) between the behavioral and Kernel Flow data\n",
    "        files. Number of seconds that the Kernel Flow data is ahead of the behavioral data.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "\n",
    "        Returns:\n",
    "            float: Time offset (in seconds).\n",
    "        \"\"\"\n",
    "        exp = self.par_behav.get_exp(exp_name)\n",
    "        exp_start_ts = exp.start_ts\n",
    "        marker_sent_time = float(exp.marker_data[\"start_marker\"][\"sent_time\"])\n",
    "        session = self.par_behav.get_key_from_value(\n",
    "            self.par_behav.session_dict, exp_name\n",
    "        )\n",
    "        marker_df = self.create_abs_marker_df(session)\n",
    "        row = marker_df.loc[marker_df[\"Marker\"].str.startswith(exp_name)].reset_index()\n",
    "        if (\n",
    "            exp_name == \"go_no_go\"\n",
    "        ):  # Go/No-go experiment is missing start timestamp marker\n",
    "            try:\n",
    "                kernel_start_ts = row.loc[0, \"Start timestamp\"]\n",
    "                time_offset = kernel_start_ts - (exp_start_ts + marker_sent_time)\n",
    "            except:\n",
    "                time_offset = \"NaN\"\n",
    "        else:\n",
    "            kernel_start_ts = row.loc[0, \"Start timestamp\"]\n",
    "            time_offset = kernel_start_ts - (exp_start_ts + marker_sent_time)\n",
    "        return float(time_offset)\n",
    "\n",
    "    def create_time_offset_dict(self) -> dict:\n",
    "        \"\"\"\n",
    "        Create a dictionary containing the time offset (in seconds) for each experiment.\n",
    "\n",
    "        Returns:\n",
    "            dict: Time offset dictionary.\n",
    "        \"\"\"\n",
    "        time_offset_dict = {}\n",
    "        for exp_name in self.par_behav.exp_order:\n",
    "            if (\n",
    "                exp_name == \"go_no_go\"\n",
    "            ):  # Go/No-go experiment is missing start timestamp marker\n",
    "                if np.isnan(self.calc_time_offset(exp_name)):\n",
    "                    session = self.par_behav.get_key_from_value(\n",
    "                        self.par_behav.session_dict, exp_name\n",
    "                    )\n",
    "                    session_exp_names = self.par_behav.session_dict[session]\n",
    "                    other_exp_names = [\n",
    "                        temp_exp_name\n",
    "                        for temp_exp_name in session_exp_names\n",
    "                        if temp_exp_name != \"go_no_go\"\n",
    "                    ]\n",
    "                    other_exp_time_offsets = []\n",
    "                    for temp_exp_name in other_exp_names:\n",
    "                        time_offset = self.calc_time_offset(temp_exp_name)\n",
    "                        other_exp_time_offsets.append(time_offset)\n",
    "                    avg_time_offset = np.mean(other_exp_time_offsets)\n",
    "                    time_offset_dict[exp_name] = avg_time_offset\n",
    "            else:\n",
    "                time_offset_dict[exp_name] = self.calc_time_offset(exp_name)\n",
    "        for session, exp_list in self.par_behav.session_dict.items():\n",
    "            session_offset = np.mean(\n",
    "                [time_offset_dict[exp_name] for exp_name in exp_list]\n",
    "            )\n",
    "            time_offset_dict[session] = session_offset\n",
    "        return time_offset_dict\n",
    "\n",
    "    def get_time_offset(self, exp_name: str) -> float:\n",
    "        \"\"\"\n",
    "        Get the time offset for an experiment.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Experiment name.\n",
    "\n",
    "        Returns:\n",
    "            float: Time offset (in seconds).\n",
    "        \"\"\"\n",
    "        return self.time_offset_dict[exp_name]\n",
    "\n",
    "    def offset_time_array(self, exp_name: str, time_array: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Offset a Kernel Flow datetime array for an experiment by the time-offset.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "            time_array (np.ndarray): Datetime array.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Time-offset datetime array.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            time_offset = self.get_time_offset(exp_name)\n",
    "        except KeyError:  # if experiment start time is missing, use avg of other session experiments\n",
    "            time_offset_list = []\n",
    "            for exp_name in self.par_behav.exp_order:\n",
    "                try:\n",
    "                    time_offset = self.get_time_offset(exp_name)\n",
    "                    time_offset_list.append(time_offset)\n",
    "                except KeyError:\n",
    "                    pass\n",
    "            time_offset = mean(time_offset_list)\n",
    "        time_offset_dt = datetime.timedelta(seconds=time_offset)\n",
    "        time_abs_dt_offset = time_array - time_offset_dt\n",
    "        return time_abs_dt_offset\n",
    "\n",
    "    def load_flow_session(\n",
    "        self, session: list[str | int], wrapper: bool = False\n",
    "    ) -> snirf.Snirf:\n",
    "        \"\"\"\n",
    "        Load Kernel Flow data for an experiment session.\n",
    "\n",
    "        Args:\n",
    "            session list[str | int]: Experiment session.\n",
    "            wrapper (bool, optional) Option to return Process_Flow-wrapped SNIRF file.\n",
    "                                     Defaults to False.\n",
    "\n",
    "        Raises:\n",
    "            Exception: Invalid session number argument.\n",
    "\n",
    "        Returns:\n",
    "            snirf.Snirf: SNIRF file object.\n",
    "            -or-\n",
    "            Process_Flow object for each experiment session.\n",
    "        \"\"\"\n",
    "        if isinstance(session, str):\n",
    "            if \"session\" not in session:\n",
    "                session = f\"session_{session}\"\n",
    "        elif isinstance(session, int):\n",
    "            session = f\"session_{session}\"\n",
    "        try:\n",
    "            session_dir = os.path.join(self.flow_raw_data_dir, session)\n",
    "            filename = os.listdir(session_dir)[0]\n",
    "            filepath = os.path.join(session_dir, filename)\n",
    "            if wrapper:\n",
    "                return Process_Flow(filepath)\n",
    "            else:\n",
    "                return Process_Flow(filepath).snirf_file\n",
    "        except:\n",
    "            print(\"Invalid session number.\")\n",
    "            raise\n",
    "\n",
    "    def load_flow_exp(self, exp_name: str, filter_type: str = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load Kernel Flow data for the time frame of a specified experiment.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "            filter_type (str): Filter to apply to the data. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Kernel Flow data for an experiment.\n",
    "        \"\"\"\n",
    "        session = self.par_behav.get_key_from_value(\n",
    "            self.par_behav.session_dict, exp_name\n",
    "        )\n",
    "        flow_session = self.load_flow_session(session, wrapper=True)\n",
    "\n",
    "        start_dt = self.par_behav.get_start_dt(exp_name, self.adj_ts_markers)\n",
    "        end_dt = self.par_behav.get_end_dt(exp_name, self.adj_ts_markers)\n",
    "        time_abs_dt = flow_session.get_time_abs(\"datetime\")\n",
    "        time_abs_dt_offset = self.offset_time_array(exp_name, time_abs_dt)\n",
    "        start_idx = self.par_behav.get_start_index_dt(time_abs_dt_offset, start_dt)\n",
    "        end_idx = self.par_behav.get_end_index_dt(time_abs_dt_offset, end_dt)\n",
    "\n",
    "        flow_data = flow_session.get_data(\"dataframe\")\n",
    "        if filter_type.lower() == \"lowpass\":\n",
    "            flow_data = flow_data.apply(lambda x: self.lowpass_filter(x), axis=0)\n",
    "        elif filter_type.lower() == \"bandpass\":\n",
    "            flow_data = flow_data.apply(lambda x: self.bandpass_filter(x), axis=0)\n",
    "        flow_data.insert(0, \"datetime\", time_abs_dt_offset)\n",
    "        return flow_data.iloc[start_idx:end_idx, :]\n",
    "\n",
    "    def create_flow_session_dict(self, wrapper: bool = False) -> dict:\n",
    "        \"\"\"\n",
    "        Create a dictionary of Kernel Flow data for all experiment sessions.\n",
    "\n",
    "        wrapper (bool, optional) Option to return Process_Flow-wrapped SNIRF file.\n",
    "                                 Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            dict: Kernel Flow data for all experiment sessions.\n",
    "                keys:\n",
    "                    \"session_1001\", \"session_1002\", \"session_1003\"\n",
    "                values:\n",
    "                    SNIRF file object for each experiment session\n",
    "                    -or-\n",
    "                    Process_Flow object for each experiment session\n",
    "        \"\"\"\n",
    "        flow_session_dict = {}\n",
    "        for session in self.par_behav.session_dict.keys():\n",
    "            flow_session_dict[session] = self.load_flow_session(session, wrapper)\n",
    "        return flow_session_dict\n",
    "\n",
    "    def create_abs_marker_df(self, session: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Convert the \"stim\" marker DataFrame into absolute time.\n",
    "\n",
    "        Args:\n",
    "            session (str): Experiment session.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Marker \"stim\" data in absolute time.\n",
    "        \"\"\"\n",
    "        marker_df = self.flow_session_dict[session].get_marker_df()\n",
    "        time_origin_ts = self.flow_session_dict[session].get_time_origin(\"timestamp\")\n",
    "        marker_df[\"Timestamp\"] = marker_df[\"Timestamp\"] + time_origin_ts\n",
    "        marker_df.rename({\"Timestamp\": \"Start timestamp\"}, axis=1, inplace=True)\n",
    "\n",
    "        for idx, row in marker_df.iterrows():\n",
    "            end_ts = row[\"Start timestamp\"] + row[\"Duration\"]\n",
    "            marker_df.at[idx, \"End timestamp\"] = end_ts\n",
    "            exp_num = int(row[\"Experiment\"])\n",
    "            exp_name = self.par_behav.marker_dict[exp_num]\n",
    "            marker_df.at[idx, \"Experiment\"] = exp_name\n",
    "\n",
    "        marker_df.rename({\"Experiment\": \"Marker\"}, axis=1, inplace=True)\n",
    "        marker_df.drop([\"Value\"], axis=1, inplace=True)\n",
    "        marker_df = marker_df[\n",
    "            [\"Marker\", \"Start timestamp\", \"Duration\", \"End timestamp\"]\n",
    "        ]\n",
    "        return marker_df\n",
    "\n",
    "    def create_exp_stim_response_dict(\n",
    "        self, exp_name: str, filter_type: str = None\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Create a dictionary that contains the processed Kernel Flow data in response\n",
    "        to a stimulus. It is organized by block (keys) and for each block, the value is\n",
    "        a list of Pandas Series. Each Series is normalized, averaged, Kernel Flow data\n",
    "        during a presented stimulus duration for each channel. Each block is baselined\n",
    "        to the first 5 seconds, and the stim response is averaged over the stimulus\n",
    "        presentation duration.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "            filter_type (str): Filter to apply to the data. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            dict:\n",
    "                keys:\n",
    "                    \"block 1\", \"block 2\", ... \"block N\"\n",
    "                values:\n",
    "                    dicts:\n",
    "                        keys:\n",
    "                            \"trial 1\", \"trial 2\", ... \"trial N\"\n",
    "                        values:\n",
    "                            lists of averaged, normalized Kernel Flow data Series for each\n",
    "                            channel during the stimulus duration\n",
    "        \"\"\"\n",
    "        exp_results = load_results(\n",
    "            self.par_behav.processed_data_dir, exp_name, self.par_behav.par_num\n",
    "        )\n",
    "        flow_exp = self.load_flow_exp(exp_name, filter_type)\n",
    "        session = self.par_behav.get_key_from_value(\n",
    "            self.par_behav.session_dict, exp_name\n",
    "        )\n",
    "        ts_list = self.flow_session_dict[session].get_time_abs(\"timestamp\")\n",
    "        exp_time_offset = self.time_offset_dict[exp_name]\n",
    "        exp_by_block = self.par_behav.by_block_ts_dict[exp_name]\n",
    "\n",
    "        blocks = list(exp_results[\"block\"].unique())\n",
    "        exp_stim_resp_dict = {\n",
    "            block: {} for block in blocks\n",
    "        }  # initialize with unique blocks\n",
    "        processed_blocks = []\n",
    "\n",
    "        if exp_name == \"king_devick\":  # normalize all blocks to the first block\n",
    "            (first_block_start_ts, first_block_end_ts) = next(\n",
    "                iter(exp_by_block.keys())\n",
    "            )  # start/end of first block\n",
    "            first_block_start_ts_offset = first_block_start_ts + exp_time_offset\n",
    "            first_block_start_idx, _ = self.data_fun.find_closest_ts(\n",
    "                first_block_start_ts_offset, ts_list\n",
    "            )\n",
    "            first_block_end_ts_offset = first_block_end_ts + exp_time_offset\n",
    "            first_block_end_idx, _ = self.data_fun.find_closest_ts(\n",
    "                first_block_end_ts_offset, ts_list\n",
    "            )\n",
    "            baseline_rows = flow_exp.loc[\n",
    "                first_block_start_idx : first_block_start_idx + 35, 0:\n",
    "            ]  # first 5 seconds of a block\n",
    "            baseline = pd.DataFrame(baseline_rows.mean()).T\n",
    "\n",
    "            for (\n",
    "                block_start_ts,\n",
    "                block_end_ts,\n",
    "            ) in exp_by_block.keys():  # for each block in the experiment\n",
    "                block_start_ts_offset = block_start_ts + exp_time_offset\n",
    "                block_start_idx, _ = self.data_fun.find_closest_ts(\n",
    "                    block_start_ts_offset, ts_list\n",
    "                )\n",
    "                block_end_ts_offset = block_end_ts + exp_time_offset\n",
    "                block_end_idx, _ = self.data_fun.find_closest_ts(\n",
    "                    block_end_ts_offset, ts_list\n",
    "                )\n",
    "                block_rows = flow_exp.loc[\n",
    "                    block_start_idx:block_end_idx, 0:\n",
    "                ]  # rows from block start to end\n",
    "\n",
    "                baseline_df = pd.concat(\n",
    "                    [baseline] * block_rows.shape[0], ignore_index=True\n",
    "                )\n",
    "                baseline_df = baseline_df.set_index(\n",
    "                    pd.Index(range(block_start_idx, block_start_idx + len(baseline_df)))\n",
    "                )\n",
    "\n",
    "                block_rows_norm = block_rows.subtract(\n",
    "                    baseline_df, fill_value=0\n",
    "                )  # normalize the block rows\n",
    "                processed_blocks.append(block_rows_norm)\n",
    "        else:  # normalize each block to the start of the block\n",
    "            for (\n",
    "                block_start_ts,\n",
    "                block_end_ts,\n",
    "            ) in exp_by_block.keys():  # for each block in the experiment\n",
    "                block_start_ts_offset = block_start_ts + exp_time_offset\n",
    "                block_start_idx, _ = self.data_fun.find_closest_ts(\n",
    "                    block_start_ts_offset, ts_list\n",
    "                )\n",
    "                block_end_ts_offset = block_end_ts + exp_time_offset\n",
    "                block_end_idx, _ = self.data_fun.find_closest_ts(\n",
    "                    block_end_ts_offset, ts_list\n",
    "                )\n",
    "                block_rows = flow_exp.loc[\n",
    "                    block_start_idx:block_end_idx, 0:\n",
    "                ]  # rows from block start to end\n",
    "\n",
    "                baseline_rows = flow_exp.loc[\n",
    "                    block_start_idx : block_start_idx + 35, 0:\n",
    "                ]  # first 5 seconds of a block\n",
    "                baseline = pd.DataFrame(baseline_rows.mean()).T\n",
    "                baseline_df = pd.concat(\n",
    "                    [baseline] * block_rows.shape[0], ignore_index=True\n",
    "                )\n",
    "                baseline_df = baseline_df.set_index(\n",
    "                    pd.Index(range(block_start_idx, block_start_idx + len(baseline_df)))\n",
    "                )\n",
    "\n",
    "                block_rows_norm = block_rows.subtract(\n",
    "                    baseline_df, fill_value=0\n",
    "                )  # normalize the block rows\n",
    "                processed_blocks.append(block_rows_norm)\n",
    "\n",
    "        processed_block_df = pd.concat(\n",
    "            processed_blocks\n",
    "        )  # all processed blocks for an experiment\n",
    "\n",
    "        for _, row in exp_results.iterrows():\n",
    "            stim_start_ts = row[\"stim_start\"]\n",
    "            stim_start_ts_offset = stim_start_ts + exp_time_offset\n",
    "            start_idx, _ = self.data_fun.find_closest_ts(stim_start_ts_offset, ts_list)\n",
    "            stim_end_ts = row[\"stim_end\"]\n",
    "            stim_end_ts_offset = stim_end_ts + exp_time_offset\n",
    "            end_idx, _ = self.data_fun.find_closest_ts(stim_end_ts_offset, ts_list)\n",
    "\n",
    "            stim_rows = processed_block_df.loc[start_idx:end_idx, 0:]\n",
    "            avg_stim_rows = stim_rows.mean()  # all channels for a stim\n",
    "\n",
    "            block = row[\"block\"]\n",
    "            trial = row[\"trial\"]\n",
    "\n",
    "            if trial not in exp_stim_resp_dict[block].keys():\n",
    "                exp_stim_resp_dict[block][trial] = []\n",
    "            exp_stim_resp_dict[block][trial].append(\n",
    "                avg_stim_rows\n",
    "            )  # add to a block in dict\n",
    "\n",
    "        return exp_stim_resp_dict\n",
    "\n",
    "    def create_exp_stim_response_df(\n",
    "        self, exp_name: str, filter_type: str = None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create a DataFrame that contains the processed Kernel Flow data in response\n",
    "        to each stimulus in an experiment. Each channel is normalized and averaged.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "            filter_type (str): Filter to apply to the data. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Processed Kernel Flow data.\n",
    "        \"\"\"\n",
    "\n",
    "        def _split_col(row: pd.Series) -> pd.Series:\n",
    "            \"\"\"\n",
    "            Split a column containing an array into separate columns for each\n",
    "            element in the array.\n",
    "\n",
    "            Args:\n",
    "                row (pd.Series): DataFrame row.\n",
    "\n",
    "            Returns:\n",
    "                pd.Series: DataFrame row with split column.\n",
    "            \"\"\"\n",
    "            arr = row[\"channels\"]\n",
    "            num_elements = len(arr)\n",
    "            col_names = [i for i in range(num_elements)]\n",
    "            return pd.Series(arr, index=col_names)\n",
    "\n",
    "        exp_baseline_avg_dict = self.create_exp_stim_response_dict(\n",
    "            exp_name, filter_type\n",
    "        )\n",
    "        rows = []\n",
    "        for block, block_data in sorted(exp_baseline_avg_dict.items()):\n",
    "            for trial, stim_resp_data in block_data.items():\n",
    "                trial_avg = np.mean(stim_resp_data, axis=0)\n",
    "                row = {\n",
    "                    \"participant\": self.par_num,\n",
    "                    \"block\": block,\n",
    "                    \"channels\": trial_avg,\n",
    "                }\n",
    "                rows.append(row)\n",
    "\n",
    "        stim_resp_df = pd.DataFrame(rows)\n",
    "        channel_cols = stim_resp_df.apply(_split_col, axis=1)\n",
    "        stim_resp_df = pd.concat(\n",
    "            [stim_resp_df, channel_cols], axis=1\n",
    "        )  # merge with original DataFrame\n",
    "        stim_resp_df = stim_resp_df.drop(\n",
    "            \"channels\", axis=1\n",
    "        )  # drop the original \"channels\" column\n",
    "        return stim_resp_df\n",
    "\n",
    "    def create_inter_module_exp_results_df(\n",
    "        self, exp_name: str, hemo_type: str = None, filter_type: str = None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create a DataFrame with the inter-module channels for an experiment.\n",
    "        This DataFrame can include both HbO and HbR channels in alternating columns\n",
    "        or just \"HbO\", \"HbR\", \"HbTot\", or \"HbDiff\" channels.\n",
    "\n",
    "        Args:\n",
    "            hemo_type (str, optional): \"HbO\", \"HbR\", \"HbTot\", or \"HbDiff\" channels.\n",
    "                                 Defaults to None (all inter-module channels).\n",
    "            filter_type (str): Filter to apply to the data. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Inter-module channels for an experiment.\n",
    "        \"\"\"\n",
    "\n",
    "        def _compute_df(hemo_type: str) -> pd.DataFrame:\n",
    "            \"\"\"\n",
    "            Create the HbTot and HbDiff DataFrames.\n",
    "\n",
    "            Args:\n",
    "                hemo_type (str): \"HbTot\" or \"HbDiff\".\n",
    "\n",
    "            Returns:\n",
    "                pd.DataFrame: HbTot or HbDiff DataFrame.\n",
    "            \"\"\"\n",
    "            HbO_df = inter_module_df.iloc[\n",
    "                :, np.r_[0, 1, 2 : len(inter_module_df.columns) : 2]\n",
    "            ]\n",
    "            HbO_data_cols = HbO_df.iloc[:, 2:]\n",
    "            HbR_df = inter_module_df.iloc[\n",
    "                :, np.r_[0, 1, 3 : len(inter_module_df.columns) : 2]\n",
    "            ]\n",
    "            HbR_data_cols = HbR_df.iloc[:, 2:]\n",
    "            cols_dict = {}\n",
    "            for i, col_name in enumerate(HbO_data_cols.columns):\n",
    "                if hemo_type.lower() == \"hbtot\":\n",
    "                    cols_dict[col_name] = (\n",
    "                        HbO_data_cols.iloc[:, i] + HbR_data_cols.iloc[:, i]\n",
    "                    )\n",
    "                elif hemo_type.lower() == \"hbdiff\":\n",
    "                    cols_dict[col_name] = (\n",
    "                        HbO_data_cols.iloc[:, i] - HbR_data_cols.iloc[:, i]\n",
    "                    )\n",
    "            df = pd.DataFrame(cols_dict)\n",
    "            df.insert(0, \"block\", HbO_df[\"block\"])\n",
    "            df.insert(0, \"participant\", HbO_df[\"participant\"])\n",
    "            return df\n",
    "\n",
    "        if filter_type:\n",
    "            exp_results = load_results(\n",
    "                os.path.join(self.flow_processed_data_dir, \"all_channels\", filter_type),\n",
    "                exp_name,\n",
    "            )\n",
    "        else:\n",
    "            exp_results = load_results(\n",
    "                os.path.join(\n",
    "                    self.flow_processed_data_dir, \"all_channels\", \"unfiltered\"\n",
    "                ),\n",
    "                exp_name,\n",
    "            )\n",
    "        session = self.par_behav.get_key_from_value(\n",
    "            self.par_behav.session_dict, exp_name\n",
    "        )\n",
    "        measurement_list_df = self.flow_session_dict[session].create_source_detector_df(\n",
    "            \"3D\"\n",
    "        )\n",
    "        channels = (measurement_list_df[\"measurement_list_index\"] - 1).tolist()\n",
    "        cols_to_select = [\"participant\", \"block\"] + [str(chan) for chan in channels]\n",
    "        inter_module_df = exp_results.loc[:, cols_to_select]\n",
    "        if hemo_type:\n",
    "            if hemo_type.lower() == \"hbo\":  # HbO\n",
    "                HbO_df = inter_module_df.iloc[\n",
    "                    :, np.r_[0, 1, 2 : len(inter_module_df.columns) : 2]\n",
    "                ]\n",
    "                return HbO_df\n",
    "            elif hemo_type.lower() == \"hbr\":  # HbR\n",
    "                HbR_df = inter_module_df.iloc[\n",
    "                    :, np.r_[0, 1, 3 : len(inter_module_df.columns) : 2]\n",
    "                ]\n",
    "                return HbR_df\n",
    "            elif hemo_type.lower() == \"hbtot\":  # HbTot\n",
    "                HbTot_df = _compute_df(hemo_type)\n",
    "                return HbTot_df\n",
    "            elif hemo_type.lower() == \"hbdiff\":  # HbDiff\n",
    "                HbDiff_df = _compute_df(hemo_type)\n",
    "                return HbDiff_df\n",
    "        else:\n",
    "            return inter_module_df\n",
    "\n",
    "    def lowpass_filter(\n",
    "        self,\n",
    "        data: Union[np.ndarray, pd.Series, pd.DataFrame],\n",
    "        cutoff: float = 0.1,\n",
    "        fs: float = 7.1,\n",
    "        order: int = 80,\n",
    "        sos: bool = True,\n",
    "    ) -> Union[np.ndarray, pd.Series, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Apply an IIR lowpass Butterworth filter.\n",
    "\n",
    "        Args:\n",
    "            data (Union[np.ndarray, pd.DataFrame]): Data to filter. Array, Series, or DataFrame.\n",
    "            cutoff (float): Cutoff frequency (Hz). Defaults to 0.1.\n",
    "            fs (float): System sampling frequency (Hz). Defaults to 7.1.\n",
    "            order (int): Filter order. Defaults to 80. NOTE: this is the doubled filtfilt order.\n",
    "            sos (bool): Use 'sos' or 'b, a' output. Defaults to True ('sos').\n",
    "\n",
    "        Returns:\n",
    "            Union[np.ndarray, pd.Series, pd.DataFrame]: Filtered data. Array, Series, or DataFrame.\n",
    "        \"\"\"\n",
    "        if sos:\n",
    "            sos = butter(\n",
    "                N=order / 2,\n",
    "                Wn=cutoff,\n",
    "                fs=fs,\n",
    "                btype=\"lowpass\",\n",
    "                output=\"sos\",\n",
    "                analog=False,\n",
    "            )\n",
    "            pad = int(len(data) * 0.8)\n",
    "            if type(data) == pd.DataFrame:\n",
    "                data_out = data.apply(\n",
    "                    lambda x: sosfiltfilt(sos, data, padlen=pad), axis=0\n",
    "                )  # apply lowpass filter\n",
    "            elif type(data) == np.ndarray or type(data) == pd.Series:\n",
    "                data_out = sosfiltfilt(sos, data, padlen=pad)  # apply lowpass filter\n",
    "        else:\n",
    "            b, a = butter(N=order / 2, Wn=cutoff, fs=fs, btype=\"lowpass\", analog=False)\n",
    "            pad = 3 * (max(len(b), len(a)) - 1)\n",
    "            if type(data) == pd.DataFrame:\n",
    "                data_out = data.apply(\n",
    "                    lambda x: filtfilt(b, a, data, padlen=pad), axis=0\n",
    "                )  # apply lowpass filter\n",
    "            elif type(data) == np.ndarray or type(data) == pd.Series:\n",
    "                data_out = filtfilt(b, a, data, padlen=pad)  # apply lowpass filter\n",
    "        return data_out\n",
    "\n",
    "    def bandpass_filter(\n",
    "        self,\n",
    "        data: Union[np.ndarray, pd.Series, pd.DataFrame],\n",
    "        cutoff_low: float = 0.01,\n",
    "        cutoff_high: float = 0.1,\n",
    "        fs: float = 7.1,\n",
    "        order: int = 20,\n",
    "        sos: bool = True,\n",
    "    ) -> Union[np.ndarray, pd.Series, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Apply an IIR bandpass Butterworth filter.\n",
    "\n",
    "        Args:\n",
    "            data (Union[np.ndarray, pd.DataFrame]): Data to filter. Array, Series, or DataFrame.\n",
    "            cutoff_low (float): Low cutoff frequency (Hz). Defaults to 0.01.\n",
    "            cutoff_high (float): High cutoff frequency (Hz). Defaults to 0.1.\n",
    "            fs (float): System sampling frequency (Hz). Defaults to 7.1.\n",
    "            order (int): Filter order. Defaults to 20. NOTE: this is the doubled filtfilt order.\n",
    "            sos (bool): Use 'sos' or 'b, a' output. Defaults to True ('sos').\n",
    "\n",
    "        Returns:\n",
    "            Union[np.ndarray, pd.Series, pd.DataFrame]: Filtered data. Array, Series, or DataFrame.\n",
    "        \"\"\"\n",
    "        if sos:\n",
    "            sos = butter(\n",
    "                N=order,\n",
    "                Wn=[cutoff_low, cutoff_high],\n",
    "                fs=fs,\n",
    "                btype=\"bandpass\",\n",
    "                output=\"sos\",\n",
    "                analog=False,\n",
    "            )\n",
    "            pad = int(len(data) * 0.8)\n",
    "            if type(data) == pd.DataFrame:\n",
    "                data_out = data.apply(\n",
    "                    lambda x: sosfiltfilt(sos, data, padlen=pad), axis=0\n",
    "                )  # apply bandpass filter\n",
    "            elif type(data) == np.ndarray or type(data) == pd.Series:\n",
    "                data_out = sosfiltfilt(sos, data, padlen=pad)  # apply bandpass filter\n",
    "        else:\n",
    "            b, a = butter(\n",
    "                N=order,\n",
    "                Wn=[cutoff_low, cutoff_high],\n",
    "                fs=fs,\n",
    "                btype=\"bandpass\",\n",
    "                analog=False,\n",
    "            )\n",
    "            pad = 3 * (max(len(b), len(a)) - 1)\n",
    "            if type(data) == pd.DataFrame:\n",
    "                data_out = data.apply(\n",
    "                    lambda x: filtfilt(b, a, data, padlen=pad), axis=0\n",
    "                )  # apply bandpass filter\n",
    "            elif type(data) == np.ndarray or type(data) == pd.Series:\n",
    "                data_out = filtfilt(b, a, data, padlen=pad)  # apply bandpass filter\n",
    "        return data_out\n",
    "\n",
    "    def plot_flow_session(\n",
    "        self, session: str, channels: Union[int, list, tuple], filter_type: str = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Plot Kernel flow session data.\n",
    "\n",
    "        Args:\n",
    "            session (str): Session number.\n",
    "            channels (Union[int, list, tuple]): Kernel Flow channels to plot.\n",
    "            filter_type (str, optional): Filter type to apply. Defaults to None.\n",
    "        \"\"\"\n",
    "        flow_session = self.flow_session_dict[session]\n",
    "        sel_flow_data = flow_session.get_data(\"dataframe\", channels)  # TODO\n",
    "        if filter_type == \"lowpass\":\n",
    "            sel_flow_data = self.lowpass_filter(sel_flow_data)\n",
    "        elif filter_type == \"bandpass\":\n",
    "            flow_data = flow_data.apply(lambda x: self.bandpass_filter(x), axis=0)\n",
    "        session_time_offset = self.time_offset_dict[session]\n",
    "        time_abs_dt = flow_session.get_time_abs(\"datetime\")\n",
    "        time_abs_dt_offset = time_abs_dt - datetime.timedelta(\n",
    "            seconds=session_time_offset\n",
    "        )\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(15, 6))\n",
    "\n",
    "        data_traces = []\n",
    "        data_labels = []\n",
    "        for channel_num in channels:\n",
    "            flow_data = sel_flow_data.iloc[:, channel_num]\n",
    "            data_type_label = self.flow_session_dict[session].get_data_type_label(\n",
    "                channel_num\n",
    "            )\n",
    "            legend_label = f\"Ch {channel_num} ({data_type_label})\"\n",
    "            if data_type_label == \"HbO\":\n",
    "                color = \"red\"\n",
    "            elif data_type_label == \"HbR\":\n",
    "                color = \"blue\"\n",
    "            (data_trace,) = ax.plot(\n",
    "                time_abs_dt_offset, flow_data, color=color, label=legend_label\n",
    "            )\n",
    "            data_traces.append(data_trace)\n",
    "            data_labels.append(legend_label)\n",
    "\n",
    "        exp_spans = []\n",
    "        for exp_name in self.par_behav.session_dict[session]:\n",
    "            exp_start_dt = self.par_behav.get_start_dt(exp_name)\n",
    "            exp_end_dt = self.par_behav.get_end_dt(exp_name)\n",
    "            ax.axvline(exp_start_dt, linestyle=\"dashed\", color=\"k\", alpha=0.75)\n",
    "            ax.axvline(exp_end_dt, linestyle=\"dashed\", color=\"k\", alpha=0.75)\n",
    "            exp_span = ax.axvspan(\n",
    "                exp_start_dt,\n",
    "                exp_end_dt,\n",
    "                color=self.par_behav.exp_color_dict[exp_name],\n",
    "                alpha=0.4,\n",
    "                label=exp_name,\n",
    "            )\n",
    "            exp_spans.append(exp_span)\n",
    "\n",
    "        data_legend = ax.legend(\n",
    "            handles=data_traces,\n",
    "            bbox_to_anchor=(1.0, 1.0),\n",
    "            facecolor=\"white\",\n",
    "            framealpha=1,\n",
    "            title=\"Kernel Flow Data\",\n",
    "        )\n",
    "        handles, labels = plt.gca().get_legend_handles_labels()\n",
    "        uni_labels = dict(zip(labels, handles))\n",
    "        [uni_labels.pop(data_label) for data_label in data_labels]\n",
    "\n",
    "        stim_legend = ax.legend(\n",
    "            uni_labels.values(),\n",
    "            uni_labels.keys(),\n",
    "            bbox_to_anchor=(1.0, 0.75),\n",
    "            facecolor=\"white\",\n",
    "            framealpha=1,\n",
    "            title=\"Experiment\",\n",
    "        )\n",
    "        ax.add_artist(data_legend)\n",
    "        session_split = session.split(\"_\")\n",
    "        exp_title = session_split[0].capitalize() + \" \" + session_split[1]\n",
    "        ax.set_title(exp_title)\n",
    "        datetime_fmt = mdates.DateFormatter(\"%H:%M:%S\")\n",
    "        ax.xaxis.set_major_formatter(datetime_fmt)\n",
    "        ax.set_xlabel(\"Time\", fontsize=16, color=\"k\")\n",
    "\n",
    "    def plot_flow_exp(\n",
    "        self,\n",
    "        exp_name: str,\n",
    "        channels: list,\n",
    "        filter_type: str = None,\n",
    "        filter_order: int = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Plot Kernel Flow experiment data.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "            channels (list): Kernel Flow channels to plot.\n",
    "            filter_type (str, optional): Filter type to apply. Defaults to None.\n",
    "            filter_order (int): Filter order. Defaults to None (default filter order value).\n",
    "        \"\"\"\n",
    "        flow_exp = self.load_flow_exp(exp_name)\n",
    "        session = self.par_behav.get_key_from_value(\n",
    "            self.par_behav.session_dict, exp_name\n",
    "        )\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(15, 6))\n",
    "\n",
    "        data_traces = []\n",
    "        data_labels = []\n",
    "        for channel_num in channels:\n",
    "            timeseries = flow_exp[\"datetime\"]\n",
    "            flow_data = flow_exp.iloc[:, channel_num + 1]\n",
    "            if filter_type.lower() == \"lowpass\":\n",
    "                if filter_order:\n",
    "                    flow_data = self.lowpass_filter(flow_data, order=filter_order)\n",
    "                else:\n",
    "                    flow_data = self.lowpass_filter(flow_data)\n",
    "            elif filter_type.lower() == \"bandpass\":\n",
    "                if filter_order:\n",
    "                    flow_data = self.bandpass_filter(flow_data, order=filter_order)\n",
    "                else:\n",
    "                    flow_data = self.bandpass_filter(flow_data)\n",
    "            data_type_label = self.flow_session_dict[session].get_data_type_label(\n",
    "                channel_num\n",
    "            )\n",
    "            # legend_label = f\"Ch {channel_num} ({data_type_label})\"\n",
    "            legend_label = f\"{data_type_label}\"\n",
    "            if data_type_label == \"HbO\":\n",
    "                color = \"red\"\n",
    "            elif data_type_label == \"HbR\":\n",
    "                color = \"blue\"\n",
    "            (data_trace,) = ax.plot(\n",
    "                timeseries, flow_data, color=color, label=legend_label\n",
    "            )\n",
    "            data_traces.append(data_trace)\n",
    "            data_labels.append(legend_label)\n",
    "\n",
    "        exp_start_dt = self.par_behav.get_start_dt(exp_name, self.adj_ts_markers)\n",
    "        exp_end_dt = self.par_behav.get_end_dt(exp_name, self.adj_ts_markers)\n",
    "        ax.axvline(exp_start_dt, linestyle=\"dashed\", color=\"k\", alpha=0.75)\n",
    "        ax.axvline(exp_end_dt, linestyle=\"dashed\", color=\"k\", alpha=0.75)\n",
    "        results_dir = os.path.join(os.getcwd(), \"processed_data\", \"behavioral\")\n",
    "        exp_results = load_results(results_dir, exp_name, self.par_num)\n",
    "        exp_title = self.par_behav.format_exp_name(exp_name)\n",
    "\n",
    "        stim_spans = []\n",
    "        for _, row in exp_results.iterrows():\n",
    "            try:\n",
    "                uni_stim_dict = self.par_behav.create_unique_stim_dict(\n",
    "                    exp_results, \"stim\"\n",
    "                )\n",
    "                stim = row[\"stim\"]\n",
    "                legend_label = self.par_behav.format_exp_name(row[\"stim\"])\n",
    "            except KeyError:\n",
    "                uni_stim_dict = self.par_behav.create_unique_stim_dict(\n",
    "                    exp_results, \"block\"\n",
    "                )\n",
    "                stim = row[\"block\"]\n",
    "                legend_label = self.par_behav.format_exp_name(row[\"block\"])\n",
    "            color_index = uni_stim_dict[stim]\n",
    "            stim_start = datetime.datetime.fromtimestamp(row[\"stim_start\"])\n",
    "            try:\n",
    "                stim_end = datetime.datetime.fromtimestamp(row[\"stim_end\"])\n",
    "            except ValueError:\n",
    "                if exp_name == \"go_no_go\":\n",
    "                    stim_time = 0.5  # seconds\n",
    "                stim_end = datetime.datetime.fromtimestamp(\n",
    "                    row[\"stim_start\"] + stim_time\n",
    "                )\n",
    "            stim_span = ax.axvspan(\n",
    "                stim_start,\n",
    "                stim_end,\n",
    "                color=self.plot_color_dict[color_index],\n",
    "                alpha=0.4,\n",
    "                label=legend_label,\n",
    "            )\n",
    "            stim_spans.append(stim_span)\n",
    "\n",
    "        data_legend = ax.legend(\n",
    "            handles=data_traces,\n",
    "            bbox_to_anchor=(1.0, 1.0),\n",
    "            facecolor=\"white\",\n",
    "            framealpha=1,\n",
    "            title=\"fNIRS data\",\n",
    "        )\n",
    "        handles, labels = plt.gca().get_legend_handles_labels()\n",
    "        uni_labels = dict(zip(labels, handles))\n",
    "        [uni_labels.pop(data_label) for data_label in data_labels]\n",
    "\n",
    "        stim_legend = ax.legend(\n",
    "            uni_labels.values(),\n",
    "            uni_labels.keys(),\n",
    "            bbox_to_anchor=(1.0, 0.75),\n",
    "            facecolor=\"white\",\n",
    "            framealpha=1,\n",
    "            title=\"Stimulus\",\n",
    "        )\n",
    "        ax.add_artist(data_legend)\n",
    "        ax.set_title(exp_title)\n",
    "        datetime_fmt = mdates.DateFormatter(\"%H:%M:%S\")\n",
    "        ax.xaxis.set_major_formatter(datetime_fmt)\n",
    "        ax.set_xlabel(\"Time\", fontsize=16, color=\"k\")\n",
    "        ax.set_ylabel(\"Concentration (\\u03bcM)\", fontsize=16, color=\"k\")\n",
    "\n",
    "\n",
    "class Flow_Results:\n",
    "    def __init__(self):\n",
    "        self.results_dir = os.path.join(os.getcwd(), \"results\")\n",
    "        self.exp_names = [\n",
    "            \"audio_narrative\",\n",
    "            \"go_no_go\",\n",
    "            \"king_devick\",\n",
    "            \"n_back\",\n",
    "            \"resting_state\",\n",
    "            \"tower_of_london\",\n",
    "            \"vSAT\",\n",
    "            \"video_narrative_cmiyc\",\n",
    "            \"video_narrative_sherlock\",\n",
    "        ]\n",
    "        self.hemo_types = [\"HbO\", \"HbR\", \"HbTot\", \"HbDiff\"]\n",
    "        self.par = Participant_Flow(1)\n",
    "        self.flow_session = self.par.flow_session_dict[\"session_1001\"]\n",
    "\n",
    "    def process_flow_data(\n",
    "        self, num_pars: int, inter_module_only=True, filter_type: str = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Generate a CSV file that contains the Kernel Flow stimulus response data\n",
    "        for all experiments and participants.\n",
    "\n",
    "        Args:\n",
    "            num_pars (int): Number of participants in the study.\n",
    "            inter_module_only (bool): Select only inter-module channels. Defaults to True.\n",
    "            filter_type (str): Filter to apply to the data. Defaults to None.\n",
    "        \"\"\"\n",
    "        if inter_module_only:\n",
    "            print(f\"Processing participants ...\")\n",
    "            for hemo_type in self.hemo_types:\n",
    "                all_exp_results_list = []\n",
    "                exp_results_list = []\n",
    "                for exp_name in self.exp_names:\n",
    "                    stim_resp_df = self.par.create_inter_module_exp_results_df(\n",
    "                        exp_name, hemo_type, filter_type\n",
    "                    )\n",
    "                    exp_results_list.append(stim_resp_df)\n",
    "                all_exp_results_list.append(exp_results_list)\n",
    "\n",
    "                if filter_type:\n",
    "                    filedir = os.path.join(\n",
    "                        self.par.flow_processed_data_dir,\n",
    "                        \"inter_module_channels\",\n",
    "                        filter_type,\n",
    "                        hemo_type,\n",
    "                    )\n",
    "                else:\n",
    "                    filedir = os.path.join(\n",
    "                        self.par.flow_processed_data_dir,\n",
    "                        \"inter_module_channels\",\n",
    "                        \"unfiltered\",\n",
    "                        hemo_type,\n",
    "                    )\n",
    "                if not os.path.exists(filedir):\n",
    "                    os.makedirs(filedir)\n",
    "\n",
    "                print(f\"Creating {hemo_type} CSV files ...\")\n",
    "                all_exp_filepath = os.path.join(\n",
    "                    filedir, f\"all_experiments_flow_{hemo_type}.csv\"\n",
    "                )\n",
    "                if os.path.exists(all_exp_filepath):\n",
    "                    os.remove(all_exp_filepath)\n",
    "                for i, exp_name in enumerate(self.exp_names):\n",
    "                    exp_rows = [\n",
    "                        exp_results_list[i] for exp_results_list in all_exp_results_list\n",
    "                    ]\n",
    "                    exp_df = pd.concat(exp_rows, axis=0, ignore_index=True)\n",
    "                    filepath = os.path.join(filedir, f\"{exp_name}_flow_{hemo_type}.csv\")\n",
    "                    exp_df.to_csv(filepath, index=False)\n",
    "                    all_exp_df = exp_df.copy(deep=True)\n",
    "                    exp_name_col = [exp_name] * len(all_exp_df.index)\n",
    "                    all_exp_df.insert(0, \"experiment\", exp_name_col)\n",
    "                    # TODO: add demographic data\n",
    "                    if i == 0:\n",
    "                        all_exp_df.to_csv(\n",
    "                            all_exp_filepath, mode=\"a\", header=True, index=False\n",
    "                        )\n",
    "                    else:\n",
    "                        all_exp_df.to_csv(\n",
    "                            all_exp_filepath, mode=\"a\", header=False, index=False\n",
    "                        )\n",
    "        else:\n",
    "            all_exp_results_list = []\n",
    "            for par_num in range(1, num_pars + 1):\n",
    "                print(f\"Processing participant {par_num} ...\")\n",
    "                par = Participant_Flow(par_num)\n",
    "                exp_results_list = []\n",
    "                for exp_name in self.exp_names:\n",
    "                    stim_resp_df = par.create_exp_stim_response_df(\n",
    "                        exp_name, filter_type\n",
    "                    )\n",
    "                    exp_results_list.append(stim_resp_df)\n",
    "                all_exp_results_list.append(exp_results_list)\n",
    "\n",
    "            if filter_type:\n",
    "                filedir = os.path.join(\n",
    "                    self.par.flow_processed_data_dir, \"all_channels\", filter_type\n",
    "                )\n",
    "            else:\n",
    "                filedir = os.path.join(\n",
    "                    self.par.flow_processed_data_dir, \"all_channels\", \"unfiltered\"\n",
    "                )\n",
    "            if not os.path.exists(filedir):\n",
    "                os.makedirs(filedir)\n",
    "\n",
    "            print(\"Creating CSV files ...\")\n",
    "            all_exp_filepath = os.path.join(filedir, f\"all_experiments_flow.csv\")\n",
    "            if os.path.exists(all_exp_filepath):\n",
    "                os.remove(all_exp_filepath)\n",
    "            for i, exp_name in enumerate(self.exp_names):\n",
    "                exp_rows = [\n",
    "                    exp_results_list[i] for exp_results_list in all_exp_results_list\n",
    "                ]\n",
    "                exp_df = pd.concat(exp_rows, axis=0, ignore_index=True)\n",
    "                filepath = os.path.join(filedir, f\"{exp_name}_flow.csv\")\n",
    "                exp_df.to_csv(filepath, index=False)\n",
    "                all_exp_df = exp_df.copy(deep=True)\n",
    "                exp_name_col = [exp_name] * len(all_exp_df.index)\n",
    "                all_exp_df.insert(0, \"experiment\", exp_name_col)\n",
    "                if i == 0:\n",
    "                    all_exp_df.to_csv(\n",
    "                        all_exp_filepath, mode=\"a\", header=True, index=False\n",
    "                    )\n",
    "                else:\n",
    "                    all_exp_df.to_csv(\n",
    "                        all_exp_filepath, mode=\"a\", header=False, index=False\n",
    "                    )\n",
    "\n",
    "    def load_processed_flow_data(\n",
    "        self, exp_name: str, hemo_type: str, filter_type: str = None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load processes Kernel Flow data into a DataFrame.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "            hemo_type (str): Hemodynamic type. \"HbO\", \"HbR\", \"HbTot\", or \"HbDiff\".\n",
    "            filter_type (str, optional): Filter to apply to the data. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Processed DataFrame.\n",
    "        \"\"\"\n",
    "        read_filedir = os.path.join(\n",
    "            self.par.flow_processed_data_dir,\n",
    "            \"inter_module_channels\",\n",
    "            filter_type,\n",
    "            hemo_type,\n",
    "        )\n",
    "        read_filename = f\"{exp_name}_flow_{hemo_type}.csv\"\n",
    "        read_filepath = os.path.join(read_filedir, read_filename)\n",
    "        flow_df = pd.read_csv(read_filepath)\n",
    "\n",
    "        if exp_name == \"king_devick\":\n",
    "            flow_df = flow_df.drop(\n",
    "                flow_df[\n",
    "                    (flow_df[\"participant\"] == 15) & (flow_df[\"block\"] == \"card_1\")\n",
    "                ].index\n",
    "            )\n",
    "            flow_df.loc[flow_df[\"participant\"] == 15, \"block\"] = flow_df.loc[\n",
    "                flow_df[\"participant\"] == 15, \"block\"\n",
    "            ].apply(lambda x: x[:-1] + str(int(x[-1]) - 1))\n",
    "        return flow_df\n",
    "\n",
    "    def run_rm_anova(\n",
    "        self, exp_name: str, hemo_type: str, filter_type: str = None, corr: bool = True\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Run a repeated measures ANOVA on processed inter-module channels.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "            hemo_type (str): Hemodynamic type. \"HbO\", \"HbR\", \"HbTot\", or \"HbDiff\".\n",
    "            filter_type (str): Filter to apply to the data. Defaults to None.\n",
    "            corr (bool): Apply a Bonferroni correction to the p-values. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: ANOVA results.\n",
    "        \"\"\"\n",
    "        flow_df = self.load_processed_flow_data(exp_name, hemo_type, filter_type)\n",
    "        channels = list(flow_df.columns[2:])\n",
    "        num_channels = len(channels)\n",
    "        aov_list = []\n",
    "        for channel in channels:\n",
    "            aov = pg.rm_anova(\n",
    "                data=flow_df,\n",
    "                dv=channel,\n",
    "                within=\"block\",\n",
    "                subject=\"participant\",\n",
    "                effsize=\"np2\",\n",
    "            )\n",
    "            aov_final = aov[[\"p-unc\", \"F\", \"ddof1\", \"ddof2\"]].copy()\n",
    "            aov_final.rename(\n",
    "                columns={\n",
    "                    \"p-unc\": \"p_value\",\n",
    "                    \"F\": \"F_value\",\n",
    "                    \"ddof1\": \"df1\",\n",
    "                    \"ddof2\": \"df2\",\n",
    "                },\n",
    "                inplace=True,\n",
    "            )\n",
    "            aov_final[\"is_sig\"] = aov_final[\"p_value\"] < 0.05\n",
    "            if corr:  # apply Bonferroni correction\n",
    "                alpha_corr = 0.05 / num_channels\n",
    "                aov_final.insert(0, \"alpha_corr\", alpha_corr)\n",
    "                aov_final[\"is_sig_corr\"] = aov_final[\"p_value\"] < alpha_corr\n",
    "            aov_final.insert(0, \"channel_num\", channel)\n",
    "            aov_final[\"channel_num\"] = aov_final[\"channel_num\"].astype(int)\n",
    "            aov_list.append(aov_final)\n",
    "        exp_aov_results = pd.concat(aov_list)\n",
    "        return exp_aov_results\n",
    "\n",
    "    def run_all_rm_anovas(\n",
    "        self,\n",
    "        filter_type: str = None,\n",
    "        corr: bool = True,\n",
    "        brain_regions: bool = False,\n",
    "        depth: Union[int, float] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Run a repeated measures ANOVA for all experiments and hemodynamic types. Results are\n",
    "        written to CSV files and corresponding brain regions can be added.\n",
    "\n",
    "        Args:\n",
    "            filter_type (str): Filter to apply to the data. Defaults to None.\n",
    "            corr (bool): Apply a Bonferroni correction to the p-values. Defaults to True.\n",
    "            brain_regions (bool): Include AAL and BA brain region columns. Defaults to False.\n",
    "            depth (Union[int, float], optional): Depth into the brain. Defaults to None (brain surface).\n",
    "        \"\"\"\n",
    "        for exp_name in [\n",
    "            \"go_no_go\",\n",
    "            \"king_devick\",\n",
    "            \"n_back\",\n",
    "            \"tower_of_london\",\n",
    "            \"vSAT\",\n",
    "        ]:\n",
    "            for hemo_type in self.hemo_types:\n",
    "                if not filter_type:\n",
    "                    filter_type = \"unfiltered\"\n",
    "                write_filedir = os.path.join(\n",
    "                    self.results_dir, \"inter_module_channels\", exp_name, hemo_type\n",
    "                )\n",
    "                if not os.path.exists(write_filedir):\n",
    "                    os.makedirs(write_filedir)\n",
    "                if brain_regions:\n",
    "                    if depth is None:\n",
    "                        depth = 0\n",
    "                    flow_atlas = self.par.flow.load_flow_atlas(depth, minimal=True)\n",
    "                    flow_atlas.dropna(subset=[\"channel_num\"], inplace=True)\n",
    "                    write_filename = f\"{exp_name}_flow_stats_{hemo_type}_{filter_type}_depth_{depth}.csv\"\n",
    "                else:\n",
    "                    write_filename = (\n",
    "                        f\"{exp_name}_flow_stats_{hemo_type}_{filter_type}.csv\"\n",
    "                    )\n",
    "                write_filepath = os.path.join(write_filedir, write_filename)\n",
    "                exp_aov_results = self.run_rm_anova(\n",
    "                    exp_name, hemo_type, filter_type, corr\n",
    "                )\n",
    "                exp_aov_results_final = pd.merge(\n",
    "                    exp_aov_results, flow_atlas, on=\"channel_num\", how=\"left\"\n",
    "                )\n",
    "                exp_aov_results_final.to_csv(write_filepath, index=False)\n",
    "\n",
    "    def run_pos_hoc_test(\n",
    "        self, exp_name: str, hemo_type: str, filter_type: str = None, drop: bool = False\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Run pairwise t-tests for post-hoc ANOVA analysis.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "            hemo_type (str): Hemodynamic type. \"HbO\", \"HbR\", \"HbTot\", or \"HbDiff\".\n",
    "            filter_type (str): Filter to apply to the data. Defaults to None.\n",
    "            drop (bool): Drop columns with extra post-hoc info. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Post-hoc test results.\n",
    "        \"\"\"\n",
    "        flow_df = self.load_processed_flow_data(exp_name, hemo_type, filter_type)\n",
    "        sig_df = self.load_flow_stats(exp_name, hemo_type, filter_type, sig_only=True)\n",
    "        sig_channels = list(sig_df[\"channel_num\"].astype(str))\n",
    "        sig_flow_df = flow_df.loc[:, flow_df.columns.isin(sig_channels)]\n",
    "\n",
    "        pos_hoc_list = []\n",
    "        for channel in sig_flow_df.columns:\n",
    "            results = pg.pairwise_tests(\n",
    "                data=flow_df, dv=channel, within=\"block\", subject=\"participant\"\n",
    "            )\n",
    "            aov_p_value = float(\n",
    "                sig_df[sig_df[\"channel_num\"] == int(channel)][\"p_value\"]\n",
    "            )\n",
    "            results.insert(0, \"aov_p_value\", aov_p_value)\n",
    "            results.insert(0, \"channel_num\", channel)\n",
    "            pos_hoc_list.append(results)\n",
    "        post_hoc_results = pd.concat(pos_hoc_list, ignore_index=True)\n",
    "        post_hoc_results = post_hoc_results.rename(\n",
    "            columns={\n",
    "                \"Contrast\": \"within\",\n",
    "                \"A\": \"condition_A\",\n",
    "                \"B\": \"condition_B\",\n",
    "                \"T\": \"t_stat\",\n",
    "                \"dof\": \"df\",\n",
    "                \"p-unc\": \"p_value\",\n",
    "            }\n",
    "        )\n",
    "        if drop:\n",
    "            post_hoc_results = post_hoc_results.drop(\n",
    "                columns=[\n",
    "                    \"Paired\",\n",
    "                    \"Parametric\",\n",
    "                    \"alternative\",\n",
    "                    \"BF10\",\n",
    "                    \"hedges\",\n",
    "                ]\n",
    "            )\n",
    "        return post_hoc_results\n",
    "\n",
    "    def run_all_pos_hoc_tests(\n",
    "        self, filter_type: str = None, drop: bool = False\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Run pairwise t-tests for post-hoc ANOVA analysis for all experiments and hemodynamic types.\n",
    "        Results are written to CSV files and corresponding brain regions can be added..\n",
    "\n",
    "        Args:\n",
    "            filter_type (str): Filter to apply to the data. Defaults to None.\n",
    "            drop (bool): Drop columns with extra post-hoc info. Defaults to False.\n",
    "        \"\"\"\n",
    "        for exp_name in [\n",
    "            \"go_no_go\",\n",
    "            \"king_devick\",\n",
    "            \"n_back\",\n",
    "            \"tower_of_london\",\n",
    "            \"vSAT\",\n",
    "        ]:\n",
    "            for hemo_type in self.hemo_types:\n",
    "                if not filter_type:\n",
    "                    filter_type = \"unfiltered\"\n",
    "                write_filedir = os.path.join(\n",
    "                    self.results_dir,\n",
    "                    \"inter_module_channels\",\n",
    "                    exp_name,\n",
    "                    hemo_type,\n",
    "                )\n",
    "                write_filename = f\"{exp_name}_post_hoc_{hemo_type}_{filter_type}.csv\"\n",
    "                write_filepath = os.path.join(write_filedir, write_filename)\n",
    "                post_hoc_results = self.run_pos_hoc_test(\n",
    "                    exp_name, hemo_type, filter_type, drop\n",
    "                )\n",
    "                post_hoc_results.to_csv(write_filepath, index=False)\n",
    "\n",
    "    def load_flow_stats(\n",
    "        self,\n",
    "        exp_name: str,\n",
    "        hemo_type: str,\n",
    "        filter_type: str = None,\n",
    "        corr: bool = True,\n",
    "        sig_only: bool = False,\n",
    "        brain_regions: bool = False,\n",
    "        depth: Union[int, float] = None,\n",
    "        print_sig_results: bool = False,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load Kernel Flow statistical results.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "            hemo_type (str): Hemodynamic type. \"HbO\", \"HbR\", \"HbTot\", or \"HbDiff\".\n",
    "            filter_type (str): Filter to apply to the data. Defaults to None.\n",
    "            corr (bool): Apply a Bonferroni correction to the p-values. Defaults to True.\n",
    "            sig_only (bool): Return only significant results (p < 0.05). Defaults to False.\n",
    "            brain_regions (bool): Include AAL and BA brain region columns. Defaults to False.\n",
    "            depth (Union[int, float], optional): Depth into the brain. Defaults to None (brain surface).\n",
    "            print_sig_results (bool): Print significant results. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Statistical results for an experiment and hemodynamic type.\n",
    "        \"\"\"\n",
    "        if not filter_type:\n",
    "            filter_type = \"unfiltered\"\n",
    "        if depth is None:\n",
    "            depth = 0\n",
    "        filename = f\"{exp_name}_flow_stats_{hemo_type}_{filter_type}_depth_{depth}.csv\"\n",
    "        filepath = os.path.join(\n",
    "            self.results_dir,\n",
    "            \"inter_module_channels\",\n",
    "            exp_name,\n",
    "            hemo_type,\n",
    "            filename,\n",
    "        )\n",
    "        flow_stats = pd.read_csv(filepath)\n",
    "        if corr:\n",
    "            sig_col_name = \"is_sig_corr\"\n",
    "            flow_stats_out = flow_stats[\n",
    "                [\n",
    "                    \"channel_num\",\n",
    "                    \"alpha_corr\",\n",
    "                    \"p_value\",\n",
    "                    \"F_value\",\n",
    "                    \"df1\",\n",
    "                    \"df2\",\n",
    "                    sig_col_name,\n",
    "                ]\n",
    "            ]\n",
    "        else:\n",
    "            sig_col_name = \"is_sig\"\n",
    "            flow_stats_out = flow_stats[\n",
    "                [\"channel_num\", \"p_value\", \"F_value\", \"df1\", \"df2\", sig_col_name]\n",
    "            ]\n",
    "        if brain_regions:\n",
    "            flow_atlas = self.par.flow.load_flow_atlas(depth, minimal=True)\n",
    "            flow_atlas.dropna(subset=[\"channel_num\"], inplace=True)\n",
    "            flow_stats_out = pd.merge(\n",
    "                flow_stats_out, flow_atlas, on=\"channel_num\", how=\"left\"\n",
    "            )\n",
    "        sig_stats = flow_stats_out[flow_stats_out[sig_col_name] == True].sort_values(\n",
    "            by=\"p_value\", ascending=True\n",
    "        )\n",
    "        if print_sig_results:\n",
    "            print(sig_stats.to_string(index=False))\n",
    "        if sig_only:\n",
    "            return sig_stats\n",
    "        else:\n",
    "            return flow_stats_out\n",
    "\n",
    "    def load_post_hoc_stats(\n",
    "        self, exp_name: str, hemo_type: str, filter_type: str = None, drop: bool = False\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load Kernel Flow ANOVA post-hoc statistical results.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "            hemo_type (str): Hemodynamic type. \"HbO\", \"HbR\", \"HbTot\", or \"HbDiff\".\n",
    "            filter_type (str): Filter to apply to the data. Defaults to None.\n",
    "            drop (bool): Drop columns with extra post-hoc info. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Post-hoc statistical results for an experiment and hemodynamic type.\n",
    "        \"\"\"\n",
    "        if not filter_type:\n",
    "            filter_type = \"unfiltered\"\n",
    "        filename = f\"{exp_name}_post_hoc_{hemo_type}_{filter_type}.csv\"\n",
    "        filepath = os.path.join(\n",
    "            self.results_dir,\n",
    "            \"inter_module_channels\",\n",
    "            exp_name,\n",
    "            hemo_type,\n",
    "            filename,\n",
    "        )\n",
    "        post_hoc_stats = pd.read_csv(filepath)\n",
    "        if drop:\n",
    "            try:\n",
    "                post_hoc_stats = post_hoc_stats.drop(\n",
    "                    columns=[\"Paired\", \"Parametric\", \"alternative\", \"BF10\", \"hedges\"]\n",
    "                )\n",
    "            except KeyError:\n",
    "                pass\n",
    "        return post_hoc_stats\n",
    "\n",
    "    def create_flow_stats_df(\n",
    "        self, exp_name: str, hemo_type: str, filter_type: str = None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create a DataFrame with significant channels and corresponding brain regions.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "            hemo_type (str): Hemodynamic type. \"HbO\", \"HbR\", \"HbTot\", or \"HbDiff\".\n",
    "            filter_type (str): Filter to apply to the data. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Significant stats DataFrame with brain regions.\n",
    "        \"\"\"\n",
    "        sig_stats = self.load_flow_stats(\n",
    "            exp_name, hemo_type, filter_type, sig_only=True\n",
    "        )\n",
    "        sig_channels = list(sig_stats[\"channel_num\"])\n",
    "        source_detector_df = self.flow_session.create_source_detector_df(\n",
    "            \"3D\", brain_regions=True, channels=sig_channels\n",
    "        )\n",
    "        merged_df = pd.merge(\n",
    "            sig_stats, source_detector_df, on=\"channel_num\", how=\"left\"\n",
    "        )\n",
    "        flow_stats_df = merged_df.loc[\n",
    "            :,\n",
    "            [\n",
    "                \"channel_num\",\n",
    "                \"p_value\",\n",
    "                \"F_value\",\n",
    "                \"AAL_distance\",\n",
    "                \"AAL_region\",\n",
    "                \"BA_distance\",\n",
    "                \"BA_region\",\n",
    "            ],\n",
    "        ]\n",
    "        return flow_stats_df\n",
    "\n",
    "    def plot_stat_results(\n",
    "        self,\n",
    "        exp_name: str,\n",
    "        dim: str,\n",
    "        hemo_type: str,\n",
    "        add_labels: bool = False,\n",
    "        filter_type: str = None,\n",
    "        filepath: str = None,\n",
    "        show: bool = True,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Plot Kernel Flow statistical results.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "            dim (str): Position data dimension \"2D\" or \"3D\".\n",
    "            hemo_type (str): Hemodynamic type. \"HbO\", \"HbR\", \"HbTot\", or \"HbDiff\".\n",
    "            add_labels (bool): Add a channel number label at each detector position. Defaults to False.\n",
    "            filter_type (str): Filter to apply to the data. Defaults to None.\n",
    "            filepath (str): Filepath to save figure. Default to None (no output).\n",
    "            show (bool): Display the figure. Defaults to True.\n",
    "        \"\"\"\n",
    "\n",
    "        def _add_missing_pos(dim: str) -> pd.DataFrame:\n",
    "            \"\"\"\n",
    "            Add missing detector/source positions to the plot DataFrame.\n",
    "\n",
    "            Args:\n",
    "                dim (str): Position data dimension \"2D\" or \"3D\".\n",
    "\n",
    "            Returns:\n",
    "                pd.DataFrame: Plot DataFrame with missing positions added.\n",
    "            \"\"\"\n",
    "            nan_columns = [\n",
    "                \"channel_num\",\n",
    "                \"F_value\",\n",
    "                \"p_value\",\n",
    "                \"measurement_list_index\",\n",
    "                \"data_type\",\n",
    "                \"data_type_index\",\n",
    "                \"detector_index\",\n",
    "                \"source_index\",\n",
    "                \"source_label\",\n",
    "                \"detector_label\",\n",
    "            ]\n",
    "            plot_df_temp = pd.merge(flow_stats, source_detector_df, on=\"channel_num\")\n",
    "            row_list = []\n",
    "            if dim.lower() == \"2d\":\n",
    "                for detector_pos in self.flow_session.missing_detector_pos_2d:\n",
    "                    new_row = pd.Series(\n",
    "                        {\n",
    "                            \"source_x_pos\": self.flow_session.missing_source_pos_2d[0],\n",
    "                            \"source_y_pos\": self.flow_session.missing_source_pos_2d[1],\n",
    "                            \"detector_x_pos\": detector_pos[0],\n",
    "                            \"detector_y_pos\": detector_pos[1],\n",
    "                        }\n",
    "                    )\n",
    "                    row_list.append(new_row)\n",
    "                missing_pos_df = pd.DataFrame(row_list)\n",
    "                plot_df = pd.concat(\n",
    "                    [plot_df_temp, missing_pos_df], axis=0, ignore_index=True\n",
    "                )\n",
    "                plot_df.loc[\n",
    "                    plot_df.shape[0] - len(self.flow_session.missing_detector_pos_2d) :,\n",
    "                    nan_columns,\n",
    "                ] = float(\"NaN\")\n",
    "            elif dim.lower() == \"3d\":\n",
    "                for detector_pos in self.flow_session.missing_detector_pos_3d:\n",
    "                    new_row = pd.Series(\n",
    "                        {\n",
    "                            \"source_x_pos\": self.flow_session.missing_source_pos_3d[0],\n",
    "                            \"source_y_pos\": self.flow_session.missing_source_pos_3d[1],\n",
    "                            \"source_z_pos\": self.flow_session.missing_source_pos_3d[2],\n",
    "                            \"detector_x_pos\": detector_pos[0],\n",
    "                            \"detector_y_pos\": detector_pos[1],\n",
    "                            \"detector_z_pos\": detector_pos[2],\n",
    "                        }\n",
    "                    )\n",
    "                    row_list.append(new_row)\n",
    "                missing_pos_df = pd.DataFrame(row_list)\n",
    "                plot_df = pd.concat(\n",
    "                    [plot_df_temp, missing_pos_df], axis=0, ignore_index=True\n",
    "                )\n",
    "                plot_df.loc[\n",
    "                    plot_df.shape[0] - len(self.flow_session.missing_detector_pos_3d) :,\n",
    "                    nan_columns,\n",
    "                ] = float(\"NaN\")\n",
    "            return plot_df\n",
    "\n",
    "        flow_stats = self.load_flow_stats(exp_name, hemo_type, filter_type)\n",
    "        if dim.lower() == \"2d\":\n",
    "            source_detector_df = self.flow_session.create_source_detector_df(\"2D\")\n",
    "            plot_df = _add_missing_pos(dim)\n",
    "            fig = plt.figure(figsize=(6, 5))\n",
    "            ax = fig.add_subplot(111)\n",
    "            sig_detector_plot_df = plot_df[plot_df[\"p_value\"] <= 0.05]\n",
    "            not_sig_detector_plot_df = plot_df.loc[\n",
    "                (plot_df[\"p_value\"] > 0.05) | (pd.isna(plot_df[\"p_value\"]))\n",
    "            ]\n",
    "            scatter = ax.scatter(\n",
    "                sig_detector_plot_df[\"detector_x_pos\"],\n",
    "                sig_detector_plot_df[\"detector_y_pos\"],\n",
    "                s=70,\n",
    "                c=sig_detector_plot_df[\"p_value\"],\n",
    "                cmap=\"autumn_r\",\n",
    "                edgecolors=\"black\",\n",
    "                alpha=1,\n",
    "                zorder=3,\n",
    "            )\n",
    "            ax.scatter(\n",
    "                not_sig_detector_plot_df[\"detector_x_pos\"],\n",
    "                not_sig_detector_plot_df[\"detector_y_pos\"],\n",
    "                s=20,\n",
    "                c=\"dodgerblue\",\n",
    "                edgecolors=\"black\",\n",
    "                alpha=1,\n",
    "                zorder=2,\n",
    "            )\n",
    "            ax.scatter(\n",
    "                plot_df[\"source_x_pos\"],\n",
    "                plot_df[\"source_y_pos\"],\n",
    "                s=30,\n",
    "                c=\"black\",\n",
    "                zorder=1,\n",
    "            )\n",
    "            if add_labels:\n",
    "                labels = [\n",
    "                    plt.text(\n",
    "                        sig_detector_plot_df[\"detector_x_pos\"].iloc[i],\n",
    "                        sig_detector_plot_df[\"detector_y_pos\"].iloc[i],\n",
    "                        int(sig_detector_plot_df[\"channel_num\"].iloc[i]),\n",
    "                        fontsize=8,\n",
    "                        ha=\"center\",\n",
    "                        va=\"center\",\n",
    "                        bbox=dict(\n",
    "                            boxstyle=\"round,pad=0.15\",\n",
    "                            edgecolor=\"black\",\n",
    "                            facecolor=\"white\",\n",
    "                            alpha=1,\n",
    "                        ),\n",
    "                        zorder=4,\n",
    "                    )\n",
    "                    for i in range(sig_detector_plot_df.shape[0])\n",
    "                ]\n",
    "                adjust_text(\n",
    "                    labels,\n",
    "                    ax=ax,\n",
    "                    arrowprops=dict(\n",
    "                        arrowstyle=\"-|>\",\n",
    "                        facecolor=\"black\",\n",
    "                        linewidth=2,\n",
    "                        shrinkA=0,\n",
    "                        shrinkB=0,\n",
    "                        zorder=0,\n",
    "                    ),\n",
    "                    expand_points=(4, 4),\n",
    "                    expand_text=(1.5, 1.5),\n",
    "                    force_points=(0.8, 0.8),\n",
    "                )\n",
    "            ax.spines[\"top\"].set_visible(False)\n",
    "            ax.spines[\"right\"].set_visible(False)\n",
    "            ax.spines[\"bottom\"].set_visible(False)\n",
    "            ax.spines[\"left\"].set_visible(False)\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.set_title(\"Anterior\", fontweight=\"bold\", fontsize=14, y=1)\n",
    "            ax.text(\n",
    "                0.5,\n",
    "                -0.06,\n",
    "                \"Posterior\",\n",
    "                fontweight=\"bold\",\n",
    "                fontsize=14,\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "                transform=ax.transAxes,\n",
    "            )\n",
    "            ax.text(\n",
    "                -0.02,\n",
    "                0.5,\n",
    "                \"Left\",\n",
    "                fontsize=14,\n",
    "                fontweight=\"bold\",\n",
    "                rotation=90,\n",
    "                va=\"center\",\n",
    "                ha=\"center\",\n",
    "                transform=ax.transAxes,\n",
    "            )\n",
    "            ax.text(\n",
    "                1.02,\n",
    "                0.5,\n",
    "                \"Right\",\n",
    "                fontsize=14,\n",
    "                fontweight=\"bold\",\n",
    "                rotation=90,\n",
    "                va=\"center\",\n",
    "                ha=\"center\",\n",
    "                transform=ax.transAxes,\n",
    "            )\n",
    "            scatter.set_clim([0, 0.05])\n",
    "            colorbar = plt.colorbar(\n",
    "                scatter, ticks=[0, 0.01, 0.02, 0.03, 0.04, 0.05], shrink=0.7, pad=0.1\n",
    "            )\n",
    "            font_props = FontProperties(size=12)\n",
    "            colorbar.set_label(\"p-value\", fontproperties=font_props)\n",
    "            try:\n",
    "                title_text = f\"{exp_name_to_title(exp_name)} - {hemo_type} - {filter_type.title()}\"\n",
    "            except AttributeError:\n",
    "                title_text = f\"{exp_name_to_title(exp_name)} - {hemo_type} - Unfiltered\"\n",
    "            ax.text(\n",
    "                0.5,\n",
    "                1.12,\n",
    "                title_text,\n",
    "                fontweight=\"bold\",\n",
    "                fontsize=14,\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "                transform=ax.transAxes,\n",
    "            )\n",
    "            if show:  # TODO\n",
    "                plt.show()\n",
    "            if filepath:\n",
    "                fig.savefig(filepath, dpi=300, bbox_inches=\"tight\")\n",
    "        elif dim.lower() == \"3d\":\n",
    "            source_detector_df = self.flow_session.create_source_detector_df(\"3D\")\n",
    "            plot_df = _add_missing_pos(dim)\n",
    "            fig = plt.figure(figsize=[8, 8])\n",
    "            views = {\n",
    "                \"right\": {\"idx\": 1, \"azim\": 0},\n",
    "                \"left\": {\"idx\": 2, \"azim\": 180},\n",
    "                \"anterior\": {\"idx\": 3, \"azim\": 90},\n",
    "                \"posterior\": {\"idx\": 4, \"azim\": 270},\n",
    "            }\n",
    "            for view_name, view_info in views.items():\n",
    "                ax = fig.add_subplot(\n",
    "                    2, 2, view_info[\"idx\"], projection=\"3d\", computed_zorder=False\n",
    "                )\n",
    "                ax.view_init(elev=0, azim=view_info[\"azim\"])\n",
    "                if view_name == \"right\":\n",
    "                    source_plot_df = plot_df[plot_df[\"source_x_pos\"] >= 0]\n",
    "                    detector_plot_df = plot_df[plot_df[\"detector_x_pos\"] >= 0]\n",
    "                    ax.set_title(\"Right View\", fontweight=\"bold\", fontsize=14, y=0.85)\n",
    "                elif view_name == \"left\":\n",
    "                    source_plot_df = plot_df[plot_df[\"source_x_pos\"] <= 0]\n",
    "                    detector_plot_df = plot_df[plot_df[\"detector_x_pos\"] <= 0]\n",
    "                    ax.set_title(\"Left View\", fontweight=\"bold\", fontsize=14, y=0.85)\n",
    "                elif view_name == \"anterior\":\n",
    "                    source_plot_df = plot_df[plot_df[\"source_y_pos\"] > 0]\n",
    "                    detector_plot_df = plot_df[plot_df[\"detector_y_pos\"] > 0]\n",
    "                    ax.set_title(\n",
    "                        \"Anterior View\", fontweight=\"bold\", fontsize=14, y=0.85\n",
    "                    )\n",
    "                elif view_name == \"posterior\":\n",
    "                    source_plot_df = plot_df[plot_df[\"source_y_pos\"] <= 0]\n",
    "                    detector_plot_df = plot_df[plot_df[\"detector_y_pos\"] <= 0]\n",
    "                    ax.set_title(\n",
    "                        \"Posterior View\", fontweight=\"bold\", fontsize=14, y=0.85\n",
    "                    )\n",
    "                sig_detector_plot_df = detector_plot_df[\n",
    "                    detector_plot_df[\"p_value\"] <= 0.05\n",
    "                ]\n",
    "                not_sig_detector_plot_df = detector_plot_df.loc[\n",
    "                    (detector_plot_df[\"p_value\"] > 0.05)\n",
    "                    | (pd.isna(detector_plot_df[\"p_value\"]))\n",
    "                ]\n",
    "                scatter = ax.scatter(\n",
    "                    sig_detector_plot_df[\"detector_x_pos\"],\n",
    "                    sig_detector_plot_df[\"detector_y_pos\"],\n",
    "                    sig_detector_plot_df[\"detector_z_pos\"],\n",
    "                    s=70,\n",
    "                    c=sig_detector_plot_df[\"p_value\"],\n",
    "                    cmap=\"autumn_r\",\n",
    "                    edgecolors=\"black\",\n",
    "                    alpha=1,\n",
    "                    zorder=3,\n",
    "                )\n",
    "                ax.scatter(\n",
    "                    not_sig_detector_plot_df[\"detector_x_pos\"],\n",
    "                    not_sig_detector_plot_df[\"detector_y_pos\"],\n",
    "                    not_sig_detector_plot_df[\"detector_z_pos\"],\n",
    "                    s=20,\n",
    "                    c=\"dodgerblue\",\n",
    "                    edgecolors=\"black\",\n",
    "                    alpha=1,\n",
    "                    zorder=2,\n",
    "                )\n",
    "                ax.scatter(\n",
    "                    source_plot_df[\"source_x_pos\"],\n",
    "                    source_plot_df[\"source_y_pos\"],\n",
    "                    source_plot_df[\"source_z_pos\"],\n",
    "                    s=30,\n",
    "                    c=\"black\",\n",
    "                    zorder=1,\n",
    "                )\n",
    "                ax.patch.set_alpha(0.0)\n",
    "                ax.xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "                ax.xaxis.line.set_color(\"none\")\n",
    "                ax.yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "                ax.yaxis.line.set_color(\"none\")\n",
    "                ax.zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "                ax.zaxis.line.set_color(\"none\")\n",
    "                ax.grid(False)\n",
    "                ax.set_xticklabels([])\n",
    "                ax.set_yticklabels([])\n",
    "                ax.set_zticklabels([])\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "                ax.set_zticks([])\n",
    "            sm = plt.cm.ScalarMappable(\n",
    "                cmap=\"autumn_r\", norm=plt.Normalize(vmin=0, vmax=0.05)\n",
    "            )\n",
    "            sm.set_array([])\n",
    "            colorbar_ax = fig.add_axes([0.87, 0.32, 0.017, 0.4])\n",
    "            colorbar = fig.colorbar(sm, cax=colorbar_ax)\n",
    "            colorbar.set_label(\"p-value\", fontsize=12)\n",
    "            plt.subplots_adjust(wspace=-0.3, hspace=-0.4)\n",
    "            if show:  # TODO\n",
    "                plt.show()\n",
    "            if filepath:\n",
    "                fig.savefig(filepath, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    def create_stat_results_figs(self, overwrite: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        Create figures (.png images) for each experiment, hemodynamic type, and filter type.\n",
    "        There are individual figures for each filter type and a combined figure that has all filter types.\n",
    "        These figures are saved in the corresponding results directory.\n",
    "\n",
    "        Args:\n",
    "            overwrite (bool): Overwrite existing filter figures. Significant performance increase when False.\n",
    "                              Defaults to True.\n",
    "        \"\"\"\n",
    "\n",
    "        def _combine_figs(filedir: str) -> None:\n",
    "            \"\"\"\n",
    "            Combine three individual filter figures into one figure.\n",
    "\n",
    "            Args:\n",
    "                filedir (str): Directory of an experiment hemodynamic type.\n",
    "            \"\"\"\n",
    "            all_filenames = os.listdir(filedir)\n",
    "            all_fig_filenames = [f for f in all_filenames if not f.endswith(\".csv\")]\n",
    "            order = [\"unfiltered\", \"lowpass\", \"bandpass\"]\n",
    "            fig_filenames = sorted(\n",
    "                [f for f in all_fig_filenames if any(o in f for o in order)],\n",
    "                key=lambda f: next(i for i, o in enumerate(order) if o in f),\n",
    "            )\n",
    "            figs = [\n",
    "                Image.open(os.path.join(filedir, fig_name))\n",
    "                for fig_name in fig_filenames\n",
    "            ]\n",
    "            widths, heights = zip(*(fig.size for fig in figs))\n",
    "            total_width = sum(widths)\n",
    "            max_height = max(heights)\n",
    "            fig_out = Image.new(\"RGB\", (total_width, max_height))\n",
    "            x_offset = 0\n",
    "            for fig in figs:\n",
    "                fig_out.paste(fig, (x_offset, 0))\n",
    "                x_offset += fig.size[0]\n",
    "            filename = fig_filenames[0].rpartition(\"_\")[0] + \"_all.png\"\n",
    "            fig_out.save(os.path.join(filedir, filename))\n",
    "\n",
    "        filter_types = [\"unfiltered\", \"lowpass\", \"bandpass\"]\n",
    "        for exp_name in [\n",
    "            \"go_no_go\",\n",
    "            \"king_devick\",\n",
    "            \"n_back\",\n",
    "            \"tower_of_london\",\n",
    "            \"vSAT\",\n",
    "        ]:\n",
    "            for hemo_type in self.hemo_types:\n",
    "                for filter_type in filter_types:\n",
    "                    filedir = os.path.join(\n",
    "                        self.results_dir, \"inter_module_channels\", exp_name, hemo_type\n",
    "                    )\n",
    "                    filename = f\"{exp_name}_{hemo_type}_{filter_type}.png\"\n",
    "                    filepath = os.path.join(filedir, filename)\n",
    "                    if not os.path.exists(filepath) or overwrite:\n",
    "                        out = self.plot_stat_results(\n",
    "                            exp_name,\n",
    "                            dim=\"2D\",\n",
    "                            hemo_type=hemo_type,\n",
    "                            filter_type=filter_type,\n",
    "                            add_labels=True,\n",
    "                            filepath=filepath,\n",
    "                            show=False,\n",
    "                        )\n",
    "                filedir = os.path.join(\n",
    "                    self.results_dir, \"inter_module_channels\", exp_name, hemo_type\n",
    "                )\n",
    "                _combine_figs(filedir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_num</th>\n",
       "      <th>p_value</th>\n",
       "      <th>F_value</th>\n",
       "      <th>df1</th>\n",
       "      <th>df2</th>\n",
       "      <th>is_sig</th>\n",
       "      <th>AAL_distance</th>\n",
       "      <th>AAL_region</th>\n",
       "      <th>BA_distance</th>\n",
       "      <th>BA_region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>1900</td>\n",
       "      <td>0.003103</td>\n",
       "      <td>7.148797</td>\n",
       "      <td>2</td>\n",
       "      <td>28</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Postcentral_L</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Left-PrimMotor (4)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>3254</td>\n",
       "      <td>0.004841</td>\n",
       "      <td>6.711299</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Temporal_Inf_L</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Left-Fusiform (37)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>1468</td>\n",
       "      <td>0.010267</td>\n",
       "      <td>5.488827</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Temporal_Pole_Sup_R</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Right-BA38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>4062</td>\n",
       "      <td>0.012992</td>\n",
       "      <td>5.325874</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>True</td>\n",
       "      <td>8.49</td>\n",
       "      <td>Cerebelum_Crus2_R</td>\n",
       "      <td>26.31</td>\n",
       "      <td>Right-VisualAssoc (18)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>3686</td>\n",
       "      <td>0.014541</td>\n",
       "      <td>5.159521</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>True</td>\n",
       "      <td>4.58</td>\n",
       "      <td>Lingual_R</td>\n",
       "      <td>2.24</td>\n",
       "      <td>Right-VisualAssoc (18)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>1720</td>\n",
       "      <td>0.014804</td>\n",
       "      <td>4.975601</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Precentral_L</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Left-BA6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>3256</td>\n",
       "      <td>0.015033</td>\n",
       "      <td>5.025239</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Temporal_Inf_L</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Left-Fusiform (37)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>2876</td>\n",
       "      <td>0.015633</td>\n",
       "      <td>4.969826</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Temporal_Mid_R</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Right-BA21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>1610</td>\n",
       "      <td>0.021109</td>\n",
       "      <td>4.707966</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Frontal_Sup_R</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Right-BA6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>2262</td>\n",
       "      <td>0.025076</td>\n",
       "      <td>4.378548</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Temporal_Mid_L</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Left-BA21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>1554</td>\n",
       "      <td>0.026214</td>\n",
       "      <td>4.254350</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "      <td>True</td>\n",
       "      <td>2.24</td>\n",
       "      <td>Temporal_Pole_Mid_L</td>\n",
       "      <td>2.83</td>\n",
       "      <td>Left-BA38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>1474</td>\n",
       "      <td>0.036311</td>\n",
       "      <td>3.869555</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Temporal_Pole_Sup_R</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Right-BA38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>3170</td>\n",
       "      <td>0.039398</td>\n",
       "      <td>3.759662</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Temporal_Inf_R</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Right-Fusiform (37)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>3308</td>\n",
       "      <td>0.040545</td>\n",
       "      <td>3.721222</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Cuneus_R</td>\n",
       "      <td>2.45</td>\n",
       "      <td>Right-VisualAssoc (18)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>2194</td>\n",
       "      <td>0.042011</td>\n",
       "      <td>3.673756</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Temporal_Mid_R</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Right-BA21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>1078</td>\n",
       "      <td>0.043991</td>\n",
       "      <td>3.531025</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Frontal_Mid_R</td>\n",
       "      <td>1.00</td>\n",
       "      <td>Right-BA6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>3168</td>\n",
       "      <td>0.046483</td>\n",
       "      <td>3.591593</td>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Temporal_Inf_R</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Right-Fusiform (37)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>1714</td>\n",
       "      <td>0.046504</td>\n",
       "      <td>3.460540</td>\n",
       "      <td>2</td>\n",
       "      <td>26</td>\n",
       "      <td>True</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Precentral_L</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Left-BA6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>294</th>\n",
       "      <td>4214</td>\n",
       "      <td>0.048896</td>\n",
       "      <td>3.472706</td>\n",
       "      <td>2</td>\n",
       "      <td>22</td>\n",
       "      <td>True</td>\n",
       "      <td>3.00</td>\n",
       "      <td>Cerebelum_Crus2_R</td>\n",
       "      <td>30.08</td>\n",
       "      <td>Right-Fusiform (37)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     channel_num   p_value   F_value  df1  df2  is_sig  AAL_distance  \\\n",
       "134         1900  0.003103  7.148797    2   28    True          0.00   \n",
       "230         3254  0.004841  6.711299    2   24    True          0.00   \n",
       "102         1468  0.010267  5.488827    2   26    True          0.00   \n",
       "282         4062  0.012992  5.325874    2   22    True          8.49   \n",
       "263         3686  0.014541  5.159521    2   22    True          4.58   \n",
       "123         1720  0.014804  4.975601    2   26    True          0.00   \n",
       "231         3256  0.015033  5.025239    2   24    True          0.00   \n",
       "198         2876  0.015633  4.969826    2   24    True          0.00   \n",
       "114         1610  0.021109  4.707966    2   20    True          0.00   \n",
       "157         2262  0.025076  4.378548    2   22    True          0.00   \n",
       "112         1554  0.026214  4.254350    2   24    True          2.24   \n",
       "105         1474  0.036311  3.869555    2   22    True          0.00   \n",
       "224         3170  0.039398  3.759662    2   22    True          0.00   \n",
       "235         3308  0.040545  3.721222    2   22    True          0.00   \n",
       "153         2194  0.042011  3.673756    2   22    True          0.00   \n",
       "80          1078  0.043991  3.531025    2   26    True          0.00   \n",
       "223         3168  0.046483  3.591593    2   20    True          0.00   \n",
       "120         1714  0.046504  3.460540    2   26    True          0.00   \n",
       "294         4214  0.048896  3.472706    2   22    True          3.00   \n",
       "\n",
       "              AAL_region  BA_distance               BA_region  \n",
       "134        Postcentral_L         0.00      Left-PrimMotor (4)  \n",
       "230       Temporal_Inf_L         0.00      Left-Fusiform (37)  \n",
       "102  Temporal_Pole_Sup_R         0.00              Right-BA38  \n",
       "282    Cerebelum_Crus2_R        26.31  Right-VisualAssoc (18)  \n",
       "263            Lingual_R         2.24  Right-VisualAssoc (18)  \n",
       "123         Precentral_L         0.00                Left-BA6  \n",
       "231       Temporal_Inf_L         0.00      Left-Fusiform (37)  \n",
       "198       Temporal_Mid_R         0.00              Right-BA21  \n",
       "114        Frontal_Sup_R         0.00               Right-BA6  \n",
       "157       Temporal_Mid_L         0.00               Left-BA21  \n",
       "112  Temporal_Pole_Mid_L         2.83               Left-BA38  \n",
       "105  Temporal_Pole_Sup_R         0.00              Right-BA38  \n",
       "224       Temporal_Inf_R         0.00     Right-Fusiform (37)  \n",
       "235             Cuneus_R         2.45  Right-VisualAssoc (18)  \n",
       "153       Temporal_Mid_R         0.00              Right-BA21  \n",
       "80         Frontal_Mid_R         1.00               Right-BA6  \n",
       "223       Temporal_Inf_R         0.00     Right-Fusiform (37)  \n",
       "120         Precentral_L         0.00                Left-BA6  \n",
       "294    Cerebelum_Crus2_R        30.08     Right-Fusiform (37)  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FR = Flow_Results()\n",
    "exp_name = \"n_back\"\n",
    "hemo_type = \"HbO\"\n",
    "filter_type = \"lowpass\"\n",
    "# results = FR.run_rm_anova(exp_name, hemo_type, filter_type, corr=True)\n",
    "flow_stats_out = FR.load_flow_stats(\"n_back\", \"HbO\", \"lowpass\", sig_only=True, brain_regions=True, corr=False, depth=10)\n",
    "flow_stats_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zackg\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pingouin\\distribution.py:1006: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  W = np.product(eig) / (eig.sum() / d) ** d\n",
      "C:\\Users\\zackg\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pingouin\\distribution.py:1006: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  W = np.product(eig) / (eig.sum() / d) ** d\n"
     ]
    }
   ],
   "source": [
    "# exp_name = \"n_back\"\n",
    "# hemo_type = \"HbO\"\n",
    "for filter_type in [\"lowpass\", \"bandpass\", \"unfiltered\"]:\n",
    "    FR.run_all_rm_anovas(filter_type, corr=True, brain_regions=True, depth=0)\n",
    "    FR.run_all_rm_anovas(filter_type, corr=True, brain_regions=True, depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9c1e24752b3065052c27c07c0a22748a6118d05dd89b50e77c79e7ce74e5970"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
