{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from flow_analysis import Participant_Flow, Flow_Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import snirf\n",
    "import ctypes\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pingouin as pg\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from PIL import Image\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from adjustText import adjust_text\n",
    "from scipy.signal import butter, filtfilt, sosfiltfilt\n",
    "from typing import Union, Tuple, List\n",
    "from statistics import mean\n",
    "from behav_analysis import Participant_Behav\n",
    "from data_functions import Data_Functions, load_results, exp_name_to_title\n",
    "\n",
    "hllDll = ctypes.WinDLL(\n",
    "    r\"C:\\Program Files\\R\\R-4.2.3\\bin\\x64\\R.dll\"\n",
    ")  # path to R DLL file\n",
    "import rpy2.robjects as robjects\n",
    "\n",
    "\n",
    "class Process_Flow:\n",
    "    \"\"\"\n",
    "    This class contains functions for processing Kernel Flow data.\n",
    "    Wrapper around an snirf.Snirf object.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filepath: str) -> None:\n",
    "        \"\"\"\n",
    "        Initialize by loading SNIRF file.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to SNIRF file.\n",
    "        \"\"\"\n",
    "        self.data_fun = Data_Functions()\n",
    "        self.snirf_file = self.load_snirf(filepath)\n",
    "\n",
    "        self.missing_detector_pos_2d = [\n",
    "            [0.2700519522879849, 0.92534462173171],\n",
    "            [0.2100404073350992, 0.9599923033647436],\n",
    "            [0.1500288623822143, 0.92534462173171],\n",
    "            [0.1500288623822143, 0.856049258465643],\n",
    "            [0.2100404073350992, 0.8214015768326095],\n",
    "            [0.2700519522879849, 0.856049258465643],\n",
    "        ]\n",
    "        self.missing_source_pos_2d = [0.2100404073350983, 0.8906969400986755]\n",
    "        self.missing_detector_pos_3d = [\n",
    "            [34.18373257128052, 83.84749436111261, -3.421772079425661],\n",
    "            [24.89193921324638, 87.59280827807989, -3.877662542873584],\n",
    "            [19.49960518952535, 88.52633022589306, 4.53462776618961],\n",
    "            [23.69484819349888, 86.5963118571706, 13.38774165295894],\n",
    "            [32.93421777049451, 82.87888296072012, 13.83928277924401],\n",
    "            [37.86338484008788, 80.87503761567585, 5.394829563438814],\n",
    "        ]\n",
    "        self.missing_source_pos_3d = [\n",
    "            28.65886271209007,\n",
    "            84.52123706248807,\n",
    "            4.746746612880643,\n",
    "        ]\n",
    "        self.missing_measurement_list_data = {\n",
    "            \"measurement_list_index\": [float(\"NaN\")] * 12,\n",
    "            \"data_type\": [99999] * 12,\n",
    "            \"data_type_index\": [\"HbO\", \"HbR\"] * 6,\n",
    "            \"detector_index\": [\n",
    "                307,\n",
    "                307,\n",
    "                308,\n",
    "                308,\n",
    "                309,\n",
    "                309,\n",
    "                310,\n",
    "                310,\n",
    "                311,\n",
    "                311,\n",
    "                312,\n",
    "                312,\n",
    "            ],\n",
    "            \"source_index\": [0] * 12,\n",
    "        }\n",
    "\n",
    "    def load_snirf(self, filepath: str) -> snirf.Snirf:\n",
    "        \"\"\"\n",
    "        Load SNIRF file.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            snirf.Snirf: SNIRF file object.\n",
    "        \"\"\"\n",
    "        return snirf.Snirf(filepath, \"r+\", dynamic_loading=True)\n",
    "\n",
    "    def get_time_origin(\n",
    "        self, fmt: str = \"datetime\", offset=True\n",
    "    ) -> Union[datetime.datetime, float]:\n",
    "        \"\"\"\n",
    "        Get the time origin (start time) from the SNIRF file.\n",
    "\n",
    "        Args:\n",
    "            fmt (str, optional): Format to get the time origin in: \"datetime\" or \"timestamp\". Defaults to \"datetime\".\n",
    "            offset (bool): Offset the datetime by 4 hours. Defaults to True.\n",
    "\n",
    "        Raises:\n",
    "            Exception: Invalid fmt argument.\n",
    "\n",
    "        Returns:\n",
    "            Union[datetime.datetime, float]:\n",
    "                datetime.datetime: Time origin datetime.\n",
    "                -or-\n",
    "                float: Time origin timestamp.\n",
    "        \"\"\"\n",
    "        start_date = self.snirf_file.nirs[0].metaDataTags.MeasurementDate\n",
    "        start_time = self.snirf_file.nirs[0].metaDataTags.MeasurementTime\n",
    "        start_str = start_date + \" \" + start_time\n",
    "        if offset:\n",
    "            time_origin = datetime.datetime.strptime(\n",
    "                start_str, \"%Y-%m-%d %H:%M:%S\"\n",
    "            ) - datetime.timedelta(\n",
    "                hours=4\n",
    "            )  # 4 hour offset\n",
    "        else:\n",
    "            time_origin = datetime.datetime.strptime(start_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "        if fmt.lower() == \"datetime\":\n",
    "            return time_origin\n",
    "        elif fmt.lower() == \"timestamp\":\n",
    "            return datetime.datetime.timestamp(time_origin)\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"Invalid 'fmt' argument. Must be 'datetime' or 'timestamp'.\"\n",
    "            )\n",
    "\n",
    "    def get_subject_ID(self) -> str:\n",
    "        \"\"\"\n",
    "        Get the subject ID from the SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            str: Subject ID.\n",
    "        \"\"\"\n",
    "        return self.snirf_file.nirs[0].metaDataTags.SubjectID\n",
    "\n",
    "    def get_time_rel(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get the relative time array from the SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Relative time array.\n",
    "        \"\"\"\n",
    "        return self.snirf_file.nirs[0].data[0].time\n",
    "\n",
    "    def get_time_abs(self, fmt: str = \"datetime\") -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Convert relative time array into an absolute time array.\n",
    "\n",
    "        Args:\n",
    "            fmt (str, optional): Format to get the time array in: \"datetime\" or \"timestamp\". Defaults to \"datetime\".\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Absolute time array.\n",
    "        \"\"\"\n",
    "        time_rel = self.get_time_rel()\n",
    "        if fmt.lower() == \"datetime\":\n",
    "            time_origin_dt = self.get_time_origin(\"datetime\")\n",
    "            return np.array(\n",
    "                [\n",
    "                    datetime.timedelta(seconds=time_rel[i]) + time_origin_dt\n",
    "                    for i in range(len(time_rel))\n",
    "                ]\n",
    "            )\n",
    "        elif fmt.lower() == \"timestamp\":\n",
    "            time_origin_ts = self.get_time_origin(\"timestamp\")\n",
    "            return time_rel + time_origin_ts\n",
    "\n",
    "    def get_data(\n",
    "        self, fmt: str = \"array\", cols: list[int | list | tuple] = None\n",
    "    ) -> Union[np.ndarray, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Get timeseries data from the SNIRF file.\n",
    "\n",
    "        Args:\n",
    "            fmt (str): Format of data (np.ndarray or pd.DataFrame). Defaults to \"array\".\n",
    "            cols (list[int | list | tuple]): Data cols to select. Single col, list of cols, or slice of cols.\n",
    "                                             Defaults to None (all columns).\n",
    "\n",
    "        Raises:\n",
    "            Exception: Invalid fmt argument.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Timeseries data array.\n",
    "        \"\"\"\n",
    "        if cols or cols == 0:\n",
    "            if isinstance(cols, tuple):\n",
    "                data = (\n",
    "                    self.snirf_file.nirs[0].data[0].dataTimeSeries[:, cols[0] : cols[1]]\n",
    "                )\n",
    "            else:\n",
    "                data = self.snirf_file.nirs[0].data[0].dataTimeSeries[:, cols]\n",
    "        else:\n",
    "            data = self.snirf_file.nirs[0].data[0].dataTimeSeries\n",
    "\n",
    "        if \"array\" in fmt.lower():\n",
    "            return data\n",
    "        elif \"dataframe\" in fmt.lower():\n",
    "            return pd.DataFrame(data)\n",
    "        else:\n",
    "            raise Exception(\"Invalid fmt argument. Must be 'array' or 'dataframe'.\")\n",
    "\n",
    "    def get_source_pos(self, dim: str, add_missing: bool = False) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get the 2D or 3D source position array.\n",
    "\n",
    "        Args:\n",
    "            dim (str): Position data dimension \"2D\" or \"3D\".\n",
    "            add_missing (bool): Add missing source data. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: 2D or 3D source position array.\n",
    "        \"\"\"\n",
    "        if dim.lower() == \"2d\":\n",
    "            source_pos_array_og = self.snirf_file.nirs[0].probe.sourcePos2D\n",
    "            if add_missing:\n",
    "                source_pos_array = np.vstack(\n",
    "                    [np.array(self.missing_source_pos_2d), source_pos_array_og]\n",
    "                )\n",
    "                return source_pos_array\n",
    "            else:\n",
    "                return source_pos_array_og\n",
    "        elif dim.lower() == \"3d\":\n",
    "            source_pos_array_og = self.snirf_file.nirs[0].probe.sourcePos3D\n",
    "            if add_missing:\n",
    "                source_pos_array = np.vstack(\n",
    "                    [np.array(self.missing_source_pos_3d), source_pos_array_og]\n",
    "                )\n",
    "                return source_pos_array\n",
    "            else:\n",
    "                return source_pos_array_og\n",
    "\n",
    "    def get_detector_pos(self, dim: str, add_missing: bool = False) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get the 2D or 3D detector position array.\n",
    "\n",
    "        Args:\n",
    "            dim (str): Position data dimension \"2D\" or \"3D\".\n",
    "            add_missing (bool): Add missing detector data. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: 2D or 3D detector position array.\n",
    "        \"\"\"\n",
    "        if dim.lower() == \"2d\":\n",
    "            detector_pos_array_og = self.snirf_file.nirs[0].probe.detectorPos2D\n",
    "            if add_missing:\n",
    "                detector_pos_array = np.vstack(\n",
    "                    [np.array(self.missing_detector_pos_2d), detector_pos_array_og]\n",
    "                )\n",
    "                return detector_pos_array\n",
    "            else:\n",
    "                return detector_pos_array_og\n",
    "        elif dim.lower() == \"3d\":\n",
    "            detector_pos_array_og = self.snirf_file.nirs[0].probe.detectorPos3D\n",
    "            if add_missing:\n",
    "                detector_pos_array_og = self.snirf_file.nirs[0].probe.detectorPos3D\n",
    "                detector_pos_array = np.vstack(\n",
    "                    [np.array(self.missing_detector_pos_3d), detector_pos_array_og]\n",
    "                )\n",
    "                return detector_pos_array\n",
    "            else:\n",
    "                return detector_pos_array_og\n",
    "\n",
    "    def get_measurement_list(self) -> np.array:\n",
    "        \"\"\"\n",
    "        Get the data measurement list.\n",
    "\n",
    "        Returns:\n",
    "            np.array: Data measurement list array.\n",
    "        \"\"\"\n",
    "        return self.snirf_file.nirs[0].data[0].measurementList\n",
    "\n",
    "    def get_source_labels(self, add_missing: bool = False) -> np.array:\n",
    "        \"\"\"\n",
    "        Get the source labels.\n",
    "\n",
    "        Args:\n",
    "            add_missing (bool): Add missing source label. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            np.array: Source label array.\n",
    "        \"\"\"\n",
    "        source_labels_og = self.snirf_file.nirs[0].probe.sourceLabels\n",
    "        if add_missing:\n",
    "            missing_source_label = \"S00\"\n",
    "            source_labels = np.insert(source_labels_og, 0, missing_source_label)\n",
    "            return source_labels\n",
    "        else:\n",
    "            return source_labels_og\n",
    "\n",
    "    def get_detector_labels(self, add_missing: bool = False) -> np.array:\n",
    "        \"\"\"\n",
    "        Get the detector labels.\n",
    "\n",
    "        Args:\n",
    "            add_missing (bool): Add missing detector labels. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            np.array: Detector label array.\n",
    "        \"\"\"\n",
    "        detector_labels_og = self.snirf_file.nirs[0].probe.detectorLabels\n",
    "        if add_missing:\n",
    "            missing_detector_labels = [\n",
    "                \"D00d0\",\n",
    "                \"D00d1\",\n",
    "                \"D00d2\",\n",
    "                \"D00d3\",\n",
    "                \"D00d4\",\n",
    "                \"D00d5\",\n",
    "            ]\n",
    "            detector_labels = np.insert(detector_labels_og, 0, missing_detector_labels)\n",
    "            return detector_labels\n",
    "        else:\n",
    "            return detector_labels_og\n",
    "\n",
    "    def get_marker_df(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get a DataFrame of marker data from the \"stim\" part of the SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Marker \"stim\" data.\n",
    "        \"\"\"\n",
    "        marker_data = self.snirf_file.nirs[0].stim[0].data\n",
    "        marker_data_cols = self.snirf_file.nirs[0].stim[0].dataLabels\n",
    "        return pd.DataFrame(marker_data, columns=marker_data_cols)\n",
    "\n",
    "    def get_unique_data_types(self) -> list:\n",
    "        \"\"\"\n",
    "        Get unique data types from the SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            list: Unique data types.\n",
    "        \"\"\"\n",
    "        data_types = []\n",
    "        for i in range(len(self.snirf_file.nirs[0].data[0].measurementList)):\n",
    "            data_type = self.snirf_file.nirs[0].data[0].measurementList[i].dataType\n",
    "            if data_type not in data_types:\n",
    "                data_types.append(data_type)\n",
    "        return data_types\n",
    "\n",
    "    def get_data_type_label(self, channel_num: int) -> str:\n",
    "        \"\"\"\n",
    "        Get the data type label for a channel(s).\n",
    "\n",
    "        Args:\n",
    "            channel_num (int): Channel number to get the data type label of.\n",
    "\n",
    "        Returns:\n",
    "            str: Data type label of the channel.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            self.snirf_file.nirs[0].data[0].measurementList[channel_num].dataTypeLabel\n",
    "        )\n",
    "\n",
    "    def get_unique_data_type_labels(self) -> list:\n",
    "        \"\"\"\n",
    "        Get unique data type labels from the SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            list: Unique data type labels.\n",
    "        \"\"\"\n",
    "        data_type_labels = []\n",
    "        for i in range(len(self.snirf_file.nirs[0].data[0].measurementList)):\n",
    "            data_type_label = (\n",
    "                self.snirf_file.nirs[0].data[0].measurementList[i].dataTypeLabel\n",
    "            )\n",
    "            if data_type_label not in data_type_labels:\n",
    "                data_type_labels.append(data_type_label)\n",
    "        return data_type_labels\n",
    "\n",
    "    def create_source_dict(self) -> dict:\n",
    "        \"\"\"\n",
    "        Count the occurrences of each source index.\n",
    "\n",
    "        Returns:\n",
    "            dict: Counts for each source index.\n",
    "        \"\"\"\n",
    "        source_dict = {}\n",
    "        for i in range(len(self.snirf_file.nirs[0].data[0].measurementList)):\n",
    "            source = self.snirf_file.nirs[0].data[0].measurementList[i].sourceIndex\n",
    "            source_dict[source] = source_dict.get(source, 0) + 1\n",
    "        source_dict = self.data_fun.sort_dict(source_dict, \"keys\")\n",
    "        return source_dict\n",
    "\n",
    "    def create_detector_dict(self) -> dict:\n",
    "        \"\"\"\n",
    "        Count the occurrences of each detector index.\n",
    "\n",
    "        Returns:\n",
    "            dict: Counts for each detector index.\n",
    "        \"\"\"\n",
    "        detector_dict = {}\n",
    "        for i in range(len(self.snirf_file.nirs[0].data[0].measurementList)):\n",
    "            detector = self.snirf_file.nirs[0].data[0].measurementList[i].detectorIndex\n",
    "            detector_dict[detector] = detector_dict.get(detector, 0) + 1\n",
    "        detector_dict = self.data_fun.sort_dict(detector_dict, \"keys\")\n",
    "        return detector_dict\n",
    "\n",
    "    def create_measurement_list_df(self, add_missing: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create a DataFrame with all the data measurement list information.\n",
    "\n",
    "        Args:\n",
    "            add_missing (bool): Add missing detector data. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Data measurement list DataFrame.\n",
    "        \"\"\"\n",
    "        measurement_list = self.get_measurement_list()\n",
    "        dict_list = []\n",
    "\n",
    "        for i in range(len(measurement_list)):\n",
    "            measurement_list_i = measurement_list[i]\n",
    "            measurement_dict = {}\n",
    "            measurement_dict[\"measurement_list_index\"] = (\n",
    "                i + 1\n",
    "            )  # TODO if missing, start at detector_index 7\n",
    "            measurement_dict[\"data_type\"] = measurement_list_i.dataType\n",
    "            measurement_dict[\"data_type_index\"] = measurement_list_i.dataTypeLabel\n",
    "            measurement_dict[\"detector_index\"] = measurement_list_i.detectorIndex\n",
    "            measurement_dict[\"source_index\"] = measurement_list_i.sourceIndex\n",
    "            dict_list.append(measurement_dict)\n",
    "\n",
    "        measurement_list_df = pd.DataFrame(dict_list)\n",
    "\n",
    "        if add_missing:\n",
    "            missing_data_df = pd.DataFrame(self.missing_measurement_list_data)\n",
    "            measurement_list_df = pd.concat(\n",
    "                [missing_data_df, measurement_list_df], ignore_index=True\n",
    "            )\n",
    "            measurement_list_df[\"measurement_list_index\"] = measurement_list_df[\n",
    "                \"measurement_list_index\"\n",
    "            ].astype(pd.Int64Dtype())\n",
    "        return measurement_list_df\n",
    "\n",
    "    def create_source_df(self, dim: str, add_missing: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create a DataFrame with the source labels and 2D or 3D source positions.\n",
    "\n",
    "        Args:\n",
    "            dim (str): Position data dimension \"2D\" or \"3D\".\n",
    "            add_missing (bool): Add missing detector data. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Source labels and positions.\n",
    "        \"\"\"\n",
    "        source_labels = self.get_source_labels(add_missing)\n",
    "        if dim.lower() == \"2d\":\n",
    "            source_pos_2d = self.get_source_pos(dim, add_missing)\n",
    "            source_data = [\n",
    "                (label, *lst) for label, lst in zip(source_labels, source_pos_2d)\n",
    "            ]\n",
    "            source_df = pd.DataFrame(\n",
    "                source_data, columns=[\"source_label\", \"source_x_pos\", \"source_y_pos\"]\n",
    "            )\n",
    "        elif dim.lower() == \"3d\":\n",
    "            source_pos_3d = self.get_source_pos(dim, add_missing)\n",
    "            source_data = [\n",
    "                (label, *lst) for label, lst in zip(source_labels, source_pos_3d)\n",
    "            ]\n",
    "            source_df = pd.DataFrame(\n",
    "                source_data,\n",
    "                columns=[\n",
    "                    \"source_label\",\n",
    "                    \"source_x_pos\",\n",
    "                    \"source_y_pos\",\n",
    "                    \"source_z_pos\",\n",
    "                ],\n",
    "            )\n",
    "        # NOTE: Kernel changed source and detector label formats after a certain date\n",
    "        try:\n",
    "            f = lambda x: int(x.lstrip(\"S\"))\n",
    "            source_df.insert(1, \"source_index\", source_df[\"source_label\"].apply(f))\n",
    "        except ValueError:  # Format changed for participants 12+\n",
    "            f = lambda x: int(x[1:4].lstrip(\"0\"))\n",
    "            source_df.insert(1, \"source_index\", source_df[\"source_label\"].apply(f))\n",
    "        return source_df\n",
    "\n",
    "    def create_detector_df(self, dim: str, add_missing: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create a DataFrame with the detector labels and 2D or 3D detector positions.\n",
    "\n",
    "        Args:\n",
    "            dim (str): Position data dimension \"2D\" or \"3D\".\n",
    "            add_missing (bool): Add missing detector data. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Detector labels and positions.\n",
    "        \"\"\"\n",
    "        detector_labels = self.get_detector_labels(add_missing)\n",
    "        if dim.lower() == \"2d\":\n",
    "            detector_pos_2d = self.get_detector_pos(dim, add_missing)\n",
    "            detector_data = [\n",
    "                (label, *lst) for label, lst in zip(detector_labels, detector_pos_2d)\n",
    "            ]\n",
    "            detector_df = pd.DataFrame(\n",
    "                detector_data,\n",
    "                columns=[\"detector_label\", \"detector_x_pos\", \"detector_y_pos\"],\n",
    "            )\n",
    "        elif dim.lower() == \"3d\":\n",
    "            detector_pos_3d = self.get_detector_pos(dim, add_missing)\n",
    "            detector_data = [\n",
    "                (label, *lst) for label, lst in zip(detector_labels, detector_pos_3d)\n",
    "            ]\n",
    "            detector_df = pd.DataFrame(\n",
    "                detector_data,\n",
    "                columns=[\n",
    "                    \"detector_label\",\n",
    "                    \"detector_x_pos\",\n",
    "                    \"detector_y_pos\",\n",
    "                    \"detector_z_pos\",\n",
    "                ],\n",
    "            )\n",
    "        # NOTE: Kernel changed source and detector label formats after a certain date\n",
    "        if len(detector_df[\"detector_label\"][7]) == 5:\n",
    "            f = lambda x: int(x[1:3])\n",
    "        elif (\n",
    "            len(detector_df[\"detector_label\"][7]) == 7\n",
    "        ):  # Format changed for participants 12+\n",
    "            f = lambda x: int(x[2:4])\n",
    "\n",
    "        detector_df.insert(1, \"source_index\", detector_df[\"detector_label\"].apply(f))\n",
    "        if add_missing:\n",
    "            detector_index_col = []\n",
    "            for i in range(307, 313):\n",
    "                detector_index_col.append(i)\n",
    "            for i in range(1, detector_df.shape[0] - 5):\n",
    "                detector_index_col.append(i)\n",
    "            detector_df.insert(1, \"detector_index\", detector_index_col)\n",
    "        else:\n",
    "            detector_df.insert(1, \"detector_index\", range(1, detector_df.shape[0] + 1))\n",
    "        return detector_df\n",
    "\n",
    "    def create_source_detector_df(\n",
    "        self,\n",
    "        dim: str,\n",
    "        add_missing: bool = False,\n",
    "        MNI: bool = False,\n",
    "        brain_regions: bool = False,\n",
    "        channels: Union[List[int], int] = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create a DataFrame with the source and detector information for the inter-module channels.\n",
    "\n",
    "        Args:\n",
    "            dim (str): Position data dimension \"2D\" or \"3D\".\n",
    "            add_missing (bool): Add missing detector data. Defaults to False.\n",
    "            MNI (bool): Include MNI coordinate system columns. Defaults to False.\n",
    "            brain_regions (bool): Include AAL and BA brain region columns. Defaults to False.\n",
    "            channels (Union[List[int], int]): Return only specific channel(s). Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Source and detector information for inter-module channels.\n",
    "        \"\"\"\n",
    "        measurement_list_df = self.create_measurement_list_df(add_missing)\n",
    "        if dim.lower() == \"2d\":\n",
    "            source_df = self.create_source_df(\"2D\", add_missing)\n",
    "            detector_df = self.create_detector_df(\"2D\", add_missing)\n",
    "        elif dim.lower() == \"3d\":\n",
    "            source_df = self.create_source_df(\"3D\", add_missing)\n",
    "            detector_df = self.create_detector_df(\"3D\", add_missing)\n",
    "        source_merge = pd.merge(measurement_list_df, source_df, on=\"source_index\")\n",
    "        merged_source_detector_df = pd.merge(\n",
    "            source_merge, detector_df, on=[\"detector_index\", \"source_index\"]\n",
    "        )\n",
    "        source_detector_df = merged_source_detector_df.copy()\n",
    "        source_detector_df.insert(\n",
    "            0, \"channel_num\", source_detector_df[\"measurement_list_index\"] - 1\n",
    "        )\n",
    "\n",
    "        if isinstance(channels, int):\n",
    "            source_detector_df = source_detector_df[\n",
    "                source_detector_df[\"channel_num\"] == channels\n",
    "            ].copy()\n",
    "        elif isinstance(channels, list):\n",
    "            source_detector_df = source_detector_df[\n",
    "                source_detector_df[\"channel_num\"].isin(channels)\n",
    "            ].copy()\n",
    "\n",
    "        if dim.lower() == \"3d\":\n",
    "            # add source/detector midpoints\n",
    "            source_detector_df[\n",
    "                [\"midpoint_x_pos\", \"midpoint_y_pos\", \"midpoint_z_pos\"]\n",
    "            ] = source_detector_df.apply(\n",
    "                lambda row: self.get_midpoint(\n",
    "                    (row[\"source_x_pos\"], row[\"source_y_pos\"], row[\"source_z_pos\"]),\n",
    "                    (\n",
    "                        row[\"detector_x_pos\"],\n",
    "                        row[\"detector_y_pos\"],\n",
    "                        row[\"detector_z_pos\"],\n",
    "                    ),\n",
    "                ),\n",
    "                axis=1,\n",
    "                result_type=\"expand\",\n",
    "            )\n",
    "            # add source/detector MNI coordinates\n",
    "            if MNI or brain_regions:\n",
    "                source_detector_df[\n",
    "                    [\"source_x_MNI\", \"source_y_MNI\", \"source_z_MNI\"]\n",
    "                ] = source_detector_df.apply(\n",
    "                    lambda row: self.xyz_to_MNI(\n",
    "                        row[\"source_x_pos\"], row[\"source_y_pos\"], row[\"source_z_pos\"]\n",
    "                    ),\n",
    "                    axis=1,\n",
    "                    result_type=\"expand\",\n",
    "                )\n",
    "                source_detector_df[\n",
    "                    [\"detector_x_MNI\", \"detector_y_MNI\", \"detector_z_MNI\"]\n",
    "                ] = source_detector_df.apply(\n",
    "                    lambda row: self.xyz_to_MNI(\n",
    "                        row[\"detector_x_pos\"],\n",
    "                        row[\"detector_y_pos\"],\n",
    "                        row[\"detector_z_pos\"],\n",
    "                    ),\n",
    "                    axis=1,\n",
    "                    result_type=\"expand\",\n",
    "                )\n",
    "                source_detector_df[\n",
    "                    [\"midpoint_x_MNI\", \"midpoint_y_MNI\", \"midpoint_z_MNI\"]\n",
    "                ] = source_detector_df.apply(\n",
    "                    lambda row: self.xyz_to_MNI(\n",
    "                        row[\"midpoint_x_pos\"],\n",
    "                        row[\"midpoint_y_pos\"],\n",
    "                        row[\"midpoint_z_pos\"],\n",
    "                    ),\n",
    "                    axis=1,\n",
    "                    result_type=\"expand\",\n",
    "                )\n",
    "            if brain_regions:\n",
    "                # load R script files here to improve performance\n",
    "                with open(\n",
    "                    os.path.join(\n",
    "                        os.getcwd(), \"label4MRI\", \"R\", \"mni_to_region_index.R\"\n",
    "                    ),\n",
    "                    \"r\",\n",
    "                ) as file:\n",
    "                    mni_to_region_index_code = \"\".join(file.readlines())\n",
    "                with open(\n",
    "                    os.path.join(os.getcwd(), \"label4MRI\", \"R\", \"mni_to_region_name.R\"),\n",
    "                    \"r\",\n",
    "                ) as file:\n",
    "                    mni_to_region_name_code = \"\".join(file.readlines())\n",
    "                # evaluate R code\n",
    "                metadata_path = os.path.join(\n",
    "                    os.getcwd(), \"label4MRI\", \"data\", \"metadata.RData\"\n",
    "                )\n",
    "                load_rdata = robjects.r[\"load\"]\n",
    "                load_rdata(metadata_path)\n",
    "                robjects.r(mni_to_region_index_code)\n",
    "                robjects.r(mni_to_region_name_code)\n",
    "                # R function as Python callable\n",
    "                self.mni_to_region_name = robjects.globalenv[\"mni_to_region_name\"]\n",
    "\n",
    "                source_detector_df[\n",
    "                    [\"AAL_distance\", \"AAL_region\", \"BA_distance\", \"BA_region\"]\n",
    "                ] = source_detector_df.apply(\n",
    "                    lambda row: self.MNI_to_region(\n",
    "                        row[\"midpoint_x_MNI\"],\n",
    "                        row[\"midpoint_y_MNI\"],\n",
    "                        row[\"midpoint_z_MNI\"],\n",
    "                    ),\n",
    "                    axis=1,\n",
    "                    result_type=\"expand\",\n",
    "                )\n",
    "\n",
    "        return source_detector_df\n",
    "\n",
    "    def get_midpoint(\n",
    "        self, point1: Tuple[float, float, float], point2: Tuple[float, float, float]\n",
    "    ) -> Tuple[float, float, float]:\n",
    "        \"\"\"\n",
    "        Get the midpoint between two x, y, z coordinate points (source and detector).\n",
    "\n",
    "        Args:\n",
    "            point1 (Tuple[float, float, float]): x, y, z coordinates of the source.\n",
    "            point2 (Tuple[float, float, float]): x, y, z coordinates of the detector.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[float, float, float]: x, y, z coordinates of the source/detector midpoint.\n",
    "        \"\"\"\n",
    "        x_mid = (point1[0] + point2[0]) / 2\n",
    "        y_mid = (point1[1] + point2[1]) / 2\n",
    "        z_mid = (point1[2] + point2[2]) / 2\n",
    "        return x_mid, y_mid, z_mid\n",
    "\n",
    "    def xyz_to_MNI(self, x: float, y: float, z: float) -> Tuple[float, float, float]:\n",
    "        \"\"\"\n",
    "        Convert x, y, z coordinates to the MNI coordinate system.\n",
    "        Adapted from https://www.nitrc.org/projects/mni2orfromxyz.\n",
    "\n",
    "        Args:\n",
    "            x (float): x position.\n",
    "            y (float): y position.\n",
    "            z (float): z position.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[float, float, float]: x, y, z MNI coordinates.\n",
    "        \"\"\"\n",
    "        origin = [45, 63, 36]  # MNI origin in voxel coordinates (anterior commissure)\n",
    "        voxel_size = 2  # mm\n",
    "        mni_x = (x - origin[0]) * voxel_size\n",
    "        mni_y = (y - origin[1]) * voxel_size\n",
    "        mni_z = (z - origin[2]) * voxel_size\n",
    "        return mni_x, mni_y, mni_z\n",
    "\n",
    "    def MNI_to_region(\n",
    "        self, mni_x: float, mni_y: float, mni_z: float, print_results: bool = False\n",
    "    ) -> Tuple[float, str, float, str]:\n",
    "        \"\"\"\n",
    "        Convert MNI coordinates to the corresponding Automated Anatomical Labeling (AAL) and\n",
    "        Brodmann area (BA) including the distance from the nearest brain region.\n",
    "        Adapted from https://github.com/yunshiuan/label4MRI.\n",
    "\n",
    "        Args:\n",
    "            mni_x (float): x MNI coordinate.\n",
    "            mni_y (float): y MNI coordinate.\n",
    "            mni_z (float): z MNI coordinate.\n",
    "            print_results (bool): Print the results. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[float, str, float, str]: Distance from AAL brain region, AAL brain region,\n",
    "                                           distance from BA brain region, and BA region.\n",
    "        \"\"\"\n",
    "        if hasattr(self.__class__, \"mni_to_region_name\"):\n",
    "            mni_to_region_name = self.mni_to_region_name\n",
    "        else:\n",
    "            # load R script files\n",
    "            with open(\n",
    "                os.path.join(os.getcwd(), \"label4MRI\", \"R\", \"mni_to_region_index.R\"),\n",
    "                \"r\",\n",
    "            ) as file:\n",
    "                mni_to_region_index_code = \"\".join(file.readlines())\n",
    "            with open(\n",
    "                os.path.join(os.getcwd(), \"label4MRI\", \"R\", \"mni_to_region_name.R\"), \"r\"\n",
    "            ) as file:\n",
    "                mni_to_region_name_code = \"\".join(file.readlines())\n",
    "            # evaluate R code\n",
    "            metadata_path = os.path.join(\n",
    "                os.getcwd(), \"label4MRI\", \"data\", \"metadata.RData\"\n",
    "            )\n",
    "            load_rdata = robjects.r[\"load\"]\n",
    "            load_rdata(metadata_path)\n",
    "            robjects.r(mni_to_region_index_code)\n",
    "            robjects.r(mni_to_region_name_code)\n",
    "            # R function as Python callable\n",
    "            mni_to_region_name = robjects.globalenv[\"mni_to_region_name\"]\n",
    "\n",
    "        result = mni_to_region_name(float(mni_x), float(mni_y), float(mni_z))\n",
    "\n",
    "        aal_distance = result.rx2(\"aal.distance\")\n",
    "        aal_label = result.rx2(\"aal.label\")\n",
    "        ba_distance = result.rx2(\"ba.distance\")\n",
    "        ba_label = result.rx2(\"ba.label\")\n",
    "\n",
    "        # convert R vector objects\n",
    "        aal_distance = round(list(aal_distance)[0], 2)\n",
    "        aal_label = list(aal_label)[0]\n",
    "        ba_distance = round(list(ba_distance)[0], 2)\n",
    "        ba_label = list(ba_label)[0]\n",
    "\n",
    "        if print_results:\n",
    "            print(f\"AAL distance: {aal_distance}\")\n",
    "            print(f\"AAL region: {aal_label}\")\n",
    "            print(f\"BA distance: {ba_distance}\")\n",
    "            print(f\"BA region: {ba_label}\")\n",
    "\n",
    "        return aal_distance, aal_label, ba_distance, ba_label\n",
    "\n",
    "    def plot_pos(\n",
    "        self,\n",
    "        dim: str,\n",
    "        add_labels: bool = False,\n",
    "        minimal: bool = True,\n",
    "        hemo_type: str = \"HbO\",\n",
    "        add_missing: bool = True,\n",
    "        azim: int = 120,\n",
    "        view: str = None,\n",
    "        channels: Union[List[int], int] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Plot the detector and source 2D or 3D positions.\n",
    "\n",
    "        Args:\n",
    "            dim (str): Position data dimension \"2D\" or \"3D\".\n",
    "            add_labels (bool): Add a channel number label at each source position. Defaults to False.\n",
    "            minimal (bool): Show minimal plot elements. Defaults to False.\n",
    "            hemo_type (str): Hemodynamic type. \"HbO\" or \"HbR\". Defaults to \"HbO\".\n",
    "            add_missing (bool): Add missing detector/source positions. Defaults to True.\n",
    "            azim (int): 3D plot azimuth. Defaults to 120 degrees.\n",
    "            view: 3D plot view. \"Anterior\", \"Posterior\", \"Left\" or \"Right\". Defaults to None.\n",
    "            channels (Union[List[int], int]): Highlight specific channel(s). Defaults to None.\n",
    "        \"\"\"\n",
    "\n",
    "        def _get_highlight_channels(\n",
    "            plot_df: pd.DataFrame, channels: Union[List[int], int]\n",
    "        ) -> pd.DataFrame:\n",
    "            if isinstance(channels, int):\n",
    "                return plot_df[plot_df[\"channel_num\"] == channels]\n",
    "            elif isinstance(channels, list):\n",
    "                return plot_df[plot_df[\"channel_num\"].isin(channels)]\n",
    "\n",
    "        def _add_labels(\n",
    "            plot_df: pd.DataFrame,\n",
    "            dim: int,\n",
    "            opt_type: str = \"source\",\n",
    "            label_x_offset: int = 0,\n",
    "            label_y_offset: int = 0,\n",
    "            label_z_offset: int = 0,\n",
    "        ):\n",
    "            if dim.lower() == \"2d\":\n",
    "                labels = plot_df[\"channel_num\"]\n",
    "                if opt_type == \"source\":\n",
    "                    x_pos = list(plot_df[\"source_x_pos\"])\n",
    "                    y_pos = list(plot_df[\"source_y_pos\"])\n",
    "                elif opt_type == \"detector\":\n",
    "                    x_pos = list(plot_df[\"detector_x_pos\"])\n",
    "                    y_pos = list(plot_df[\"detector_y_pos\"])\n",
    "                for i, label in enumerate(labels):\n",
    "                    try:\n",
    "                        ax.annotate(\n",
    "                            label,\n",
    "                            (x_pos[i] - 0.007, y_pos[i] - 0.007),\n",
    "                            xytext=(label_x_offset, label_y_offset),\n",
    "                            textcoords=\"offset points\",\n",
    "                            fontsize=8,\n",
    "                            ha=\"left\",\n",
    "                            va=\"center\",\n",
    "                            bbox=dict(\n",
    "                                boxstyle=\"round,pad=0.15\",\n",
    "                                edgecolor=\"black\",\n",
    "                                facecolor=\"white\",\n",
    "                                alpha=1,\n",
    "                            ),\n",
    "                            arrowprops=dict(\n",
    "                                arrowstyle=\"-|>\",\n",
    "                                facecolor=\"black\",\n",
    "                                linewidth=2,\n",
    "                                shrinkA=0,\n",
    "                                shrinkB=0,\n",
    "                            ),\n",
    "                        )\n",
    "                    except TypeError:\n",
    "                        ax.annotate(\n",
    "                            \"NaN\",\n",
    "                            (x_pos[i] - 0.007, y_pos[i] - 0.007),\n",
    "                            xytext=(label_x_offset, label_y_offset),\n",
    "                            textcoords=\"offset points\",\n",
    "                            fontsize=8,\n",
    "                            ha=\"left\",\n",
    "                            va=\"center\",\n",
    "                            bbox=dict(\n",
    "                                boxstyle=\"round,pad=0.15\",\n",
    "                                edgecolor=\"black\",\n",
    "                                facecolor=\"white\",\n",
    "                                alpha=1,\n",
    "                            ),\n",
    "                            arrowprops=dict(\n",
    "                                arrowstyle=\"-|>\",\n",
    "                                facecolor=\"black\",\n",
    "                                linewidth=2,\n",
    "                                shrinkA=0,\n",
    "                                shrinkB=0,\n",
    "                            ),\n",
    "                        )\n",
    "            elif dim.lower() == \"3d\":\n",
    "                labels = plot_df[\"channel_num\"]\n",
    "                if opt_type == \"source\":\n",
    "                    x_pos = list(plot_df[\"source_x_pos\"])\n",
    "                    y_pos = list(plot_df[\"source_y_pos\"])\n",
    "                    z_pos = list(plot_df[\"source_z_pos\"])\n",
    "                elif opt_type == \"detector\":\n",
    "                    x_pos = list(plot_df[\"detector_x_pos\"])\n",
    "                    y_pos = list(plot_df[\"detector_y_pos\"])\n",
    "                    z_pos = list(plot_df[\"detector_z_pos\"])\n",
    "                for i, label in enumerate(labels):\n",
    "                    label_x = x_pos[i] + label_x_offset\n",
    "                    label_y = y_pos[i] + label_y_offset\n",
    "                    label_z = z_pos[i] + label_z_offset\n",
    "                    arrow_length = np.array(\n",
    "                        [label_x_offset, label_y_offset, label_z_offset]\n",
    "                    )\n",
    "                    ax.quiver(\n",
    "                        x_pos[i] + arrow_length[0],\n",
    "                        y_pos[i] + arrow_length[1],\n",
    "                        z_pos[i] + arrow_length[2],\n",
    "                        -arrow_length[0],\n",
    "                        -arrow_length[1],\n",
    "                        -arrow_length[2],\n",
    "                        color=\"black\",\n",
    "                        linewidth=1,\n",
    "                        arrow_length_ratio=0.3,\n",
    "                    )\n",
    "                    try:\n",
    "                        ax.text(\n",
    "                            label_x,\n",
    "                            label_y,\n",
    "                            label_z,\n",
    "                            label,\n",
    "                            fontsize=8,\n",
    "                            ha=\"left\",\n",
    "                            va=\"center\",\n",
    "                            bbox=dict(\n",
    "                                boxstyle=\"round,pad=0.15\",\n",
    "                                edgecolor=\"black\",\n",
    "                                facecolor=\"white\",\n",
    "                                alpha=1,\n",
    "                            ),\n",
    "                        )\n",
    "                    except TypeError:\n",
    "                        ax.text(\n",
    "                            label_x,\n",
    "                            label_y,\n",
    "                            label_z,\n",
    "                            \"NaN\",\n",
    "                            fontsize=8,\n",
    "                            ha=\"left\",\n",
    "                            va=\"center\",\n",
    "                            bbox=dict(\n",
    "                                boxstyle=\"round,pad=0.15\",\n",
    "                                edgecolor=\"black\",\n",
    "                                facecolor=\"white\",\n",
    "                                alpha=1,\n",
    "                            ),\n",
    "                        )\n",
    "\n",
    "        source_detector_df = self.create_source_detector_df(dim, add_missing)\n",
    "        source_detector_hemo = source_detector_df[\n",
    "            source_detector_df[\"data_type_index\"] == hemo_type\n",
    "        ]\n",
    "        uni_source_label_df = source_detector_hemo.drop_duplicates(\n",
    "            subset=\"source_index\"\n",
    "        )\n",
    "\n",
    "        if dim.lower() == \"2d\":\n",
    "            x_detector = list(source_detector_hemo[\"detector_x_pos\"])\n",
    "            y_detector = list(source_detector_hemo[\"detector_y_pos\"])\n",
    "            x_source = list(uni_source_label_df[\"source_x_pos\"])\n",
    "            y_source = list(uni_source_label_df[\"source_y_pos\"])\n",
    "            fig = plt.figure(figsize=(8, 8))\n",
    "            ax = fig.add_subplot(111)\n",
    "            ax.scatter(x_detector, y_detector, s=40)\n",
    "            ax.scatter(x_source, y_source, s=70)\n",
    "            if add_labels and not channels:\n",
    "                label_x_offset = 10\n",
    "                label_y_offset = 15\n",
    "                _add_labels(\n",
    "                    uni_source_label_df, dim, \"source\", label_x_offset, label_y_offset\n",
    "                )\n",
    "            if minimal:\n",
    "                ax.set_title(\"Anterior\", fontweight=\"bold\", fontsize=14)\n",
    "                ax.set_xticklabels([])\n",
    "                ax.set_yticklabels([])\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "                for spine in ax.spines.values():\n",
    "                    spine.set_visible(False)\n",
    "                ax.text(\n",
    "                    0.5,\n",
    "                    -0.06,\n",
    "                    \"Posterior\",\n",
    "                    fontweight=\"bold\",\n",
    "                    fontsize=14,\n",
    "                    ha=\"center\",\n",
    "                    va=\"bottom\",\n",
    "                    transform=ax.transAxes,\n",
    "                )\n",
    "            else:\n",
    "                ax.set_title(\"Detector/Source 2D Plot\")\n",
    "                ax.set_xlabel(\"X-Position (mm)\")\n",
    "                ax.set_ylabel(\"Y-Position (mm)\")\n",
    "                ax.legend([\"Detector\", \"Source\"])\n",
    "            if channels:\n",
    "                label_x_offset = 12\n",
    "                label_y_offset = 12\n",
    "                highlight_rows = _get_highlight_channels(source_detector_hemo, channels)\n",
    "                _add_labels(\n",
    "                    highlight_rows, dim, \"detector\", label_x_offset, label_y_offset\n",
    "                )\n",
    "\n",
    "        elif dim.lower() == \"3d\":\n",
    "            fig = plt.figure(figsize=(8, 8))\n",
    "            ax = fig.add_subplot(111, projection=\"3d\", computed_zorder=False)\n",
    "            label_x_offset = 10\n",
    "            label_y_offset = 10\n",
    "            label_z_offset = 10\n",
    "            if not view:\n",
    "                x_detector = list(source_detector_hemo[\"detector_x_pos\"])\n",
    "                y_detector = list(source_detector_hemo[\"detector_y_pos\"])\n",
    "                z_detector = list(source_detector_hemo[\"detector_z_pos\"])\n",
    "                x_source = list(uni_source_label_df[\"source_x_pos\"])\n",
    "                y_source = list(uni_source_label_df[\"source_y_pos\"])\n",
    "                z_source = list(uni_source_label_df[\"source_z_pos\"])\n",
    "                ax.scatter(x_detector, y_detector, z_detector, s=30)\n",
    "                ax.scatter(x_source, y_source, z_source, s=55)\n",
    "                ax.view_init(azim=azim)\n",
    "                if add_labels and not channels:\n",
    "                    _add_labels(\n",
    "                        uni_source_label_df,\n",
    "                        dim,\n",
    "                        \"source\",\n",
    "                        label_x_offset,\n",
    "                        label_y_offset,\n",
    "                        label_z_offset,\n",
    "                    )\n",
    "                if channels:\n",
    "                    highlight_rows = _get_highlight_channels(\n",
    "                        source_detector_hemo, channels\n",
    "                    )\n",
    "                    _add_labels(\n",
    "                        highlight_rows,\n",
    "                        dim,\n",
    "                        \"detector\",\n",
    "                        label_x_offset,\n",
    "                        label_y_offset,\n",
    "                        label_z_offset,\n",
    "                    )\n",
    "            else:\n",
    "                views = {\n",
    "                    \"right\": 0,\n",
    "                    \"left\": 180,\n",
    "                    \"anterior\": 90,\n",
    "                    \"posterior\": 270,\n",
    "                }\n",
    "                ax.view_init(elev=0, azim=views[view])\n",
    "                if view == \"right\":\n",
    "                    source_plot_df = uni_source_label_df[\n",
    "                        uni_source_label_df[\"source_x_pos\"] >= 0\n",
    "                    ]\n",
    "                    detector_plot_df = source_detector_hemo[\n",
    "                        source_detector_hemo[\"detector_x_pos\"] >= 0\n",
    "                    ]\n",
    "                    ax.set_title(\"Right View\", fontweight=\"bold\", fontsize=14, y=0.85)\n",
    "                elif view == \"left\":\n",
    "                    source_plot_df = uni_source_label_df[\n",
    "                        uni_source_label_df[\"source_x_pos\"] <= 0\n",
    "                    ]\n",
    "                    detector_plot_df = source_detector_hemo[\n",
    "                        source_detector_hemo[\"detector_x_pos\"] <= 0\n",
    "                    ]\n",
    "                    ax.set_title(\"Left View\", fontweight=\"bold\", fontsize=14, y=0.85)\n",
    "                elif view == \"anterior\":\n",
    "                    source_plot_df = uni_source_label_df[\n",
    "                        uni_source_label_df[\"source_y_pos\"] > 0\n",
    "                    ]\n",
    "                    detector_plot_df = source_detector_hemo[\n",
    "                        source_detector_hemo[\"detector_y_pos\"] > 0\n",
    "                    ]\n",
    "                    ax.set_title(\n",
    "                        \"Anterior View\", fontweight=\"bold\", fontsize=14, y=0.85\n",
    "                    )\n",
    "                elif view == \"posterior\":\n",
    "                    source_plot_df = uni_source_label_df[\n",
    "                        uni_source_label_df[\"source_y_pos\"] <= 0\n",
    "                    ]\n",
    "                    detector_plot_df = source_detector_hemo[\n",
    "                        source_detector_hemo[\"detector_y_pos\"] <= 0\n",
    "                    ]\n",
    "                    ax.set_title(\n",
    "                        \"Posterior View\", fontweight=\"bold\", fontsize=14, y=0.85\n",
    "                    )\n",
    "                if add_labels and not channels:\n",
    "                    try:\n",
    "                        _add_labels(\n",
    "                            source_plot_df,\n",
    "                            dim,\n",
    "                            \"source\",\n",
    "                            label_x_offset,\n",
    "                            label_y_offset,\n",
    "                            label_z_offset,\n",
    "                        )\n",
    "                    except NameError:\n",
    "                        _add_labels(\n",
    "                            source_plot_df,\n",
    "                            dim,\n",
    "                            \"source\",\n",
    "                            label_x_offset,\n",
    "                            label_y_offset,\n",
    "                            label_z_offset,\n",
    "                        )\n",
    "                ax.scatter(\n",
    "                    detector_plot_df[\"detector_x_pos\"],\n",
    "                    detector_plot_df[\"detector_y_pos\"],\n",
    "                    detector_plot_df[\"detector_z_pos\"],\n",
    "                    s=30,\n",
    "                    alpha=1,\n",
    "                    zorder=2,\n",
    "                )\n",
    "                ax.scatter(\n",
    "                    source_plot_df[\"source_x_pos\"],\n",
    "                    source_plot_df[\"source_y_pos\"],\n",
    "                    source_plot_df[\"source_z_pos\"],\n",
    "                    s=55,\n",
    "                    alpha=1,\n",
    "                    zorder=1,\n",
    "                )\n",
    "                if channels:\n",
    "                    highlight_rows = _get_highlight_channels(detector_plot_df, channels)\n",
    "                    _add_labels(\n",
    "                        highlight_rows,\n",
    "                        dim,\n",
    "                        \"detector\",\n",
    "                        label_x_offset,\n",
    "                        label_y_offset,\n",
    "                        label_z_offset,\n",
    "                    )\n",
    "\n",
    "            if minimal:\n",
    "                ax.patch.set_alpha(0.0)\n",
    "                ax.xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "                ax.xaxis.line.set_color(\"none\")\n",
    "                ax.yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "                ax.yaxis.line.set_color(\"none\")\n",
    "                ax.zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "                ax.zaxis.line.set_color(\"none\")\n",
    "                ax.set_xticklabels([])\n",
    "                ax.set_yticklabels([])\n",
    "                ax.set_zticklabels([])\n",
    "                if not view:\n",
    "                    if azim > 180:\n",
    "                        ax.set_xlabel(\"Posterior\", fontweight=\"bold\", fontsize=14)\n",
    "                    else:\n",
    "                        ax.set_xlabel(\"Anterior\", fontweight=\"bold\", fontsize=14)\n",
    "                    if azim >= 270 or (azim >= 0 and azim <= 90):\n",
    "                        ax.set_ylabel(\"Right\", fontweight=\"bold\", fontsize=14)\n",
    "                    else:\n",
    "                        ax.set_ylabel(\"Left\", fontweight=\"bold\", fontsize=14)\n",
    "            else:\n",
    "                ax.set_title(\"Detector/Source 3D Plot\")\n",
    "                ax.set_xlabel(\"X-Position (mm)\")\n",
    "                ax.set_ylabel(\"Y-Position (mm)\")\n",
    "                ax.set_zlabel(\"Z-Position (mm)\")\n",
    "                ax.legend([\"Detector\", \"Source\"])\n",
    "\n",
    "\n",
    "class Participant_Flow:\n",
    "    \"\"\"\n",
    "    This class contains functions, data structures, and info necessary for\n",
    "    processing Kernel Flow data from the experiments.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, par_num):\n",
    "        self.data_fun = Data_Functions()\n",
    "        self.adj_ts_markers = True\n",
    "        self.par_behav = Participant_Behav(par_num, self.adj_ts_markers)\n",
    "        self.par_num, self.par_ID = self.data_fun.process_par(par_num)\n",
    "        self.flow_raw_data_dir = os.path.join(\n",
    "            self.par_behav.raw_data_dir, self.par_ID, \"kernel_data\"\n",
    "        )\n",
    "        self.flow_processed_data_dir = os.path.join(\n",
    "            os.getcwd(), \"processed_data\", \"flow\"\n",
    "        )\n",
    "        self.flow = self.load_flow_session(\"1001\", wrapper=True)\n",
    "        self.flow_session_dict = self.create_flow_session_dict(wrapper=True)\n",
    "        self.time_offset_dict = self.create_time_offset_dict()\n",
    "        self.plot_color_dict = {\n",
    "            0: \"purple\",\n",
    "            1: \"orange\",\n",
    "            2: \"green\",\n",
    "            3: \"yellow\",\n",
    "            4: \"pink\",\n",
    "            5: \"skyblue\",\n",
    "        }\n",
    "\n",
    "    def calc_time_offset(self, exp_name: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the time offset (in seconds) between the behavioral and Kernel Flow data\n",
    "        files. Number of seconds that the Kernel Flow data is ahead of the behavioral data.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "\n",
    "        Returns:\n",
    "            float: Time offset (in seconds).\n",
    "        \"\"\"\n",
    "        exp = self.par_behav.get_exp(exp_name)\n",
    "        exp_start_ts = exp.start_ts\n",
    "        marker_sent_time = float(exp.marker_data[\"start_marker\"][\"sent_time\"])\n",
    "        session = self.par_behav.get_key_from_value(\n",
    "            self.par_behav.session_dict, exp_name\n",
    "        )\n",
    "        marker_df = self.create_abs_marker_df(session)\n",
    "        row = marker_df.loc[marker_df[\"Marker\"].str.startswith(exp_name)].reset_index()\n",
    "        if (\n",
    "            exp_name == \"go_no_go\"\n",
    "        ):  # Go/No-go experiment is missing start timestamp marker\n",
    "            try:\n",
    "                kernel_start_ts = row.loc[0, \"Start timestamp\"]\n",
    "                time_offset = kernel_start_ts - (exp_start_ts + marker_sent_time)\n",
    "            except:\n",
    "                time_offset = \"NaN\"\n",
    "        else:\n",
    "            kernel_start_ts = row.loc[0, \"Start timestamp\"]\n",
    "            time_offset = kernel_start_ts - (exp_start_ts + marker_sent_time)\n",
    "        return float(time_offset)\n",
    "\n",
    "    def create_time_offset_dict(self) -> dict:\n",
    "        \"\"\"\n",
    "        Create a dictionary containing the time offset (in seconds) for each experiment.\n",
    "\n",
    "        Returns:\n",
    "            dict: Time offset dictionary.\n",
    "        \"\"\"\n",
    "        time_offset_dict = {}\n",
    "        for exp_name in self.par_behav.exp_order:\n",
    "            if (\n",
    "                exp_name == \"go_no_go\"\n",
    "            ):  # Go/No-go experiment is missing start timestamp marker\n",
    "                if np.isnan(self.calc_time_offset(exp_name)):\n",
    "                    session = self.par_behav.get_key_from_value(\n",
    "                        self.par_behav.session_dict, exp_name\n",
    "                    )\n",
    "                    session_exp_names = self.par_behav.session_dict[session]\n",
    "                    other_exp_names = [\n",
    "                        temp_exp_name\n",
    "                        for temp_exp_name in session_exp_names\n",
    "                        if temp_exp_name != \"go_no_go\"\n",
    "                    ]\n",
    "                    other_exp_time_offsets = []\n",
    "                    for temp_exp_name in other_exp_names:\n",
    "                        time_offset = self.calc_time_offset(temp_exp_name)\n",
    "                        other_exp_time_offsets.append(time_offset)\n",
    "                    avg_time_offset = np.mean(other_exp_time_offsets)\n",
    "                    time_offset_dict[exp_name] = avg_time_offset\n",
    "            else:\n",
    "                time_offset_dict[exp_name] = self.calc_time_offset(exp_name)\n",
    "        for session, exp_list in self.par_behav.session_dict.items():\n",
    "            session_offset = np.mean(\n",
    "                [time_offset_dict[exp_name] for exp_name in exp_list]\n",
    "            )\n",
    "            time_offset_dict[session] = session_offset\n",
    "        return time_offset_dict\n",
    "\n",
    "    def get_time_offset(self, exp_name: str) -> float:\n",
    "        \"\"\"\n",
    "        Get the time offset for an experiment.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Experiment name.\n",
    "\n",
    "        Returns:\n",
    "            float: Time offset (in seconds).\n",
    "        \"\"\"\n",
    "        return self.time_offset_dict[exp_name]\n",
    "\n",
    "    def offset_time_array(self, exp_name: str, time_array: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Offset a Kernel Flow datetime array for an experiment by the time-offset.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "            time_array (np.ndarray): Datetime array.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Time-offset datetime array.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            time_offset = self.get_time_offset(exp_name)\n",
    "        except KeyError:  # if experiment start time is missing, use avg of other session experiments\n",
    "            time_offset_list = []\n",
    "            for exp_name in self.par_behav.exp_order:\n",
    "                try:\n",
    "                    time_offset = self.get_time_offset(exp_name)\n",
    "                    time_offset_list.append(time_offset)\n",
    "                except KeyError:\n",
    "                    pass\n",
    "            time_offset = mean(time_offset_list)\n",
    "        time_offset_dt = datetime.timedelta(seconds=time_offset)\n",
    "        time_abs_dt_offset = time_array - time_offset_dt\n",
    "        return time_abs_dt_offset\n",
    "\n",
    "    def load_flow_session(\n",
    "        self, session: list[str | int], wrapper: bool = False\n",
    "    ) -> snirf.Snirf:\n",
    "        \"\"\"\n",
    "        Load Kernel Flow data for an experiment session.\n",
    "\n",
    "        Args:\n",
    "            session list[str | int]: Experiment session.\n",
    "            wrapper (bool, optional) Option to return Process_Flow-wrapped SNIRF file.\n",
    "                                     Defaults to False.\n",
    "\n",
    "        Raises:\n",
    "            Exception: Invalid session number argument.\n",
    "\n",
    "        Returns:\n",
    "            snirf.Snirf: SNIRF file object.\n",
    "            -or-\n",
    "            Process_Flow object for each experiment session.\n",
    "        \"\"\"\n",
    "        if isinstance(session, str):\n",
    "            if \"session\" not in session:\n",
    "                session = f\"session_{session}\"\n",
    "        elif isinstance(session, int):\n",
    "            session = f\"session_{session}\"\n",
    "        try:\n",
    "            session_dir = os.path.join(self.flow_raw_data_dir, session)\n",
    "            filename = os.listdir(session_dir)[0]\n",
    "            filepath = os.path.join(session_dir, filename)\n",
    "            if wrapper:\n",
    "                return Process_Flow(filepath)\n",
    "            else:\n",
    "                return Process_Flow(filepath).snirf_file\n",
    "        except:\n",
    "            print(\"Invalid session number.\")\n",
    "            raise\n",
    "\n",
    "    def load_flow_exp(self, exp_name: str, filter_type: str = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load Kernel Flow data for the time frame of a specified experiment.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "            filter_type (str): Filter to apply to the data. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Kernel Flow data for an experiment.\n",
    "        \"\"\"\n",
    "        session = self.par_behav.get_key_from_value(\n",
    "            self.par_behav.session_dict, exp_name\n",
    "        )\n",
    "        flow_session = self.load_flow_session(session, wrapper=True)\n",
    "\n",
    "        start_dt = self.par_behav.get_start_dt(exp_name, self.adj_ts_markers)\n",
    "        end_dt = self.par_behav.get_end_dt(exp_name, self.adj_ts_markers)\n",
    "        time_abs_dt = flow_session.get_time_abs(\"datetime\")\n",
    "        time_abs_dt_offset = self.offset_time_array(exp_name, time_abs_dt)\n",
    "        start_idx = self.par_behav.get_start_index_dt(time_abs_dt_offset, start_dt)\n",
    "        end_idx = self.par_behav.get_end_index_dt(time_abs_dt_offset, end_dt)\n",
    "\n",
    "        flow_data = flow_session.get_data(\"dataframe\")\n",
    "        if filter_type.lower() == \"lowpass\":\n",
    "            flow_data = flow_data.apply(lambda x: self.lowpass_filter(x), axis=0)\n",
    "        elif filter_type.lower() == \"bandpass\":\n",
    "            flow_data = flow_data.apply(lambda x: self.bandpass_filter(x), axis=0)\n",
    "        flow_data.insert(0, \"datetime\", time_abs_dt_offset)\n",
    "        return flow_data.iloc[start_idx:end_idx, :]\n",
    "\n",
    "    def create_flow_session_dict(self, wrapper: bool = False) -> dict:\n",
    "        \"\"\"\n",
    "        Create a dictionary of Kernel Flow data for all experiment sessions.\n",
    "\n",
    "        wrapper (bool, optional) Option to return Process_Flow-wrapped SNIRF file.\n",
    "                                 Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            dict: Kernel Flow data for all experiment sessions.\n",
    "                keys:\n",
    "                    \"session_1001\", \"session_1002\", \"session_1003\"\n",
    "                values:\n",
    "                    SNIRF file object for each experiment session\n",
    "                    -or-\n",
    "                    Process_Flow object for each experiment session\n",
    "        \"\"\"\n",
    "        flow_session_dict = {}\n",
    "        for session in self.par_behav.session_dict.keys():\n",
    "            flow_session_dict[session] = self.load_flow_session(session, wrapper)\n",
    "        return flow_session_dict\n",
    "\n",
    "    def create_abs_marker_df(self, session: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Convert the \"stim\" marker DataFrame into absolute time.\n",
    "\n",
    "        Args:\n",
    "            session (str): Experiment session.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Marker \"stim\" data in absolute time.\n",
    "        \"\"\"\n",
    "        marker_df = self.flow_session_dict[session].get_marker_df()\n",
    "        time_origin_ts = self.flow_session_dict[session].get_time_origin(\"timestamp\")\n",
    "        marker_df[\"Timestamp\"] = marker_df[\"Timestamp\"] + time_origin_ts\n",
    "        marker_df.rename({\"Timestamp\": \"Start timestamp\"}, axis=1, inplace=True)\n",
    "\n",
    "        for idx, row in marker_df.iterrows():\n",
    "            end_ts = row[\"Start timestamp\"] + row[\"Duration\"]\n",
    "            marker_df.at[idx, \"End timestamp\"] = end_ts\n",
    "            exp_num = int(row[\"Experiment\"])\n",
    "            exp_name = self.par_behav.marker_dict[exp_num]\n",
    "            marker_df.at[idx, \"Experiment\"] = exp_name\n",
    "\n",
    "        marker_df.rename({\"Experiment\": \"Marker\"}, axis=1, inplace=True)\n",
    "        marker_df.drop([\"Value\"], axis=1, inplace=True)\n",
    "        marker_df = marker_df[\n",
    "            [\"Marker\", \"Start timestamp\", \"Duration\", \"End timestamp\"]\n",
    "        ]\n",
    "        return marker_df\n",
    "\n",
    "    def create_exp_stim_response_dict(\n",
    "        self, exp_name: str, filter_type: str = None\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Create a dictionary that contains the processed Kernel Flow data in response\n",
    "        to a stimulus. It is organized by block (keys) and for each block, the value is\n",
    "        a list of Pandas series. Each series is normalized, averaged, Kernel Flow data\n",
    "        during a presented stimulus duration for each channel. Each block is baselined\n",
    "        to the first 5 seconds, and the stim response is averaged over the stimulus\n",
    "        presentation duration.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "            filter_type (str): Filter to apply to the data. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            dict:\n",
    "                keys:\n",
    "                    \"block 1\", \"block 2\", ... \"block N\"\n",
    "                values:\n",
    "                    dicts:\n",
    "                        keys:\n",
    "                            \"trial 1\", \"trial 2\", ... \"trial N\"\n",
    "                        values:\n",
    "                            lists of averaged, normalized Kernel Flow data series for each\n",
    "                            channel during the stimulus duration\n",
    "        \"\"\"\n",
    "        exp_results = load_results(\n",
    "            self.par_behav.processed_data_dir, exp_name, self.par_behav.par_num\n",
    "        )\n",
    "        flow_exp = self.load_flow_exp(exp_name, filter_type)\n",
    "        session = self.par_behav.get_key_from_value(\n",
    "            self.par_behav.session_dict, exp_name\n",
    "        )\n",
    "        ts_list = self.flow_session_dict[session].get_time_abs(\"timestamp\")\n",
    "        exp_time_offset = self.time_offset_dict[exp_name]\n",
    "        exp_by_block = self.par_behav.by_block_ts_dict[exp_name]\n",
    "\n",
    "        blocks = list(exp_results[\"block\"].unique())\n",
    "        exp_stim_resp_dict = {\n",
    "            block: {} for block in blocks\n",
    "        }  # initialize with unique blocks\n",
    "        processed_blocks = []\n",
    "\n",
    "        if exp_name == \"king_devick\":  # normalize all blocks to the first block\n",
    "            (first_block_start_ts, first_block_end_ts) = next(\n",
    "                iter(exp_by_block.keys())\n",
    "            )  # start/end of first block\n",
    "            first_block_start_ts_offset = first_block_start_ts + exp_time_offset\n",
    "            first_block_start_idx, _ = self.data_fun.find_closest_ts(\n",
    "                first_block_start_ts_offset, ts_list\n",
    "            )\n",
    "            first_block_end_ts_offset = first_block_end_ts + exp_time_offset\n",
    "            first_block_end_idx, _ = self.data_fun.find_closest_ts(\n",
    "                first_block_end_ts_offset, ts_list\n",
    "            )\n",
    "            baseline_rows = flow_exp.loc[\n",
    "                first_block_start_idx : first_block_start_idx + 35, 0:\n",
    "            ]  # first 5 seconds of a block\n",
    "            baseline = pd.DataFrame(baseline_rows.mean()).T\n",
    "\n",
    "            for (\n",
    "                block_start_ts,\n",
    "                block_end_ts,\n",
    "            ) in exp_by_block.keys():  # for each block in the experiment\n",
    "                block_start_ts_offset = block_start_ts + exp_time_offset\n",
    "                block_start_idx, _ = self.data_fun.find_closest_ts(\n",
    "                    block_start_ts_offset, ts_list\n",
    "                )\n",
    "                block_end_ts_offset = block_end_ts + exp_time_offset\n",
    "                block_end_idx, _ = self.data_fun.find_closest_ts(\n",
    "                    block_end_ts_offset, ts_list\n",
    "                )\n",
    "                block_rows = flow_exp.loc[\n",
    "                    block_start_idx:block_end_idx, 0:\n",
    "                ]  # rows from block start to end\n",
    "\n",
    "                baseline_df = pd.concat(\n",
    "                    [baseline] * block_rows.shape[0], ignore_index=True\n",
    "                )\n",
    "                baseline_df = baseline_df.set_index(\n",
    "                    pd.Index(range(block_start_idx, block_start_idx + len(baseline_df)))\n",
    "                )\n",
    "\n",
    "                block_rows_norm = block_rows.subtract(\n",
    "                    baseline_df, fill_value=0\n",
    "                )  # normalize the block rows\n",
    "                processed_blocks.append(block_rows_norm)\n",
    "        else:  # normalize each block to the start of the block\n",
    "            for (\n",
    "                block_start_ts,\n",
    "                block_end_ts,\n",
    "            ) in exp_by_block.keys():  # for each block in the experiment\n",
    "                block_start_ts_offset = block_start_ts + exp_time_offset\n",
    "                block_start_idx, _ = self.data_fun.find_closest_ts(\n",
    "                    block_start_ts_offset, ts_list\n",
    "                )\n",
    "                block_end_ts_offset = block_end_ts + exp_time_offset\n",
    "                block_end_idx, _ = self.data_fun.find_closest_ts(\n",
    "                    block_end_ts_offset, ts_list\n",
    "                )\n",
    "                block_rows = flow_exp.loc[\n",
    "                    block_start_idx:block_end_idx, 0:\n",
    "                ]  # rows from block start to end\n",
    "\n",
    "                baseline_rows = flow_exp.loc[\n",
    "                    block_start_idx : block_start_idx + 35, 0:\n",
    "                ]  # first 5 seconds of a block\n",
    "                baseline = pd.DataFrame(baseline_rows.mean()).T\n",
    "                baseline_df = pd.concat(\n",
    "                    [baseline] * block_rows.shape[0], ignore_index=True\n",
    "                )\n",
    "                baseline_df = baseline_df.set_index(\n",
    "                    pd.Index(range(block_start_idx, block_start_idx + len(baseline_df)))\n",
    "                )\n",
    "\n",
    "                block_rows_norm = block_rows.subtract(\n",
    "                    baseline_df, fill_value=0\n",
    "                )  # normalize the block rows\n",
    "                processed_blocks.append(block_rows_norm)\n",
    "\n",
    "        processed_block_df = pd.concat(\n",
    "            processed_blocks\n",
    "        )  # all processed blocks for an experiment\n",
    "\n",
    "        for _, row in exp_results.iterrows():\n",
    "            stim_start_ts = row[\"stim_start\"]\n",
    "            stim_start_ts_offset = stim_start_ts + exp_time_offset\n",
    "            start_idx, _ = self.data_fun.find_closest_ts(stim_start_ts_offset, ts_list)\n",
    "            stim_end_ts = row[\"stim_end\"]\n",
    "            stim_end_ts_offset = stim_end_ts + exp_time_offset\n",
    "            end_idx, _ = self.data_fun.find_closest_ts(stim_end_ts_offset, ts_list)\n",
    "\n",
    "            stim_rows = processed_block_df.loc[start_idx:end_idx, 0:]\n",
    "            avg_stim_rows = stim_rows.mean()  # all channels for a stim\n",
    "\n",
    "            block = row[\"block\"]\n",
    "            trial = row[\"trial\"]\n",
    "\n",
    "            if trial not in exp_stim_resp_dict[block].keys():\n",
    "                exp_stim_resp_dict[block][trial] = []\n",
    "            exp_stim_resp_dict[block][trial].append(\n",
    "                avg_stim_rows\n",
    "            )  # add to a block in dict\n",
    "\n",
    "        return exp_stim_resp_dict\n",
    "\n",
    "    def create_exp_stim_response_df(\n",
    "        self, exp_name: str, filter_type: str = None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create a DataFrame that contains the processed Kernel Flow data in response\n",
    "        to each stimulus in an experiment. Each channel is normalized and averaged.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "            filter_type (str): Filter to apply to the data. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Processed Kernel Flow data.\n",
    "        \"\"\"\n",
    "\n",
    "        def _split_col(row: pd.Series) -> pd.Series:\n",
    "            \"\"\"\n",
    "            Split a column containing an array into separate columns for each\n",
    "            element in the array.\n",
    "\n",
    "            Args:\n",
    "                row (pd.Series): DataFrame row.\n",
    "\n",
    "            Returns:\n",
    "                pd.Series: DataFrame row with split column.\n",
    "            \"\"\"\n",
    "            arr = row[\"channels\"]\n",
    "            num_elements = len(arr)\n",
    "            col_names = [i for i in range(num_elements)]\n",
    "            return pd.Series(arr, index=col_names)\n",
    "\n",
    "        exp_baseline_avg_dict = self.create_exp_stim_response_dict(\n",
    "            exp_name, filter_type\n",
    "        )\n",
    "        rows = []\n",
    "        for block, block_data in sorted(exp_baseline_avg_dict.items()):\n",
    "            for trial, stim_resp_data in block_data.items():\n",
    "                trial_avg = np.mean(stim_resp_data, axis=0)\n",
    "                row = {\n",
    "                    \"participant\": self.par_num,\n",
    "                    \"block\": block,\n",
    "                    \"channels\": trial_avg,\n",
    "                }\n",
    "                rows.append(row)\n",
    "\n",
    "        stim_resp_df = pd.DataFrame(rows)\n",
    "        channel_cols = stim_resp_df.apply(_split_col, axis=1)\n",
    "        stim_resp_df = pd.concat(\n",
    "            [stim_resp_df, channel_cols], axis=1\n",
    "        )  # merge with original DataFrame\n",
    "        stim_resp_df = stim_resp_df.drop(\n",
    "            \"channels\", axis=1\n",
    "        )  # drop the original \"channels\" column\n",
    "        return stim_resp_df\n",
    "\n",
    "    def create_inter_module_exp_results_df(\n",
    "        self, exp_name: str, hemo_type: str = None, filter_type: str = None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create a DataFrame with the inter-module channels for an experiment.\n",
    "        This DataFrame can include both HbO and HbR channels in alternating columns\n",
    "        or just \"HbO\", \"HbR\", \"HbTot\", or \"HbDiff\" channels.\n",
    "\n",
    "        Args:\n",
    "            hemo_type (str, optional): \"HbO\", \"HbR\", \"HbTot\", or \"HbDiff\" channels.\n",
    "                                 Defaults to None (all inter-module channels).\n",
    "            filter_type (str): Filter to apply to the data. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Inter-module channels for an experiment.\n",
    "        \"\"\"\n",
    "\n",
    "        def _compute_df(hemo_type: str) -> pd.DataFrame:\n",
    "            \"\"\"\n",
    "            Create the HbTot and HbDiff DataFrames.\n",
    "\n",
    "            Args:\n",
    "                hemo_type (str): \"HbTot\" or \"HbDiff\".\n",
    "\n",
    "            Returns:\n",
    "                pd.DataFrame: HbTot or HbDiff DataFrame.\n",
    "            \"\"\"\n",
    "            HbO_df = inter_module_df.iloc[\n",
    "                :, np.r_[0, 1, 2 : len(inter_module_df.columns) : 2]\n",
    "            ]\n",
    "            HbO_data_cols = HbO_df.iloc[:, 2:]\n",
    "            HbR_df = inter_module_df.iloc[\n",
    "                :, np.r_[0, 1, 3 : len(inter_module_df.columns) : 2]\n",
    "            ]\n",
    "            HbR_data_cols = HbR_df.iloc[:, 2:]\n",
    "            cols_dict = {}\n",
    "            for i, col_name in enumerate(HbO_data_cols.columns):\n",
    "                if hemo_type.lower() == \"hbtot\":\n",
    "                    cols_dict[col_name] = (\n",
    "                        HbO_data_cols.iloc[:, i] + HbR_data_cols.iloc[:, i]\n",
    "                    )\n",
    "                elif hemo_type.lower() == \"hbdiff\":\n",
    "                    cols_dict[col_name] = (\n",
    "                        HbO_data_cols.iloc[:, i] - HbR_data_cols.iloc[:, i]\n",
    "                    )\n",
    "            df = pd.DataFrame(cols_dict)\n",
    "            df.insert(0, \"block\", HbO_df[\"block\"])\n",
    "            df.insert(0, \"participant\", HbO_df[\"participant\"])\n",
    "            return df\n",
    "\n",
    "        if filter_type:\n",
    "            exp_results = load_results(\n",
    "                os.path.join(self.flow_processed_data_dir, \"all_channels\", filter_type),\n",
    "                exp_name,\n",
    "            )\n",
    "        else:\n",
    "            exp_results = load_results(\n",
    "                os.path.join(\n",
    "                    self.flow_processed_data_dir, \"all_channels\", \"unfiltered\"\n",
    "                ),\n",
    "                exp_name,\n",
    "            )\n",
    "        session = self.par_behav.get_key_from_value(\n",
    "            self.par_behav.session_dict, exp_name\n",
    "        )\n",
    "        measurement_list_df = self.flow_session_dict[session].create_source_detector_df(\n",
    "            \"3D\"\n",
    "        )\n",
    "        channels = (measurement_list_df[\"measurement_list_index\"] - 1).tolist()\n",
    "        cols_to_select = [\"participant\", \"block\"] + [str(chan) for chan in channels]\n",
    "        inter_module_df = exp_results.loc[:, cols_to_select]\n",
    "        if hemo_type:\n",
    "            if hemo_type.lower() == \"hbo\":  # HbO\n",
    "                HbO_df = inter_module_df.iloc[\n",
    "                    :, np.r_[0, 1, 2 : len(inter_module_df.columns) : 2]\n",
    "                ]\n",
    "                return HbO_df\n",
    "            elif hemo_type.lower() == \"hbr\":  # HbR\n",
    "                HbR_df = inter_module_df.iloc[\n",
    "                    :, np.r_[0, 1, 3 : len(inter_module_df.columns) : 2]\n",
    "                ]\n",
    "                return HbR_df\n",
    "            elif hemo_type.lower() == \"hbtot\":  # HbTot\n",
    "                HbTot_df = _compute_df(hemo_type)\n",
    "                return HbTot_df\n",
    "            elif hemo_type.lower() == \"hbdiff\":  # HbDiff\n",
    "                HbDiff_df = _compute_df(hemo_type)\n",
    "                return HbDiff_df\n",
    "        else:\n",
    "            return inter_module_df\n",
    "\n",
    "    def lowpass_filter(\n",
    "        self,\n",
    "        data: Union[np.ndarray, pd.Series, pd.DataFrame],\n",
    "        cutoff: float = 0.1,\n",
    "        fs: float = 7.1,\n",
    "        order: int = 80,\n",
    "        sos: bool = True,\n",
    "    ) -> Union[np.ndarray, pd.Series, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Apply an IIR lowpass Butterworth filter.\n",
    "\n",
    "        Args:\n",
    "            data (Union[np.ndarray, pd.DataFrame]): Data to filter. Array, Series, or DataFrame.\n",
    "            cutoff (float): Cutoff frequency (Hz). Defaults to 0.1.\n",
    "            fs (float): System sampling frequency (Hz). Defaults to 7.1.\n",
    "            order (int): Filter order. Defaults to 80. NOTE: this is the doubled filtfilt order.\n",
    "            sos (bool): Use 'sos' or 'b, a' output. Defaults to True ('sos').\n",
    "\n",
    "        Returns:\n",
    "            Union[np.ndarray, pd.Series, pd.DataFrame]: Filtered data. Array, Series, or DataFrame.\n",
    "        \"\"\"\n",
    "        if sos:\n",
    "            sos = butter(\n",
    "                N=order / 2,\n",
    "                Wn=cutoff,\n",
    "                fs=fs,\n",
    "                btype=\"lowpass\",\n",
    "                output=\"sos\",\n",
    "                analog=False,\n",
    "            )\n",
    "            pad = int(len(data) * 0.8)\n",
    "            if type(data) == pd.DataFrame:\n",
    "                data_out = data.apply(\n",
    "                    lambda x: sosfiltfilt(sos, data, padlen=pad), axis=0\n",
    "                )  # apply lowpass filter\n",
    "            elif type(data) == np.ndarray or type(data) == pd.Series:\n",
    "                data_out = sosfiltfilt(sos, data, padlen=pad)  # apply lowpass filter\n",
    "        else:\n",
    "            b, a = butter(N=order / 2, Wn=cutoff, fs=fs, btype=\"lowpass\", analog=False)\n",
    "            pad = 3 * (max(len(b), len(a)) - 1)\n",
    "            if type(data) == pd.DataFrame:\n",
    "                data_out = data.apply(\n",
    "                    lambda x: filtfilt(b, a, data, padlen=pad), axis=0\n",
    "                )  # apply lowpass filter\n",
    "            elif type(data) == np.ndarray or type(data) == pd.Series:\n",
    "                data_out = filtfilt(b, a, data, padlen=pad)  # apply lowpass filter\n",
    "        return data_out\n",
    "\n",
    "    def bandpass_filter(\n",
    "        self,\n",
    "        data: Union[np.ndarray, pd.Series, pd.DataFrame],\n",
    "        cutoff_low: float = 0.01,\n",
    "        cutoff_high: float = 0.1,\n",
    "        fs: float = 7.1,\n",
    "        order: int = 20,\n",
    "        sos: bool = True,\n",
    "    ) -> Union[np.ndarray, pd.Series, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Apply an IIR bandpass Butterworth filter.\n",
    "\n",
    "        Args:\n",
    "            data (Union[np.ndarray, pd.DataFrame]): Data to filter. Array, Series, or DataFrame.\n",
    "            cutoff_low (float): Low cutoff frequency (Hz). Defaults to 0.01.\n",
    "            cutoff_high (float): High cutoff frequency (Hz). Defaults to 0.1.\n",
    "            fs (float): System sampling frequency (Hz). Defaults to 7.1.\n",
    "            order (int): Filter order. Defaults to 20. NOTE: this is the doubled filtfilt order.\n",
    "            sos (bool): Use 'sos' or 'b, a' output. Defaults to True ('sos').\n",
    "\n",
    "        Returns:\n",
    "            Union[np.ndarray, pd.Series, pd.DataFrame]: Filtered data. Array, Series, or DataFrame.\n",
    "        \"\"\"\n",
    "        if sos:\n",
    "            sos = butter(\n",
    "                N=order,\n",
    "                Wn=[cutoff_low, cutoff_high],\n",
    "                fs=fs,\n",
    "                btype=\"bandpass\",\n",
    "                output=\"sos\",\n",
    "                analog=False,\n",
    "            )\n",
    "            pad = int(len(data) * 0.8)\n",
    "            if type(data) == pd.DataFrame:\n",
    "                data_out = data.apply(\n",
    "                    lambda x: sosfiltfilt(sos, data, padlen=pad), axis=0\n",
    "                )  # apply bandpass filter\n",
    "            elif type(data) == np.ndarray or type(data) == pd.Series:\n",
    "                data_out = sosfiltfilt(sos, data, padlen=pad)  # apply bandpass filter\n",
    "        else:\n",
    "            b, a = butter(\n",
    "                N=order,\n",
    "                Wn=[cutoff_low, cutoff_high],\n",
    "                fs=fs,\n",
    "                btype=\"bandpass\",\n",
    "                analog=False,\n",
    "            )\n",
    "            pad = 3 * (max(len(b), len(a)) - 1)\n",
    "            if type(data) == pd.DataFrame:\n",
    "                data_out = data.apply(\n",
    "                    lambda x: filtfilt(b, a, data, padlen=pad), axis=0\n",
    "                )  # apply bandpass filter\n",
    "            elif type(data) == np.ndarray or type(data) == pd.Series:\n",
    "                data_out = filtfilt(b, a, data, padlen=pad)  # apply bandpass filter\n",
    "        return data_out\n",
    "\n",
    "    def plot_flow_session(\n",
    "        self, session: str, channels: Union[int, list, tuple], filter_type: str = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Plot Kernel flow session data.\n",
    "\n",
    "        Args:\n",
    "            session (str): Session number.\n",
    "            channels (Union[int, list, tuple]): Kernel Flow channels to plot.\n",
    "            filter_type (str, optional): Filter type to apply. Defaults to None.\n",
    "        \"\"\"\n",
    "        flow_session = self.flow_session_dict[session]\n",
    "        sel_flow_data = flow_session.get_data(\"dataframe\", channels)  # TODO\n",
    "        if filter_type == \"lowpass\":\n",
    "            sel_flow_data = self.lowpass_filter(sel_flow_data)\n",
    "        elif filter_type == \"bandpass\":\n",
    "            flow_data = flow_data.apply(lambda x: self.bandpass_filter(x), axis=0)\n",
    "        session_time_offset = self.time_offset_dict[session]\n",
    "        time_abs_dt = flow_session.get_time_abs(\"datetime\")\n",
    "        time_abs_dt_offset = time_abs_dt - datetime.timedelta(\n",
    "            seconds=session_time_offset\n",
    "        )\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(15, 6))\n",
    "\n",
    "        data_traces = []\n",
    "        data_labels = []\n",
    "        for channel_num in channels:\n",
    "            flow_data = sel_flow_data.iloc[:, channel_num]\n",
    "            data_type_label = self.flow_session_dict[session].get_data_type_label(\n",
    "                channel_num\n",
    "            )\n",
    "            legend_label = f\"Ch {channel_num} ({data_type_label})\"\n",
    "            if data_type_label == \"HbO\":\n",
    "                color = \"red\"\n",
    "            elif data_type_label == \"HbR\":\n",
    "                color = \"blue\"\n",
    "            (data_trace,) = ax.plot(\n",
    "                time_abs_dt_offset, flow_data, color=color, label=legend_label\n",
    "            )\n",
    "            data_traces.append(data_trace)\n",
    "            data_labels.append(legend_label)\n",
    "\n",
    "        exp_spans = []\n",
    "        for exp_name in self.par_behav.session_dict[session]:\n",
    "            exp_start_dt = self.par_behav.get_start_dt(exp_name)\n",
    "            exp_end_dt = self.par_behav.get_end_dt(exp_name)\n",
    "            ax.axvline(exp_start_dt, linestyle=\"dashed\", color=\"k\", alpha=0.75)\n",
    "            ax.axvline(exp_end_dt, linestyle=\"dashed\", color=\"k\", alpha=0.75)\n",
    "            exp_span = ax.axvspan(\n",
    "                exp_start_dt,\n",
    "                exp_end_dt,\n",
    "                color=self.par_behav.exp_color_dict[exp_name],\n",
    "                alpha=0.4,\n",
    "                label=exp_name,\n",
    "            )\n",
    "            exp_spans.append(exp_span)\n",
    "\n",
    "        data_legend = ax.legend(\n",
    "            handles=data_traces,\n",
    "            bbox_to_anchor=(1.0, 1.0),\n",
    "            facecolor=\"white\",\n",
    "            framealpha=1,\n",
    "            title=\"Kernel Flow Data\",\n",
    "        )\n",
    "        handles, labels = plt.gca().get_legend_handles_labels()\n",
    "        uni_labels = dict(zip(labels, handles))\n",
    "        [uni_labels.pop(data_label) for data_label in data_labels]\n",
    "\n",
    "        stim_legend = ax.legend(\n",
    "            uni_labels.values(),\n",
    "            uni_labels.keys(),\n",
    "            bbox_to_anchor=(1.0, 0.75),\n",
    "            facecolor=\"white\",\n",
    "            framealpha=1,\n",
    "            title=\"Experiment\",\n",
    "        )\n",
    "        ax.add_artist(data_legend)\n",
    "        session_split = session.split(\"_\")\n",
    "        exp_title = session_split[0].capitalize() + \" \" + session_split[1]\n",
    "        ax.set_title(exp_title)\n",
    "        datetime_fmt = mdates.DateFormatter(\"%H:%M:%S\")\n",
    "        ax.xaxis.set_major_formatter(datetime_fmt)\n",
    "        ax.set_xlabel(\"Time\", fontsize=16, color=\"k\")\n",
    "\n",
    "    def plot_flow_exp(\n",
    "        self,\n",
    "        exp_name: str,\n",
    "        channels: list,\n",
    "        filter_type: str = None,\n",
    "        filter_order: int = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Plot Kernel Flow experiment data.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "            channels (list): Kernel Flow channels to plot.\n",
    "            filter_type (str, optional): Filter type to apply. Defaults to None.\n",
    "            filter_order (int): Filter order. Defaults to None (default filter order value).\n",
    "        \"\"\"\n",
    "        flow_exp = self.load_flow_exp(exp_name)\n",
    "        session = self.par_behav.get_key_from_value(\n",
    "            self.par_behav.session_dict, exp_name\n",
    "        )\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(15, 6))\n",
    "\n",
    "        data_traces = []\n",
    "        data_labels = []\n",
    "        for channel_num in channels:\n",
    "            timeseries = flow_exp[\"datetime\"]\n",
    "            flow_data = flow_exp.iloc[:, channel_num + 1]\n",
    "            if filter_type.lower() == \"lowpass\":\n",
    "                if filter_order:\n",
    "                    flow_data = self.lowpass_filter(flow_data, order=filter_order)\n",
    "                else:\n",
    "                    flow_data = self.lowpass_filter(flow_data)\n",
    "            elif filter_type.lower() == \"bandpass\":\n",
    "                if filter_order:\n",
    "                    flow_data = self.bandpass_filter(flow_data, order=filter_order)\n",
    "                else:\n",
    "                    flow_data = self.bandpass_filter(flow_data)\n",
    "            data_type_label = self.flow_session_dict[session].get_data_type_label(\n",
    "                channel_num\n",
    "            )\n",
    "            # legend_label = f\"Ch {channel_num} ({data_type_label})\"\n",
    "            legend_label = f\"{data_type_label}\"\n",
    "            if data_type_label == \"HbO\":\n",
    "                color = \"red\"\n",
    "            elif data_type_label == \"HbR\":\n",
    "                color = \"blue\"\n",
    "            (data_trace,) = ax.plot(\n",
    "                timeseries, flow_data, color=color, label=legend_label\n",
    "            )\n",
    "            data_traces.append(data_trace)\n",
    "            data_labels.append(legend_label)\n",
    "\n",
    "        exp_start_dt = self.par_behav.get_start_dt(exp_name, self.adj_ts_markers)\n",
    "        exp_end_dt = self.par_behav.get_end_dt(exp_name, self.adj_ts_markers)\n",
    "        ax.axvline(exp_start_dt, linestyle=\"dashed\", color=\"k\", alpha=0.75)\n",
    "        ax.axvline(exp_end_dt, linestyle=\"dashed\", color=\"k\", alpha=0.75)\n",
    "        results_dir = os.path.join(os.getcwd(), \"processed_data\", \"behavioral\")\n",
    "        exp_results = load_results(results_dir, exp_name, self.par_num)\n",
    "        exp_title = self.par_behav.format_exp_name(exp_name)\n",
    "\n",
    "        stim_spans = []\n",
    "        for _, row in exp_results.iterrows():\n",
    "            try:\n",
    "                uni_stim_dict = self.par_behav.create_unique_stim_dict(\n",
    "                    exp_results, \"stim\"\n",
    "                )\n",
    "                stim = row[\"stim\"]\n",
    "                legend_label = self.par_behav.format_exp_name(row[\"stim\"])\n",
    "            except KeyError:\n",
    "                uni_stim_dict = self.par_behav.create_unique_stim_dict(\n",
    "                    exp_results, \"block\"\n",
    "                )\n",
    "                stim = row[\"block\"]\n",
    "                legend_label = self.par_behav.format_exp_name(row[\"block\"])\n",
    "            color_index = uni_stim_dict[stim]\n",
    "            stim_start = datetime.datetime.fromtimestamp(row[\"stim_start\"])\n",
    "            try:\n",
    "                stim_end = datetime.datetime.fromtimestamp(row[\"stim_end\"])\n",
    "            except ValueError:\n",
    "                if exp_name == \"go_no_go\":\n",
    "                    stim_time = 0.5  # seconds\n",
    "                stim_end = datetime.datetime.fromtimestamp(\n",
    "                    row[\"stim_start\"] + stim_time\n",
    "                )\n",
    "            stim_span = ax.axvspan(\n",
    "                stim_start,\n",
    "                stim_end,\n",
    "                color=self.plot_color_dict[color_index],\n",
    "                alpha=0.4,\n",
    "                label=legend_label,\n",
    "            )\n",
    "            stim_spans.append(stim_span)\n",
    "\n",
    "        data_legend = ax.legend(\n",
    "            handles=data_traces,\n",
    "            bbox_to_anchor=(1.0, 1.0),\n",
    "            facecolor=\"white\",\n",
    "            framealpha=1,\n",
    "            title=\"fNIRS data\",\n",
    "        )\n",
    "        handles, labels = plt.gca().get_legend_handles_labels()\n",
    "        uni_labels = dict(zip(labels, handles))\n",
    "        [uni_labels.pop(data_label) for data_label in data_labels]\n",
    "\n",
    "        stim_legend = ax.legend(\n",
    "            uni_labels.values(),\n",
    "            uni_labels.keys(),\n",
    "            bbox_to_anchor=(1.0, 0.75),\n",
    "            facecolor=\"white\",\n",
    "            framealpha=1,\n",
    "            title=\"Stimulus\",\n",
    "        )\n",
    "        ax.add_artist(data_legend)\n",
    "        ax.set_title(exp_title)\n",
    "        datetime_fmt = mdates.DateFormatter(\"%H:%M:%S\")\n",
    "        ax.xaxis.set_major_formatter(datetime_fmt)\n",
    "        ax.set_xlabel(\"Time\", fontsize=16, color=\"k\")\n",
    "        ax.set_ylabel(\"Concentration (\\u03bcM)\", fontsize=16, color=\"k\")\n",
    "\n",
    "\n",
    "class Flow_Results:\n",
    "    def __init__(self):\n",
    "        self.results_dir = os.path.join(os.getcwd(), \"results\")\n",
    "        self.exp_names = [\n",
    "            \"audio_narrative\",\n",
    "            \"go_no_go\",\n",
    "            \"king_devick\",\n",
    "            \"n_back\",\n",
    "            \"resting_state\",\n",
    "            \"tower_of_london\",\n",
    "            \"vSAT\",\n",
    "            \"video_narrative_cmiyc\",\n",
    "            \"video_narrative_sherlock\",\n",
    "        ]\n",
    "        self.hemo_types = [\"HbO\", \"HbR\", \"HbTot\", \"HbDiff\"]\n",
    "        self.par = Participant_Flow(1)\n",
    "        self.flow_session = self.par.flow_session_dict[\"session_1001\"]\n",
    "\n",
    "    def process_flow_data(\n",
    "        self, num_pars: int, inter_module_only=True, filter_type: str = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Generate a CSV file that contains the Kernel Flow stimulus response data\n",
    "        for all experiments and participants.\n",
    "\n",
    "        Args:\n",
    "            num_pars (int): Number of participants in the study.\n",
    "            inter_module_only (bool): Select only inter-module channels. Defaults to True.\n",
    "            filter_type (str): Filter to apply to the data. Defaults to None.\n",
    "        \"\"\"\n",
    "        if inter_module_only:\n",
    "            print(f\"Processing participants ...\")\n",
    "            for hemo_type in self.hemo_types:\n",
    "                all_exp_results_list = []\n",
    "                exp_results_list = []\n",
    "                for exp_name in self.exp_names:\n",
    "                    stim_resp_df = self.par.create_inter_module_exp_results_df(\n",
    "                        exp_name, hemo_type, filter_type\n",
    "                    )\n",
    "                    exp_results_list.append(stim_resp_df)\n",
    "                all_exp_results_list.append(exp_results_list)\n",
    "\n",
    "                if filter_type:\n",
    "                    filedir = os.path.join(\n",
    "                        self.par.flow_processed_data_dir,\n",
    "                        \"inter_module_channels\",\n",
    "                        filter_type,\n",
    "                        hemo_type,\n",
    "                    )\n",
    "                else:\n",
    "                    filedir = os.path.join(\n",
    "                        self.par.flow_processed_data_dir,\n",
    "                        \"inter_module_channels\",\n",
    "                        \"unfiltered\",\n",
    "                        hemo_type,\n",
    "                    )\n",
    "                if not os.path.exists(filedir):\n",
    "                    os.makedirs(filedir)\n",
    "\n",
    "                print(f\"Creating {hemo_type} CSV files ...\")\n",
    "                all_exp_filepath = os.path.join(\n",
    "                    filedir, f\"all_experiments_flow_{hemo_type}.csv\"\n",
    "                )\n",
    "                if os.path.exists(all_exp_filepath):\n",
    "                    os.remove(all_exp_filepath)\n",
    "                for i, exp_name in enumerate(self.exp_names):\n",
    "                    exp_rows = [\n",
    "                        exp_results_list[i] for exp_results_list in all_exp_results_list\n",
    "                    ]\n",
    "                    exp_df = pd.concat(exp_rows, axis=0, ignore_index=True)\n",
    "                    filepath = os.path.join(filedir, f\"{exp_name}_flow_{hemo_type}.csv\")\n",
    "                    exp_df.to_csv(filepath, index=False)\n",
    "                    all_exp_df = exp_df.copy(deep=True)\n",
    "                    exp_name_col = [exp_name] * len(all_exp_df.index)\n",
    "                    all_exp_df.insert(0, \"experiment\", exp_name_col)\n",
    "                    # TODO: add demographic data\n",
    "                    if i == 0:\n",
    "                        all_exp_df.to_csv(\n",
    "                            all_exp_filepath, mode=\"a\", header=True, index=False\n",
    "                        )\n",
    "                    else:\n",
    "                        all_exp_df.to_csv(\n",
    "                            all_exp_filepath, mode=\"a\", header=False, index=False\n",
    "                        )\n",
    "        else:\n",
    "            all_exp_results_list = []\n",
    "            for par_num in range(1, num_pars + 1):\n",
    "                print(f\"Processing participant {par_num} ...\")\n",
    "                par = Participant_Flow(par_num)\n",
    "                exp_results_list = []\n",
    "                for exp_name in self.exp_names:\n",
    "                    stim_resp_df = par.create_exp_stim_response_df(\n",
    "                        exp_name, filter_type\n",
    "                    )\n",
    "                    exp_results_list.append(stim_resp_df)\n",
    "                all_exp_results_list.append(exp_results_list)\n",
    "\n",
    "            if filter_type:\n",
    "                filedir = os.path.join(\n",
    "                    self.par.flow_processed_data_dir, \"all_channels\", filter_type\n",
    "                )\n",
    "            else:\n",
    "                filedir = os.path.join(\n",
    "                    self.par.flow_processed_data_dir, \"all_channels\", \"unfiltered\"\n",
    "                )\n",
    "            if not os.path.exists(filedir):\n",
    "                os.makedirs(filedir)\n",
    "\n",
    "            print(\"Creating CSV files ...\")\n",
    "            all_exp_filepath = os.path.join(filedir, f\"all_experiments_flow.csv\")\n",
    "            if os.path.exists(all_exp_filepath):\n",
    "                os.remove(all_exp_filepath)\n",
    "            for i, exp_name in enumerate(self.exp_names):\n",
    "                exp_rows = [\n",
    "                    exp_results_list[i] for exp_results_list in all_exp_results_list\n",
    "                ]\n",
    "                exp_df = pd.concat(exp_rows, axis=0, ignore_index=True)\n",
    "                filepath = os.path.join(filedir, f\"{exp_name}_flow.csv\")\n",
    "                exp_df.to_csv(filepath, index=False)\n",
    "                all_exp_df = exp_df.copy(deep=True)\n",
    "                exp_name_col = [exp_name] * len(all_exp_df.index)\n",
    "                all_exp_df.insert(0, \"experiment\", exp_name_col)\n",
    "                if i == 0:\n",
    "                    all_exp_df.to_csv(\n",
    "                        all_exp_filepath, mode=\"a\", header=True, index=False\n",
    "                    )\n",
    "                else:\n",
    "                    all_exp_df.to_csv(\n",
    "                        all_exp_filepath, mode=\"a\", header=False, index=False\n",
    "                    )\n",
    "\n",
    "    def load_processed_flow_data(\n",
    "        self, exp_name: str, hemo_type: str, filter_type: str = None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load processes Kernel Flow data into a DataFrame.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "            hemo_type (str): Hemodynamic type. \"HbO\", \"HbR\", \"HbTot\", or \"HbDiff\".\n",
    "            filter_type (str, optional): Filter to apply to the data. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Processed DataFrame.\n",
    "        \"\"\"\n",
    "        read_filedir = os.path.join(\n",
    "            self.par.flow_processed_data_dir,\n",
    "            \"inter_module_channels\",\n",
    "            filter_type,\n",
    "            hemo_type,\n",
    "        )\n",
    "        read_filename = f\"{exp_name}_flow_{hemo_type}.csv\"\n",
    "        read_filepath = os.path.join(read_filedir, read_filename)\n",
    "        flow_df = pd.read_csv(read_filepath)\n",
    "\n",
    "        if exp_name == \"king_devick\":\n",
    "            flow_df = flow_df.drop(\n",
    "                flow_df[\n",
    "                    (flow_df[\"participant\"] == 15) & (flow_df[\"block\"] == \"card_1\")\n",
    "                ].index\n",
    "            )\n",
    "            flow_df.loc[flow_df[\"participant\"] == 15, \"block\"] = flow_df.loc[\n",
    "                flow_df[\"participant\"] == 15, \"block\"\n",
    "            ].apply(lambda x: x[:-1] + str(int(x[-1]) - 1))\n",
    "        return flow_df\n",
    "\n",
    "    def run_anova_rm(self, filter_type: str = None) -> None:\n",
    "        \"\"\"\n",
    "        Run a repeated measures ANOVA on the processed inter-module channels.\n",
    "\n",
    "        Args:\n",
    "            filter_type (str): Filter to apply to the data. Defaults to None.\n",
    "        \"\"\"\n",
    "        for exp_name in [\n",
    "            \"go_no_go\",\n",
    "            \"king_devick\",\n",
    "            \"n_back\",\n",
    "            \"tower_of_london\",\n",
    "            \"vSAT\",\n",
    "        ]:\n",
    "            for hemo_type in self.hemo_types:\n",
    "                if not filter_type:\n",
    "                    filter_type = \"unfiltered\"\n",
    "                write_filedir = os.path.join(\n",
    "                    self.results_dir,\n",
    "                    \"inter_module_channels\",\n",
    "                    exp_name,\n",
    "                    hemo_type,\n",
    "                )\n",
    "                if not os.path.exists(write_filedir):\n",
    "                    os.makedirs(write_filedir)\n",
    "                write_filename = f\"{exp_name}_flow_stats_{hemo_type}_{filter_type}.csv\"\n",
    "                write_filepath = os.path.join(write_filedir, write_filename)\n",
    "                flow_df = self.load_processed_flow_data(\n",
    "                    exp_name, hemo_type, filter_type\n",
    "                )\n",
    "                channels = list(flow_df.columns[2:])\n",
    "                aov_list = []\n",
    "                for channel in channels:\n",
    "                    aov = pg.rm_anova(\n",
    "                        data=flow_df,\n",
    "                        dv=channel,\n",
    "                        within=\"block\",\n",
    "                        subject=\"participant\",\n",
    "                        effsize=\"np2\",\n",
    "                    )\n",
    "                    aov_final = aov[[\"p-unc\", \"F\", \"ddof1\", \"ddof2\"]].copy()\n",
    "                    aov_final.rename(\n",
    "                        columns={\n",
    "                            \"p-unc\": \"p_value\",\n",
    "                            \"F\": \"F_value\",\n",
    "                            \"ddof1\": \"df1\",\n",
    "                            \"ddof2\": \"df2\",\n",
    "                        },\n",
    "                        inplace=True,\n",
    "                    )\n",
    "                    aov_final.insert(0, \"channel_num\", channel)\n",
    "                    aov_list.append(aov_final)\n",
    "                exp_aov_results = pd.concat(aov_list)\n",
    "                exp_aov_results.to_csv(write_filepath, index=False)\n",
    "\n",
    "    def run_pos_hoc_tests(self, filter_type: str = None, drop: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Run pairwise t-tests for post-hoc ANOVA analysis.\n",
    "\n",
    "        Args:\n",
    "            filter_type (str): Filter to apply to the data. Defaults to None.\n",
    "            drop (bool): Drop columns with extra post-hoc info. Defaults to False.\n",
    "        \"\"\"\n",
    "        for exp_name in [\n",
    "            \"go_no_go\",\n",
    "            \"king_devick\",\n",
    "            \"n_back\",\n",
    "            \"tower_of_london\",\n",
    "            \"vSAT\",\n",
    "        ]:\n",
    "            for hemo_type in self.hemo_types:\n",
    "                if not filter_type:\n",
    "                    filter_type = \"unfiltered\"\n",
    "                write_filedir = os.path.join(\n",
    "                    self.results_dir,\n",
    "                    \"inter_module_channels\",\n",
    "                    exp_name,\n",
    "                    hemo_type,\n",
    "                )\n",
    "                write_filename = f\"{exp_name}_post_hoc_{hemo_type}_{filter_type}.csv\"\n",
    "                write_filepath = os.path.join(write_filedir, write_filename)\n",
    "                sig_df = self.load_flow_stats(\n",
    "                    exp_name, hemo_type, filter_type, sig_only=True\n",
    "                )\n",
    "                sig_channels = list(sig_df[\"channel_num\"].astype(str))\n",
    "                flow_df = self.load_processed_flow_data(\n",
    "                    exp_name, hemo_type, filter_type\n",
    "                )\n",
    "                sig_flow_df = flow_df.loc[:, flow_df.columns.isin(sig_channels)]\n",
    "\n",
    "                pos_hoc_list = []\n",
    "                for channel in sig_flow_df.columns:\n",
    "                    results = pg.pairwise_tests(\n",
    "                        data=flow_df, dv=channel, within=\"block\", subject=\"participant\"\n",
    "                    )\n",
    "                    aov_p_value = float(\n",
    "                        sig_df[sig_df[\"channel_num\"] == int(channel)][\"p_value\"]\n",
    "                    )\n",
    "                    results.insert(0, \"aov_p_value\", aov_p_value)\n",
    "                    results.insert(0, \"channel_num\", channel)\n",
    "                    pos_hoc_list.append(results)\n",
    "                post_hoc_results = pd.concat(pos_hoc_list, ignore_index=True)\n",
    "                post_hoc_results = post_hoc_results.rename(\n",
    "                    columns={\n",
    "                        \"Contrast\": \"within\",\n",
    "                        \"A\": \"condition_A\",\n",
    "                        \"B\": \"condition_B\",\n",
    "                        \"T\": \"t_stat\",\n",
    "                        \"dof\": \"df\",\n",
    "                        \"p-unc\": \"p_value\",\n",
    "                    }\n",
    "                )\n",
    "                if drop:\n",
    "                    post_hoc_results = post_hoc_results.drop(\n",
    "                        columns=[\n",
    "                            \"Paired\",\n",
    "                            \"Parametric\",\n",
    "                            \"alternative\",\n",
    "                            \"BF10\",\n",
    "                            \"hedges\",\n",
    "                        ]\n",
    "                    )\n",
    "                post_hoc_results.to_csv(write_filepath, index=False)\n",
    "\n",
    "    def load_flow_stats(\n",
    "        self,\n",
    "        exp_name: str,\n",
    "        hemo_type: str,\n",
    "        filter_type: str = None,\n",
    "        sig_only: bool = False,\n",
    "        print_sig_results: bool = False,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load Kernel Flow statistical results.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "            hemo_type (str): Hemodynamic type. \"HbO\", \"HbR\", \"HbTot\", or \"HbDiff\".\n",
    "            filter_type (str): Filter to apply to the data. Defaults to None.\n",
    "            sig_only (bool): Return only significant results (p < 0.05). Defaults to False.\n",
    "            print_sig_results (bool): Print significant results. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Statistical results for an experiment and hemodynamic type.\n",
    "        \"\"\"\n",
    "        if not filter_type:\n",
    "            filter_type = \"unfiltered\"\n",
    "        filename = f\"{exp_name}_flow_stats_{hemo_type}_{filter_type}.csv\"\n",
    "        filepath = os.path.join(\n",
    "            self.results_dir,\n",
    "            \"inter_module_channels\",\n",
    "            exp_name,\n",
    "            hemo_type,\n",
    "            filename,\n",
    "        )\n",
    "        flow_stats = pd.read_csv(filepath)\n",
    "        flow_stats_out = flow_stats[[\"channel_num\", \"p_value\", \"F_value\", \"df1\", \"df2\"]]\n",
    "        sig_stats = flow_stats_out[flow_stats_out[\"p_value\"] < 0.05].sort_values(\n",
    "            by=\"p_value\", ascending=True\n",
    "        )\n",
    "        if print_sig_results:\n",
    "            print(sig_stats.to_string(index=False))\n",
    "        if sig_only:\n",
    "            return sig_stats\n",
    "        else:\n",
    "            return flow_stats_out\n",
    "\n",
    "    def load_post_hoc_stats(\n",
    "        self, exp_name: str, hemo_type: str, filter_type: str = None, drop: bool = False\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load Kernel Flow ANOVA post-hoc statistical results.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "            hemo_type (str): Hemodynamic type. \"HbO\", \"HbR\", \"HbTot\", or \"HbDiff\".\n",
    "            filter_type (str): Filter to apply to the data. Defaults to None.\n",
    "            drop (bool): Drop columns with extra post-hoc info. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Post-hoc statistical results for an experiment and hemodynamic type.\n",
    "        \"\"\"\n",
    "        if not filter_type:\n",
    "            filter_type = \"unfiltered\"\n",
    "        filename = f\"{exp_name}_post_hoc_{hemo_type}_{filter_type}.csv\"\n",
    "        filepath = os.path.join(\n",
    "            self.results_dir,\n",
    "            \"inter_module_channels\",\n",
    "            exp_name,\n",
    "            hemo_type,\n",
    "            filename,\n",
    "        )\n",
    "        post_hoc_stats = pd.read_csv(filepath)\n",
    "        if drop:\n",
    "            try:\n",
    "                post_hoc_stats = post_hoc_stats.drop(\n",
    "                    columns=[\"Paired\", \"Parametric\", \"alternative\", \"BF10\", \"hedges\"]\n",
    "                )\n",
    "            except KeyError:\n",
    "                pass\n",
    "        return post_hoc_stats\n",
    "\n",
    "    def create_flow_stats_df(\n",
    "        self, exp_name: str, hemo_type: str, filter_type: str = None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create a DataFrame with significant channels and corresponding brain regions.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "            hemo_type (str): Hemodynamic type. \"HbO\", \"HbR\", \"HbTot\", or \"HbDiff\".\n",
    "            filter_type (str): Filter to apply to the data. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Significant stats DataFrame with brain regions.\n",
    "        \"\"\"\n",
    "        sig_stats = self.load_flow_stats(\n",
    "            exp_name, hemo_type, filter_type, sig_only=True\n",
    "        )\n",
    "        sig_channels = list(sig_stats[\"channel_num\"])\n",
    "        source_detector_df = self.flow_session.create_source_detector_df(\n",
    "            \"3D\", brain_regions=True, channels=sig_channels\n",
    "        )\n",
    "        merged_df = pd.merge(\n",
    "            sig_stats, source_detector_df, on=\"channel_num\", how=\"left\"\n",
    "        )\n",
    "        flow_stats_df = merged_df.loc[\n",
    "            :,\n",
    "            [\n",
    "                \"channel_num\",\n",
    "                \"p_value\",\n",
    "                \"F_value\",\n",
    "                \"AAL_distance\",\n",
    "                \"AAL_region\",\n",
    "                \"BA_distance\",\n",
    "                \"BA_region\",\n",
    "            ],\n",
    "        ]\n",
    "        return flow_stats_df\n",
    "\n",
    "    def plot_stat_results(\n",
    "        self,\n",
    "        exp_name: str,\n",
    "        dim: str,\n",
    "        hemo_type: str,\n",
    "        add_labels: bool = False,\n",
    "        filter_type: str = None,\n",
    "        filepath: str = None,\n",
    "        show: bool = True,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Plot Kernel Flow statistical results.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "            dim (str): Position data dimension \"2D\" or \"3D\".\n",
    "            hemo_type (str): Hemodynamic type. \"HbO\", \"HbR\", \"HbTot\", or \"HbDiff\".\n",
    "            add_labels (bool): Add a channel number label at each detector position. Defaults to False.\n",
    "            filter_type (str): Filter to apply to the data. Defaults to None.\n",
    "            filepath (str): Filepath to save figure. Default to None (no output).\n",
    "            show (bool): Display the figure. Defaults to True.\n",
    "        \"\"\"\n",
    "\n",
    "        def _add_missing_pos(dim: str) -> pd.DataFrame:\n",
    "            \"\"\"\n",
    "            Add missing detector/source positions to the plot DataFrame.\n",
    "\n",
    "            Args:\n",
    "                dim (str): Position data dimension \"2D\" or \"3D\".\n",
    "\n",
    "            Returns:\n",
    "                pd.DataFrame: Plot DataFrame with missing positions added.\n",
    "            \"\"\"\n",
    "            nan_columns = [\n",
    "                \"channel_num\",\n",
    "                \"F_value\",\n",
    "                \"p_value\",\n",
    "                \"measurement_list_index\",\n",
    "                \"data_type\",\n",
    "                \"data_type_index\",\n",
    "                \"detector_index\",\n",
    "                \"source_index\",\n",
    "                \"source_label\",\n",
    "                \"detector_label\",\n",
    "            ]\n",
    "            plot_df_temp = pd.merge(flow_stats, source_detector_df, on=\"channel_num\")\n",
    "            row_list = []\n",
    "            if dim.lower() == \"2d\":\n",
    "                for detector_pos in self.flow_session.missing_detector_pos_2d:\n",
    "                    new_row = pd.Series(\n",
    "                        {\n",
    "                            \"source_x_pos\": self.flow_session.missing_source_pos_2d[0],\n",
    "                            \"source_y_pos\": self.flow_session.missing_source_pos_2d[1],\n",
    "                            \"detector_x_pos\": detector_pos[0],\n",
    "                            \"detector_y_pos\": detector_pos[1],\n",
    "                        }\n",
    "                    )\n",
    "                    row_list.append(new_row)\n",
    "                missing_pos_df = pd.DataFrame(row_list)\n",
    "                plot_df = pd.concat(\n",
    "                    [plot_df_temp, missing_pos_df], axis=0, ignore_index=True\n",
    "                )\n",
    "                plot_df.loc[\n",
    "                    plot_df.shape[0] - len(self.flow_session.missing_detector_pos_2d) :,\n",
    "                    nan_columns,\n",
    "                ] = float(\"NaN\")\n",
    "            elif dim.lower() == \"3d\":\n",
    "                for detector_pos in self.flow_session.missing_detector_pos_3d:\n",
    "                    new_row = pd.Series(\n",
    "                        {\n",
    "                            \"source_x_pos\": self.flow_session.missing_source_pos_3d[0],\n",
    "                            \"source_y_pos\": self.flow_session.missing_source_pos_3d[1],\n",
    "                            \"source_z_pos\": self.flow_session.missing_source_pos_3d[2],\n",
    "                            \"detector_x_pos\": detector_pos[0],\n",
    "                            \"detector_y_pos\": detector_pos[1],\n",
    "                            \"detector_z_pos\": detector_pos[2],\n",
    "                        }\n",
    "                    )\n",
    "                    row_list.append(new_row)\n",
    "                missing_pos_df = pd.DataFrame(row_list)\n",
    "                plot_df = pd.concat(\n",
    "                    [plot_df_temp, missing_pos_df], axis=0, ignore_index=True\n",
    "                )\n",
    "                plot_df.loc[\n",
    "                    plot_df.shape[0] - len(self.flow_session.missing_detector_pos_3d) :,\n",
    "                    nan_columns,\n",
    "                ] = float(\"NaN\")\n",
    "            return plot_df\n",
    "\n",
    "        flow_stats = self.load_flow_stats(exp_name, hemo_type, filter_type)\n",
    "        if dim.lower() == \"2d\":\n",
    "            source_detector_df = self.flow_session.create_source_detector_df(\"2D\")\n",
    "            plot_df = _add_missing_pos(dim)\n",
    "            fig = plt.figure(figsize=(6, 5))\n",
    "            ax = fig.add_subplot(111)\n",
    "            sig_detector_plot_df = plot_df[plot_df[\"p_value\"] <= 0.05]\n",
    "            not_sig_detector_plot_df = plot_df.loc[\n",
    "                (plot_df[\"p_value\"] > 0.05) | (pd.isna(plot_df[\"p_value\"]))\n",
    "            ]\n",
    "            scatter = ax.scatter(\n",
    "                sig_detector_plot_df[\"detector_x_pos\"],\n",
    "                sig_detector_plot_df[\"detector_y_pos\"],\n",
    "                s=70,\n",
    "                c=sig_detector_plot_df[\"p_value\"],\n",
    "                cmap=\"autumn_r\",\n",
    "                edgecolors=\"black\",\n",
    "                alpha=1,\n",
    "                zorder=3,\n",
    "            )\n",
    "            ax.scatter(\n",
    "                not_sig_detector_plot_df[\"detector_x_pos\"],\n",
    "                not_sig_detector_plot_df[\"detector_y_pos\"],\n",
    "                s=20,\n",
    "                c=\"dodgerblue\",\n",
    "                edgecolors=\"black\",\n",
    "                alpha=1,\n",
    "                zorder=2,\n",
    "            )\n",
    "            ax.scatter(\n",
    "                plot_df[\"source_x_pos\"],\n",
    "                plot_df[\"source_y_pos\"],\n",
    "                s=30,\n",
    "                c=\"black\",\n",
    "                zorder=1,\n",
    "            )\n",
    "            if add_labels:\n",
    "                labels = [\n",
    "                    plt.text(\n",
    "                        sig_detector_plot_df[\"detector_x_pos\"].iloc[i],\n",
    "                        sig_detector_plot_df[\"detector_y_pos\"].iloc[i],\n",
    "                        int(sig_detector_plot_df[\"channel_num\"].iloc[i]),\n",
    "                        fontsize=8,\n",
    "                        ha=\"center\",\n",
    "                        va=\"center\",\n",
    "                        bbox=dict(\n",
    "                            boxstyle=\"round,pad=0.15\",\n",
    "                            edgecolor=\"black\",\n",
    "                            facecolor=\"white\",\n",
    "                            alpha=1,\n",
    "                        ),\n",
    "                    )\n",
    "                    for i in range(sig_detector_plot_df.shape[0])\n",
    "                ]\n",
    "                adjust_text(\n",
    "                    labels,\n",
    "                    ax=ax,\n",
    "                    arrowprops=dict(\n",
    "                        arrowstyle=\"-|>\",\n",
    "                        facecolor=\"black\",\n",
    "                        linewidth=2,\n",
    "                        shrinkA=0,\n",
    "                        shrinkB=0,\n",
    "                    ),\n",
    "                    expand_points=(4, 4),\n",
    "                    expand_text=(2, 2),\n",
    "                    force_points=(0.2, 0.2),\n",
    "                )  # TODO: arrows behind labels (zorder)\n",
    "            ax.spines[\"top\"].set_visible(False)\n",
    "            ax.spines[\"right\"].set_visible(False)\n",
    "            ax.spines[\"bottom\"].set_visible(False)\n",
    "            ax.spines[\"left\"].set_visible(False)\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.set_title(\"Anterior\", fontweight=\"bold\", fontsize=14, y=1)\n",
    "            ax.text(\n",
    "                0.5,\n",
    "                -0.06,\n",
    "                \"Posterior\",\n",
    "                fontweight=\"bold\",\n",
    "                fontsize=14,\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "                transform=ax.transAxes,\n",
    "            )\n",
    "            scatter.set_clim([0, 0.05])\n",
    "            colorbar = plt.colorbar(\n",
    "                scatter, ticks=[0, 0.01, 0.02, 0.03, 0.04, 0.05], shrink=0.8\n",
    "            )\n",
    "            font_props = FontProperties(size=12)\n",
    "            colorbar.set_label(\"p-value\", fontproperties=font_props)\n",
    "            try:\n",
    "                title_text = f\"{exp_name_to_title(exp_name)} - {hemo_type} - {filter_type.title()}\"\n",
    "            except AttributeError:\n",
    "                title_text = f\"{exp_name_to_title(exp_name)} - {hemo_type} - Unfiltered\"\n",
    "            ax.text(\n",
    "                0.5,\n",
    "                1.12,\n",
    "                title_text,\n",
    "                fontweight=\"bold\",\n",
    "                fontsize=14,\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "                transform=ax.transAxes,\n",
    "            )\n",
    "            if show:  # TODO\n",
    "                plt.show()\n",
    "            if filepath:\n",
    "                fig.savefig(filepath, dpi=300, bbox_inches=\"tight\")\n",
    "        elif dim.lower() == \"3d\":\n",
    "            source_detector_df = self.flow_session.create_source_detector_df(\"3D\")\n",
    "            plot_df = _add_missing_pos(dim)\n",
    "            fig = plt.figure(figsize=[8, 8])\n",
    "            views = {\n",
    "                \"right\": {\"idx\": 1, \"azim\": 0},\n",
    "                \"left\": {\"idx\": 2, \"azim\": 180},\n",
    "                \"anterior\": {\"idx\": 3, \"azim\": 90},\n",
    "                \"posterior\": {\"idx\": 4, \"azim\": 270},\n",
    "            }\n",
    "            for view_name, view_info in views.items():\n",
    "                ax = fig.add_subplot(\n",
    "                    2, 2, view_info[\"idx\"], projection=\"3d\", computed_zorder=False\n",
    "                )\n",
    "                ax.view_init(elev=0, azim=view_info[\"azim\"])\n",
    "                if view_name == \"right\":\n",
    "                    source_plot_df = plot_df[plot_df[\"source_x_pos\"] >= 0]\n",
    "                    detector_plot_df = plot_df[plot_df[\"detector_x_pos\"] >= 0]\n",
    "                    ax.set_title(\"Right View\", fontweight=\"bold\", fontsize=14, y=0.85)\n",
    "                elif view_name == \"left\":\n",
    "                    source_plot_df = plot_df[plot_df[\"source_x_pos\"] <= 0]\n",
    "                    detector_plot_df = plot_df[plot_df[\"detector_x_pos\"] <= 0]\n",
    "                    ax.set_title(\"Left View\", fontweight=\"bold\", fontsize=14, y=0.85)\n",
    "                elif view_name == \"anterior\":\n",
    "                    source_plot_df = plot_df[plot_df[\"source_y_pos\"] > 0]\n",
    "                    detector_plot_df = plot_df[plot_df[\"detector_y_pos\"] > 0]\n",
    "                    ax.set_title(\n",
    "                        \"Anterior View\", fontweight=\"bold\", fontsize=14, y=0.85\n",
    "                    )\n",
    "                elif view_name == \"posterior\":\n",
    "                    source_plot_df = plot_df[plot_df[\"source_y_pos\"] <= 0]\n",
    "                    detector_plot_df = plot_df[plot_df[\"detector_y_pos\"] <= 0]\n",
    "                    ax.set_title(\n",
    "                        \"Posterior View\", fontweight=\"bold\", fontsize=14, y=0.85\n",
    "                    )\n",
    "                sig_detector_plot_df = detector_plot_df[\n",
    "                    detector_plot_df[\"p_value\"] <= 0.05\n",
    "                ]\n",
    "                not_sig_detector_plot_df = detector_plot_df.loc[\n",
    "                    (detector_plot_df[\"p_value\"] > 0.05)\n",
    "                    | (pd.isna(detector_plot_df[\"p_value\"]))\n",
    "                ]\n",
    "                scatter = ax.scatter(\n",
    "                    sig_detector_plot_df[\"detector_x_pos\"],\n",
    "                    sig_detector_plot_df[\"detector_y_pos\"],\n",
    "                    sig_detector_plot_df[\"detector_z_pos\"],\n",
    "                    s=70,\n",
    "                    c=sig_detector_plot_df[\"p_value\"],\n",
    "                    cmap=\"autumn_r\",\n",
    "                    edgecolors=\"black\",\n",
    "                    alpha=1,\n",
    "                    zorder=3,\n",
    "                )\n",
    "                ax.scatter(\n",
    "                    not_sig_detector_plot_df[\"detector_x_pos\"],\n",
    "                    not_sig_detector_plot_df[\"detector_y_pos\"],\n",
    "                    not_sig_detector_plot_df[\"detector_z_pos\"],\n",
    "                    s=20,\n",
    "                    c=\"dodgerblue\",\n",
    "                    edgecolors=\"black\",\n",
    "                    alpha=1,\n",
    "                    zorder=2,\n",
    "                )\n",
    "                ax.scatter(\n",
    "                    source_plot_df[\"source_x_pos\"],\n",
    "                    source_plot_df[\"source_y_pos\"],\n",
    "                    source_plot_df[\"source_z_pos\"],\n",
    "                    s=30,\n",
    "                    c=\"black\",\n",
    "                    zorder=1,\n",
    "                )\n",
    "                ax.patch.set_alpha(0.0)\n",
    "                ax.xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "                ax.xaxis.line.set_color(\"none\")\n",
    "                ax.yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "                ax.yaxis.line.set_color(\"none\")\n",
    "                ax.zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "                ax.zaxis.line.set_color(\"none\")\n",
    "                ax.grid(False)\n",
    "                ax.set_xticklabels([])\n",
    "                ax.set_yticklabels([])\n",
    "                ax.set_zticklabels([])\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "                ax.set_zticks([])\n",
    "            sm = plt.cm.ScalarMappable(\n",
    "                cmap=\"autumn_r\", norm=plt.Normalize(vmin=0, vmax=0.05)\n",
    "            )\n",
    "            sm.set_array([])\n",
    "            colorbar_ax = fig.add_axes([0.87, 0.32, 0.017, 0.4])\n",
    "            colorbar = fig.colorbar(sm, cax=colorbar_ax)\n",
    "            colorbar.set_label(\"p-value\", fontsize=12)\n",
    "            plt.subplots_adjust(wspace=-0.3, hspace=-0.4)\n",
    "            if show:  # TODO\n",
    "                plt.show()\n",
    "            if filepath:\n",
    "                fig.savefig(filepath, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    def create_stat_results_figs(self, overwrite: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        Create figures (.png images) for each experiment, hemodynamic type, and filter type.\n",
    "        There are individual figures for each filter type and a combined figure that has all filter types.\n",
    "        These figures are saved in the corresponding results directory.\n",
    "\n",
    "        Args:\n",
    "            overwrite (bool): Overwrite existing filter figures. Significant performance increase when False.\n",
    "                              Defaults to True.\n",
    "        \"\"\"\n",
    "\n",
    "        def _combine_figs(filedir: str) -> None:\n",
    "            \"\"\"\n",
    "            Combine three individual filter figures into one figure.\n",
    "\n",
    "            Args:\n",
    "                filedir (str): Directory of an experiment hemodynamic type.\n",
    "            \"\"\"\n",
    "            all_filenames = os.listdir(filedir)\n",
    "            all_fig_filenames = [f for f in all_filenames if not f.endswith(\".csv\")]\n",
    "            order = [\"unfiltered\", \"lowpass\", \"bandpass\"]\n",
    "            fig_filenames = sorted(\n",
    "                [f for f in all_fig_filenames if any(o in f for o in order)],\n",
    "                key=lambda f: next(i for i, o in enumerate(order) if o in f),\n",
    "            )\n",
    "            figs = [\n",
    "                Image.open(os.path.join(filedir, fig_name))\n",
    "                for fig_name in fig_filenames\n",
    "            ]\n",
    "            widths, heights = zip(*(fig.size for fig in figs))\n",
    "            total_width = sum(widths)\n",
    "            max_height = max(heights)\n",
    "            fig_out = Image.new(\"RGB\", (total_width, max_height))\n",
    "            x_offset = 0\n",
    "            for fig in figs:\n",
    "                fig_out.paste(fig, (x_offset, 0))\n",
    "                x_offset += fig.size[0]\n",
    "            filename = fig_filenames[0].rpartition(\"_\")[0] + \"_all.png\"\n",
    "            fig_out.save(os.path.join(filedir, filename))\n",
    "\n",
    "        filter_types = [\"unfiltered\", \"lowpass\", \"bandpass\"]\n",
    "        for exp_name in [\n",
    "            \"go_no_go\",\n",
    "            \"king_devick\",\n",
    "            \"n_back\",\n",
    "            \"tower_of_london\",\n",
    "            \"vSAT\",\n",
    "        ]:\n",
    "            for hemo_type in self.hemo_types:\n",
    "                for filter_type in filter_types:\n",
    "                    filedir = os.path.join(\n",
    "                        self.results_dir, \"inter_module_channels\", exp_name, hemo_type\n",
    "                    )\n",
    "                    filename = f\"{exp_name}_{hemo_type}_{filter_type}.png\"\n",
    "                    filepath = os.path.join(filedir, filename)\n",
    "                    if not os.path.exists(filepath) or overwrite:\n",
    "                        out = self.plot_stat_results(\n",
    "                            exp_name,\n",
    "                            dim=\"2D\",\n",
    "                            hemo_type=hemo_type,\n",
    "                            filter_type=filter_type,\n",
    "                            add_labels=True,\n",
    "                            filepath=filepath,\n",
    "                            show=False,\n",
    "                        )\n",
    "                filedir = os.path.join(\n",
    "                    self.results_dir, \"inter_module_channels\", exp_name, hemo_type\n",
    "                )\n",
    "                _combine_figs(filedir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "FR = Flow_Results()\n",
    "exp_name = \"n_back\"\n",
    "hemo_type = \"HbO\"\n",
    "filter_type = \"bandpass\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2884', '3024', '380', '4290', '3790', '1620', '3306']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sig_df = FR.load_flow_stats(exp_name, hemo_type, filter_type, sig_only=True)\n",
    "sig_channels = list(sig_df[\"channel_num\"].astype(str))\n",
    "sig_channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_num</th>\n",
       "      <th>aov_p_value</th>\n",
       "      <th>within</th>\n",
       "      <th>condition_A</th>\n",
       "      <th>condition_B</th>\n",
       "      <th>t_stat</th>\n",
       "      <th>df</th>\n",
       "      <th>p_value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>380</td>\n",
       "      <td>0.022604</td>\n",
       "      <td>block</td>\n",
       "      <td>0_back</td>\n",
       "      <td>1_back</td>\n",
       "      <td>-1.072502</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.303001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>380</td>\n",
       "      <td>0.022604</td>\n",
       "      <td>block</td>\n",
       "      <td>0_back</td>\n",
       "      <td>2_back</td>\n",
       "      <td>2.138769</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.052012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>380</td>\n",
       "      <td>0.022604</td>\n",
       "      <td>block</td>\n",
       "      <td>1_back</td>\n",
       "      <td>2_back</td>\n",
       "      <td>2.372990</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.033747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1620</td>\n",
       "      <td>0.047345</td>\n",
       "      <td>block</td>\n",
       "      <td>0_back</td>\n",
       "      <td>1_back</td>\n",
       "      <td>-1.317976</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.214299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1620</td>\n",
       "      <td>0.047345</td>\n",
       "      <td>block</td>\n",
       "      <td>0_back</td>\n",
       "      <td>2_back</td>\n",
       "      <td>-2.655634</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.022360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1620</td>\n",
       "      <td>0.047345</td>\n",
       "      <td>block</td>\n",
       "      <td>1_back</td>\n",
       "      <td>2_back</td>\n",
       "      <td>-1.331671</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.209904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2884</td>\n",
       "      <td>0.005369</td>\n",
       "      <td>block</td>\n",
       "      <td>0_back</td>\n",
       "      <td>1_back</td>\n",
       "      <td>-2.808539</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.015792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2884</td>\n",
       "      <td>0.005369</td>\n",
       "      <td>block</td>\n",
       "      <td>0_back</td>\n",
       "      <td>2_back</td>\n",
       "      <td>-1.513240</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.156102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2884</td>\n",
       "      <td>0.005369</td>\n",
       "      <td>block</td>\n",
       "      <td>1_back</td>\n",
       "      <td>2_back</td>\n",
       "      <td>2.559871</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.025007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3024</td>\n",
       "      <td>0.020802</td>\n",
       "      <td>block</td>\n",
       "      <td>0_back</td>\n",
       "      <td>1_back</td>\n",
       "      <td>1.003340</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.335500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3024</td>\n",
       "      <td>0.020802</td>\n",
       "      <td>block</td>\n",
       "      <td>0_back</td>\n",
       "      <td>2_back</td>\n",
       "      <td>2.215870</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.046784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3024</td>\n",
       "      <td>0.020802</td>\n",
       "      <td>block</td>\n",
       "      <td>1_back</td>\n",
       "      <td>2_back</td>\n",
       "      <td>3.035454</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.010361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3306</td>\n",
       "      <td>0.047775</td>\n",
       "      <td>block</td>\n",
       "      <td>0_back</td>\n",
       "      <td>1_back</td>\n",
       "      <td>-0.550409</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.593040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3306</td>\n",
       "      <td>0.047775</td>\n",
       "      <td>block</td>\n",
       "      <td>0_back</td>\n",
       "      <td>2_back</td>\n",
       "      <td>1.826447</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.095020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3306</td>\n",
       "      <td>0.047775</td>\n",
       "      <td>block</td>\n",
       "      <td>1_back</td>\n",
       "      <td>2_back</td>\n",
       "      <td>2.106323</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.058951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3790</td>\n",
       "      <td>0.046069</td>\n",
       "      <td>block</td>\n",
       "      <td>0_back</td>\n",
       "      <td>1_back</td>\n",
       "      <td>-1.083993</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.301555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3790</td>\n",
       "      <td>0.046069</td>\n",
       "      <td>block</td>\n",
       "      <td>0_back</td>\n",
       "      <td>2_back</td>\n",
       "      <td>2.002500</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.070501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3790</td>\n",
       "      <td>0.046069</td>\n",
       "      <td>block</td>\n",
       "      <td>1_back</td>\n",
       "      <td>2_back</td>\n",
       "      <td>2.066663</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.063136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4290</td>\n",
       "      <td>0.037395</td>\n",
       "      <td>block</td>\n",
       "      <td>0_back</td>\n",
       "      <td>1_back</td>\n",
       "      <td>-0.241281</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.813776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4290</td>\n",
       "      <td>0.037395</td>\n",
       "      <td>block</td>\n",
       "      <td>0_back</td>\n",
       "      <td>2_back</td>\n",
       "      <td>2.858308</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.015564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4290</td>\n",
       "      <td>0.037395</td>\n",
       "      <td>block</td>\n",
       "      <td>1_back</td>\n",
       "      <td>2_back</td>\n",
       "      <td>1.993212</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.071632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   channel_num  aov_p_value within condition_A condition_B    t_stat    df  \\\n",
       "0          380     0.022604  block      0_back      1_back -1.072502  13.0   \n",
       "1          380     0.022604  block      0_back      2_back  2.138769  13.0   \n",
       "2          380     0.022604  block      1_back      2_back  2.372990  13.0   \n",
       "3         1620     0.047345  block      0_back      1_back -1.317976  11.0   \n",
       "4         1620     0.047345  block      0_back      2_back -2.655634  11.0   \n",
       "5         1620     0.047345  block      1_back      2_back -1.331671  11.0   \n",
       "6         2884     0.005369  block      0_back      1_back -2.808539  12.0   \n",
       "7         2884     0.005369  block      0_back      2_back -1.513240  12.0   \n",
       "8         2884     0.005369  block      1_back      2_back  2.559871  12.0   \n",
       "9         3024     0.020802  block      0_back      1_back  1.003340  12.0   \n",
       "10        3024     0.020802  block      0_back      2_back  2.215870  12.0   \n",
       "11        3024     0.020802  block      1_back      2_back  3.035454  12.0   \n",
       "12        3306     0.047775  block      0_back      1_back -0.550409  11.0   \n",
       "13        3306     0.047775  block      0_back      2_back  1.826447  11.0   \n",
       "14        3306     0.047775  block      1_back      2_back  2.106323  11.0   \n",
       "15        3790     0.046069  block      0_back      1_back -1.083993  11.0   \n",
       "16        3790     0.046069  block      0_back      2_back  2.002500  11.0   \n",
       "17        3790     0.046069  block      1_back      2_back  2.066663  11.0   \n",
       "18        4290     0.037395  block      0_back      1_back -0.241281  11.0   \n",
       "19        4290     0.037395  block      0_back      2_back  2.858308  11.0   \n",
       "20        4290     0.037395  block      1_back      2_back  1.993212  11.0   \n",
       "\n",
       "     p_value  \n",
       "0   0.303001  \n",
       "1   0.052012  \n",
       "2   0.033747  \n",
       "3   0.214299  \n",
       "4   0.022360  \n",
       "5   0.209904  \n",
       "6   0.015792  \n",
       "7   0.156102  \n",
       "8   0.025007  \n",
       "9   0.335500  \n",
       "10  0.046784  \n",
       "11  0.010361  \n",
       "12  0.593040  \n",
       "13  0.095020  \n",
       "14  0.058951  \n",
       "15  0.301555  \n",
       "16  0.070501  \n",
       "17  0.063136  \n",
       "18  0.813776  \n",
       "19  0.015564  \n",
       "20  0.071632  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "drop = True\n",
    "sig_df = FR.load_flow_stats(exp_name, hemo_type, filter_type, sig_only=True)\n",
    "sig_channels = list(sig_df[\"channel_num\"].astype(str))\n",
    "flow_df = FR.load_processed_flow_data(exp_name, hemo_type, filter_type)\n",
    "sig_flow_df = flow_df.loc[:, flow_df.columns.isin(sig_channels)]\n",
    "\n",
    "pos_hoc_list = []\n",
    "for channel in sig_flow_df.columns:\n",
    "    results = pg.pairwise_tests(data=flow_df, dv=channel, within=\"block\", subject=\"participant\")\n",
    "    aov_p_value = float(sig_df[sig_df[\"channel_num\"] == int(channel)][\"p_value\"])\n",
    "    results.insert(0, \"aov_p_value\", aov_p_value)\n",
    "    results.insert(0, \"channel_num\", channel)\n",
    "    pos_hoc_list.append(results)\n",
    "post_hoc_results = pd.concat(pos_hoc_list, ignore_index=True)\n",
    "post_hoc_results = post_hoc_results.rename(columns={\"Contrast\": \"within\", \"A\": \"condition_A\", \"B\": \"condition_B\", \"T\": \"t_stat\", \"dof\": \"df\", \"p-unc\": \"p_value\"})\n",
    "if drop:\n",
    "    post_hoc_results = post_hoc_results.drop(columns=[\"Paired\", \"Parametric\", \"alternative\", \"BF10\", \"hedges\"])\n",
    "post_hoc_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_types = [\"unfiltered\", \"lowpass\", \"bandpass\"]\n",
    "for filter_type in filter_types:\n",
    "    FR.run_pos_hoc_tests(filter_type=filter_type, drop=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_num</th>\n",
       "      <th>aov_p_value</th>\n",
       "      <th>within</th>\n",
       "      <th>condition_A</th>\n",
       "      <th>condition_B</th>\n",
       "      <th>Paired</th>\n",
       "      <th>Parametric</th>\n",
       "      <th>t_stat</th>\n",
       "      <th>df</th>\n",
       "      <th>alternative</th>\n",
       "      <th>p_value</th>\n",
       "      <th>BF10</th>\n",
       "      <th>hedges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>380</td>\n",
       "      <td>0.022604</td>\n",
       "      <td>block</td>\n",
       "      <td>0_back</td>\n",
       "      <td>1_back</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>-1.072502</td>\n",
       "      <td>13.0</td>\n",
       "      <td>two-sided</td>\n",
       "      <td>0.303001</td>\n",
       "      <td>0.439</td>\n",
       "      <td>-0.192169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>380</td>\n",
       "      <td>0.022604</td>\n",
       "      <td>block</td>\n",
       "      <td>0_back</td>\n",
       "      <td>2_back</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2.138769</td>\n",
       "      <td>13.0</td>\n",
       "      <td>two-sided</td>\n",
       "      <td>0.052012</td>\n",
       "      <td>1.527</td>\n",
       "      <td>0.380054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>380</td>\n",
       "      <td>0.022604</td>\n",
       "      <td>block</td>\n",
       "      <td>1_back</td>\n",
       "      <td>2_back</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2.372990</td>\n",
       "      <td>13.0</td>\n",
       "      <td>two-sided</td>\n",
       "      <td>0.033747</td>\n",
       "      <td>2.137</td>\n",
       "      <td>0.531053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1620</td>\n",
       "      <td>0.047345</td>\n",
       "      <td>block</td>\n",
       "      <td>0_back</td>\n",
       "      <td>1_back</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>-1.317976</td>\n",
       "      <td>11.0</td>\n",
       "      <td>two-sided</td>\n",
       "      <td>0.214299</td>\n",
       "      <td>0.581</td>\n",
       "      <td>-0.421271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1620</td>\n",
       "      <td>0.047345</td>\n",
       "      <td>block</td>\n",
       "      <td>0_back</td>\n",
       "      <td>2_back</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>-2.655634</td>\n",
       "      <td>11.0</td>\n",
       "      <td>two-sided</td>\n",
       "      <td>0.022360</td>\n",
       "      <td>3.103</td>\n",
       "      <td>-0.911028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1620</td>\n",
       "      <td>0.047345</td>\n",
       "      <td>block</td>\n",
       "      <td>1_back</td>\n",
       "      <td>2_back</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>-1.331671</td>\n",
       "      <td>11.0</td>\n",
       "      <td>two-sided</td>\n",
       "      <td>0.209904</td>\n",
       "      <td>0.589</td>\n",
       "      <td>-0.416161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2884</td>\n",
       "      <td>0.005369</td>\n",
       "      <td>block</td>\n",
       "      <td>0_back</td>\n",
       "      <td>1_back</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>-2.808539</td>\n",
       "      <td>12.0</td>\n",
       "      <td>two-sided</td>\n",
       "      <td>0.015792</td>\n",
       "      <td>4.000</td>\n",
       "      <td>-1.170268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2884</td>\n",
       "      <td>0.005369</td>\n",
       "      <td>block</td>\n",
       "      <td>0_back</td>\n",
       "      <td>2_back</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>-1.513240</td>\n",
       "      <td>12.0</td>\n",
       "      <td>two-sided</td>\n",
       "      <td>0.156102</td>\n",
       "      <td>0.699</td>\n",
       "      <td>-0.364197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2884</td>\n",
       "      <td>0.005369</td>\n",
       "      <td>block</td>\n",
       "      <td>1_back</td>\n",
       "      <td>2_back</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2.559871</td>\n",
       "      <td>12.0</td>\n",
       "      <td>two-sided</td>\n",
       "      <td>0.025007</td>\n",
       "      <td>2.771</td>\n",
       "      <td>1.042520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3024</td>\n",
       "      <td>0.020802</td>\n",
       "      <td>block</td>\n",
       "      <td>0_back</td>\n",
       "      <td>1_back</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1.003340</td>\n",
       "      <td>12.0</td>\n",
       "      <td>two-sided</td>\n",
       "      <td>0.335500</td>\n",
       "      <td>0.426</td>\n",
       "      <td>0.335152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3024</td>\n",
       "      <td>0.020802</td>\n",
       "      <td>block</td>\n",
       "      <td>0_back</td>\n",
       "      <td>2_back</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2.215870</td>\n",
       "      <td>12.0</td>\n",
       "      <td>two-sided</td>\n",
       "      <td>0.046784</td>\n",
       "      <td>1.699</td>\n",
       "      <td>1.023666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3024</td>\n",
       "      <td>0.020802</td>\n",
       "      <td>block</td>\n",
       "      <td>1_back</td>\n",
       "      <td>2_back</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>3.035454</td>\n",
       "      <td>12.0</td>\n",
       "      <td>two-sided</td>\n",
       "      <td>0.010361</td>\n",
       "      <td>5.628</td>\n",
       "      <td>0.838442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>3306</td>\n",
       "      <td>0.047775</td>\n",
       "      <td>block</td>\n",
       "      <td>0_back</td>\n",
       "      <td>1_back</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.550409</td>\n",
       "      <td>11.0</td>\n",
       "      <td>two-sided</td>\n",
       "      <td>0.593040</td>\n",
       "      <td>0.327</td>\n",
       "      <td>-0.099383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3306</td>\n",
       "      <td>0.047775</td>\n",
       "      <td>block</td>\n",
       "      <td>0_back</td>\n",
       "      <td>2_back</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1.826447</td>\n",
       "      <td>11.0</td>\n",
       "      <td>two-sided</td>\n",
       "      <td>0.095020</td>\n",
       "      <td>1.026</td>\n",
       "      <td>0.772687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3306</td>\n",
       "      <td>0.047775</td>\n",
       "      <td>block</td>\n",
       "      <td>1_back</td>\n",
       "      <td>2_back</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2.106323</td>\n",
       "      <td>11.0</td>\n",
       "      <td>two-sided</td>\n",
       "      <td>0.058951</td>\n",
       "      <td>1.463</td>\n",
       "      <td>0.771627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>3790</td>\n",
       "      <td>0.046069</td>\n",
       "      <td>block</td>\n",
       "      <td>0_back</td>\n",
       "      <td>1_back</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>-1.083993</td>\n",
       "      <td>11.0</td>\n",
       "      <td>two-sided</td>\n",
       "      <td>0.301555</td>\n",
       "      <td>0.468</td>\n",
       "      <td>-0.330365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3790</td>\n",
       "      <td>0.046069</td>\n",
       "      <td>block</td>\n",
       "      <td>0_back</td>\n",
       "      <td>2_back</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2.002500</td>\n",
       "      <td>11.0</td>\n",
       "      <td>two-sided</td>\n",
       "      <td>0.070501</td>\n",
       "      <td>1.279</td>\n",
       "      <td>0.562536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3790</td>\n",
       "      <td>0.046069</td>\n",
       "      <td>block</td>\n",
       "      <td>1_back</td>\n",
       "      <td>2_back</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2.066663</td>\n",
       "      <td>11.0</td>\n",
       "      <td>two-sided</td>\n",
       "      <td>0.063136</td>\n",
       "      <td>1.389</td>\n",
       "      <td>0.814477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4290</td>\n",
       "      <td>0.037395</td>\n",
       "      <td>block</td>\n",
       "      <td>0_back</td>\n",
       "      <td>1_back</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>-0.241281</td>\n",
       "      <td>11.0</td>\n",
       "      <td>two-sided</td>\n",
       "      <td>0.813776</td>\n",
       "      <td>0.295</td>\n",
       "      <td>-0.074447</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4290</td>\n",
       "      <td>0.037395</td>\n",
       "      <td>block</td>\n",
       "      <td>0_back</td>\n",
       "      <td>2_back</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2.858308</td>\n",
       "      <td>11.0</td>\n",
       "      <td>two-sided</td>\n",
       "      <td>0.015564</td>\n",
       "      <td>4.142</td>\n",
       "      <td>0.618257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>4290</td>\n",
       "      <td>0.037395</td>\n",
       "      <td>block</td>\n",
       "      <td>1_back</td>\n",
       "      <td>2_back</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1.993212</td>\n",
       "      <td>11.0</td>\n",
       "      <td>two-sided</td>\n",
       "      <td>0.071632</td>\n",
       "      <td>1.264</td>\n",
       "      <td>0.748666</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    channel_num  aov_p_value within condition_A condition_B  Paired  \\\n",
       "0           380     0.022604  block      0_back      1_back    True   \n",
       "1           380     0.022604  block      0_back      2_back    True   \n",
       "2           380     0.022604  block      1_back      2_back    True   \n",
       "3          1620     0.047345  block      0_back      1_back    True   \n",
       "4          1620     0.047345  block      0_back      2_back    True   \n",
       "5          1620     0.047345  block      1_back      2_back    True   \n",
       "6          2884     0.005369  block      0_back      1_back    True   \n",
       "7          2884     0.005369  block      0_back      2_back    True   \n",
       "8          2884     0.005369  block      1_back      2_back    True   \n",
       "9          3024     0.020802  block      0_back      1_back    True   \n",
       "10         3024     0.020802  block      0_back      2_back    True   \n",
       "11         3024     0.020802  block      1_back      2_back    True   \n",
       "12         3306     0.047775  block      0_back      1_back    True   \n",
       "13         3306     0.047775  block      0_back      2_back    True   \n",
       "14         3306     0.047775  block      1_back      2_back    True   \n",
       "15         3790     0.046069  block      0_back      1_back    True   \n",
       "16         3790     0.046069  block      0_back      2_back    True   \n",
       "17         3790     0.046069  block      1_back      2_back    True   \n",
       "18         4290     0.037395  block      0_back      1_back    True   \n",
       "19         4290     0.037395  block      0_back      2_back    True   \n",
       "20         4290     0.037395  block      1_back      2_back    True   \n",
       "\n",
       "    Parametric    t_stat    df alternative   p_value   BF10    hedges  \n",
       "0         True -1.072502  13.0   two-sided  0.303001  0.439 -0.192169  \n",
       "1         True  2.138769  13.0   two-sided  0.052012  1.527  0.380054  \n",
       "2         True  2.372990  13.0   two-sided  0.033747  2.137  0.531053  \n",
       "3         True -1.317976  11.0   two-sided  0.214299  0.581 -0.421271  \n",
       "4         True -2.655634  11.0   two-sided  0.022360  3.103 -0.911028  \n",
       "5         True -1.331671  11.0   two-sided  0.209904  0.589 -0.416161  \n",
       "6         True -2.808539  12.0   two-sided  0.015792  4.000 -1.170268  \n",
       "7         True -1.513240  12.0   two-sided  0.156102  0.699 -0.364197  \n",
       "8         True  2.559871  12.0   two-sided  0.025007  2.771  1.042520  \n",
       "9         True  1.003340  12.0   two-sided  0.335500  0.426  0.335152  \n",
       "10        True  2.215870  12.0   two-sided  0.046784  1.699  1.023666  \n",
       "11        True  3.035454  12.0   two-sided  0.010361  5.628  0.838442  \n",
       "12        True -0.550409  11.0   two-sided  0.593040  0.327 -0.099383  \n",
       "13        True  1.826447  11.0   two-sided  0.095020  1.026  0.772687  \n",
       "14        True  2.106323  11.0   two-sided  0.058951  1.463  0.771627  \n",
       "15        True -1.083993  11.0   two-sided  0.301555  0.468 -0.330365  \n",
       "16        True  2.002500  11.0   two-sided  0.070501  1.279  0.562536  \n",
       "17        True  2.066663  11.0   two-sided  0.063136  1.389  0.814477  \n",
       "18        True -0.241281  11.0   two-sided  0.813776  0.295 -0.074447  \n",
       "19        True  2.858308  11.0   two-sided  0.015564  4.142  0.618257  \n",
       "20        True  1.993212  11.0   two-sided  0.071632  1.264  0.748666  "
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FR = Flow_Results()\n",
    "FR.load_post_hoc_stats(exp_name, hemo_type, filter_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9c1e24752b3065052c27c07c0a22748a6118d05dd89b50e77c79e7ce74e5970"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
