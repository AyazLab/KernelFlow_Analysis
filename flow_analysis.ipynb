{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import snirf\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# %matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from scipy.signal import firwin, lfilter\n",
    "from typing import Union\n",
    "from statistics import mean\n",
    "from behav_analysis import Participant_Behav, load_results\n",
    "from data_functions import Data_Functions\n",
    "\n",
    "\n",
    "class Process_Flow:\n",
    "    \"\"\"\n",
    "    This class contains functions for processing Kernel Flow data.\n",
    "    Wrapper around an snirf.Snirf object.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filepath: str) -> None:\n",
    "        \"\"\"\n",
    "        Initialize by loading SNIRF file.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to SNIRF file.\n",
    "        \"\"\"\n",
    "        self.data_fun = Data_Functions()\n",
    "        self.snirf_file = self.load_snirf(filepath)\n",
    "\n",
    "    def load_snirf(self, filepath: str) -> snirf.Snirf:\n",
    "        \"\"\"\n",
    "        Load SNIRF file.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            snirf.Snirf: SNIRF file object.\n",
    "        \"\"\"\n",
    "        return snirf.Snirf(filepath, \"r+\", dynamic_loading=True)\n",
    "\n",
    "    def get_time_origin(\n",
    "        self, fmt: str = \"datetime\", offset=True\n",
    "    ) -> Union[datetime.datetime, float]:\n",
    "        \"\"\"\n",
    "        Get the time origin (start time) from the SNIRF file.\n",
    "\n",
    "        Args:\n",
    "            fmt (str, optional): Format to get the time origin in: \"datetime\" or \"timestamp\". Defaults to \"datetime\".\n",
    "            offset (bool): Offset the datetime by 4 hours. Defaults to True.\n",
    "\n",
    "        Raises:\n",
    "            Exception: Invalid fmt argument.\n",
    "\n",
    "        Returns:\n",
    "            Union[datetime.datetime, float]:\n",
    "                datetime.datetime: Time origin datetime.\n",
    "                -or-\n",
    "                float: Time origin timestamp.\n",
    "        \"\"\"\n",
    "        start_date = self.snirf_file.nirs[0].metaDataTags.MeasurementDate\n",
    "        start_time = self.snirf_file.nirs[0].metaDataTags.MeasurementTime\n",
    "        start_str = start_date + \" \" + start_time\n",
    "        if offset:\n",
    "            time_origin = datetime.datetime.strptime(\n",
    "                start_str, \"%Y-%m-%d %H:%M:%S\"\n",
    "            ) - datetime.timedelta(\n",
    "                hours=4\n",
    "            )  # 4 hour offset\n",
    "        else:\n",
    "            time_origin = datetime.datetime.strptime(start_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "        if fmt.lower() == \"datetime\":\n",
    "            return time_origin\n",
    "        elif fmt.lower() == \"timestamp\":\n",
    "            return datetime.datetime.timestamp(time_origin)\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"Invalid 'fmt' argument. Must be 'datetime' or 'timestamp'.\"\n",
    "            )\n",
    "\n",
    "    def get_subject_ID(self) -> str:\n",
    "        \"\"\"\n",
    "        Get the subject ID from the SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            str: Subject ID.\n",
    "        \"\"\"\n",
    "        return self.snirf_file.nirs[0].metaDataTags.SubjectID\n",
    "\n",
    "    def get_time_rel(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get the relative time array from the SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Relative time array.\n",
    "        \"\"\"\n",
    "        return self.snirf_file.nirs[0].data[0].time\n",
    "\n",
    "    def get_time_abs(self, fmt: str = \"datetime\") -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Convert relative time array into an absolute time array.\n",
    "\n",
    "        Args:\n",
    "            fmt (str, optional): Format to get the time array in: \"datetime\" or \"timestamp\". Defaults to \"datetime\".\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Absolute time array.\n",
    "        \"\"\"\n",
    "        time_rel = self.get_time_rel()\n",
    "        if fmt.lower() == \"datetime\":\n",
    "            time_origin_dt = self.get_time_origin(\"datetime\")\n",
    "            return np.array(\n",
    "                [\n",
    "                    datetime.timedelta(seconds=time_rel[i]) + time_origin_dt\n",
    "                    for i in range(len(time_rel))\n",
    "                ]\n",
    "            )\n",
    "        elif fmt.lower() == \"timestamp\":\n",
    "            time_origin_ts = self.get_time_origin(\"timestamp\")\n",
    "            return time_rel + time_origin_ts\n",
    "\n",
    "    def get_data(\n",
    "        self, fmt: str = \"array\", cols: list[int | list | tuple] = None\n",
    "    ) -> Union[np.ndarray, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Get timeseries data from the SNIRF file.\n",
    "\n",
    "        Args:\n",
    "            fmt (str): Format of data (np.ndarray or pd.DataFrame). Defaults to \"array\".\n",
    "            cols (list[int | list | tuple]): Data cols to select. Single col, list of cols, or slice of cols.\n",
    "                                             Defaults to None (all columns).\n",
    "\n",
    "        Raises:\n",
    "            Exception: Invalid fmt argument.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Timeseries data array.\n",
    "        \"\"\"\n",
    "        if cols or cols == 0:\n",
    "            if isinstance(cols, tuple):\n",
    "                data = (\n",
    "                    self.snirf_file.nirs[0].data[0].dataTimeSeries[:, cols[0] : cols[1]]\n",
    "                )\n",
    "            else:\n",
    "                data = self.snirf_file.nirs[0].data[0].dataTimeSeries[:, cols]\n",
    "        else:\n",
    "            data = self.snirf_file.nirs[0].data[0].dataTimeSeries\n",
    "\n",
    "        if \"array\" in fmt.lower():\n",
    "            return data\n",
    "        elif \"dataframe\" in fmt.lower():\n",
    "            return pd.DataFrame(data)\n",
    "        else:\n",
    "            raise Exception(\"Invalid fmt argument. Must be 'array' or 'dataframe'.\")\n",
    "\n",
    "    def get_source_pos_2d(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get the 2D source position array.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: 2D source position array\n",
    "        \"\"\"\n",
    "        return self.snirf_file.nirs[0].probe.sourcePos2D\n",
    "\n",
    "    def get_source_pos_3d(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get the 3D source position array.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: 3D source position array\n",
    "        \"\"\"\n",
    "        return self.snirf_file.nirs[0].probe.sourcePos3D\n",
    "\n",
    "    def get_detector_pos_2d(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get the 2D detector position array.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: 2D detector position array\n",
    "        \"\"\"\n",
    "        return self.snirf_file.nirs[0].probe.detectorPos2D\n",
    "\n",
    "    def get_detector_pos_3d(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get the 3D detector position array.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: 3D detector position array\n",
    "        \"\"\"\n",
    "        return self.snirf_file.nirs[0].probe.detectorPos3D\n",
    "\n",
    "    def get_marker_df(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get a DataFrame of marker data from the \"stim\" part of the SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Marker \"stim\" data.\n",
    "        \"\"\"\n",
    "        marker_data = self.snirf_file.nirs[0].stim[0].data\n",
    "        marker_data_cols = self.snirf_file.nirs[0].stim[0].dataLabels\n",
    "        return pd.DataFrame(marker_data, columns=marker_data_cols)\n",
    "\n",
    "    def get_unique_data_types(self) -> list:\n",
    "        \"\"\"\n",
    "        Get unique data types from the SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            list: Unique data types.\n",
    "        \"\"\"\n",
    "        data_types = []\n",
    "        for i in range(len(self.snirf_file.nirs[0].data[0].measurementList)):\n",
    "            data_type = self.snirf_file.nirs[0].data[0].measurementList[i].dataType\n",
    "            if data_type not in data_types:\n",
    "                data_types.append(data_type)\n",
    "        return data_types\n",
    "\n",
    "    def get_data_type_label(self, channel_num: int) -> str:\n",
    "        \"\"\"\n",
    "        Get the data type label for a channel(s).\n",
    "\n",
    "        Args:\n",
    "            channel_num (int): Channel number to get the data type label of.\n",
    "\n",
    "        Returns:\n",
    "            str: Data type label of the channel.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            self.snirf_file.nirs[0].data[0].measurementList[channel_num].dataTypeLabel\n",
    "        )\n",
    "\n",
    "    def get_unique_data_type_labels(self) -> list:\n",
    "        \"\"\"\n",
    "        Get unique data type labels from the SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            list: Unique data type labels.\n",
    "        \"\"\"\n",
    "        data_type_labels = []\n",
    "        for i in range(len(self.snirf_file.nirs[0].data[0].measurementList)):\n",
    "            data_type_label = (\n",
    "                self.snirf_file.nirs[0].data[0].measurementList[i].dataTypeLabel\n",
    "            )\n",
    "            if data_type_label not in data_type_labels:\n",
    "                data_type_labels.append(data_type_label)\n",
    "        return data_type_labels\n",
    "\n",
    "    def create_source_dict(self) -> dict:\n",
    "        \"\"\"\n",
    "        Count the occurrences of each source index.\n",
    "\n",
    "        Returns:\n",
    "            dict: Counts for each source index.\n",
    "        \"\"\"\n",
    "        source_dict = {}\n",
    "        for i in range(len(self.snirf_file.nirs[0].data[0].measurementList)):\n",
    "            source = self.snirf_file.nirs[0].data[0].measurementList[i].sourceIndex\n",
    "            source_dict[source] = source_dict.get(source, 0) + 1\n",
    "        source_dict = self.data_fun.sort_dict(source_dict, \"keys\")\n",
    "        return source_dict\n",
    "\n",
    "    def create_detector_dict(self) -> dict:\n",
    "        \"\"\"\n",
    "        Count the occurrences of each detector index.\n",
    "\n",
    "        Returns:\n",
    "            dict: Counts for each detector index.\n",
    "        \"\"\"\n",
    "        detector_dict = {}\n",
    "        for i in range(len(self.snirf_file.nirs[0].data[0].measurementList)):\n",
    "            detector = self.snirf_file.nirs[0].data[0].measurementList[i].detectorIndex\n",
    "            detector_dict[detector] = detector_dict.get(detector, 0) + 1\n",
    "        detector_dict = self.data_fun.sort_dict(detector_dict, \"keys\")\n",
    "        return detector_dict\n",
    "\n",
    "    def plot_pos_2d(self) -> None:\n",
    "        \"\"\"\n",
    "        Plot the detector/source 2D positions.\n",
    "        \"\"\"\n",
    "        detector_pos_2d = self.get_detector_pos_2d()\n",
    "        x_detector = detector_pos_2d[:, 0]\n",
    "        y_detector = detector_pos_2d[:, 1]\n",
    "\n",
    "        source_pos_2d = self.get_source_pos_2d()\n",
    "        x_source = source_pos_2d[:, 0]\n",
    "        y_source = source_pos_2d[:, 1]\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.scatter(x_detector, y_detector)\n",
    "        ax.scatter(x_source, y_source)\n",
    "        ax.set_title(\"Detector/Source 2D Plot\")\n",
    "        ax.set_xlabel(\"X-Position (mm)\")\n",
    "        ax.set_ylabel(\"Y-Position (mm)\")\n",
    "        ax.legend([\"Detector\", \"Source\"])\n",
    "\n",
    "    def plot_pos_3d(self) -> None:\n",
    "        \"\"\"\n",
    "        Plot the detector/source 3D positions.\n",
    "        \"\"\"\n",
    "        detector_pos_3d = self.get_detector_pos_3d()\n",
    "        x_detector = detector_pos_3d[:, 0]\n",
    "        y_detector = detector_pos_3d[:, 1]\n",
    "        z_detector = detector_pos_3d[:, 2]\n",
    "\n",
    "        source_pos_3d = self.get_source_pos_3d()\n",
    "        x_source = source_pos_3d[:, 0]\n",
    "        y_source = source_pos_3d[:, 1]\n",
    "        z_source = source_pos_3d[:, 2]\n",
    "\n",
    "        fig = plt.figure()\n",
    "        ax = fig.add_subplot(111, projection=\"3d\")\n",
    "        ax.scatter(x_detector, y_detector, z_detector)\n",
    "        ax.scatter(x_source, y_source, z_source)\n",
    "        ax.set_title(\"Detector/Source 3D Plot\")\n",
    "        ax.set_xlabel(\"X-Position (mm)\")\n",
    "        ax.set_ylabel(\"Y-Position (mm)\")\n",
    "        ax.set_zlabel(\"Z-Position (mm)\")\n",
    "        ax.legend([\"Detector\", \"Source\"])\n",
    "\n",
    "\n",
    "class Participant_Flow:\n",
    "    \"\"\"\n",
    "    This class contains functions, data structures, and info necessary for\n",
    "    processing Kernel Flow data from the experiments.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, par_num):\n",
    "        self.data_fun = Data_Functions()\n",
    "        self.adj_ts_markers = True\n",
    "        self.par_behav = Participant_Behav(par_num, self.adj_ts_markers)\n",
    "        self.par_num, self.par_ID = self.data_fun.process_par(par_num)\n",
    "        self.flow_raw_data_dir = os.path.join(\n",
    "            self.par_behav.raw_data_dir, self.par_ID, \"kernel_data\"\n",
    "        )\n",
    "        self.flow_processed_data_dir = os.path.join(\n",
    "            os.getcwd(), \"processed_data\", \"flow\"\n",
    "        )\n",
    "        self.flow_session_dict = self.create_flow_session_dict(wrapper=True)\n",
    "        self.time_offset_dict = self.create_time_offset_dict()\n",
    "        self.plot_color_dict = {\n",
    "            0: \"purple\",\n",
    "            1: \"orange\",\n",
    "            2: \"green\",\n",
    "            3: \"yellow\",\n",
    "            4: \"pink\",\n",
    "            5: \"skyblue\",\n",
    "        }\n",
    "\n",
    "    def calc_time_offset(self, exp_name: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the time offset (in seconds) between the behavioral and Kernel Flow data\n",
    "        files. Number of seconds that the Kernel Flow data is ahead of the behavioral data.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "\n",
    "        Returns:\n",
    "            float: Time offset (in seconds).\n",
    "        \"\"\"\n",
    "        exp = self.par_behav.get_exp(exp_name)\n",
    "        exp_start_ts = exp.start_ts\n",
    "        marker_sent_time = float(exp.marker_data[\"start_marker\"][\"sent_time\"])\n",
    "        session = self.par_behav.get_key_from_value(\n",
    "            self.par_behav.session_dict, exp_name\n",
    "        )\n",
    "        marker_df = self.create_abs_marker_df(session)\n",
    "        row = marker_df.loc[marker_df[\"Marker\"].str.startswith(exp_name)].reset_index()\n",
    "        if (\n",
    "            exp_name == \"go_no_go\"\n",
    "        ):  # Go/No-go experiment is missing start timestamp marker\n",
    "            try:\n",
    "                kernel_start_ts = row.loc[0, \"Start timestamp\"]\n",
    "                time_offset = kernel_start_ts - (exp_start_ts + marker_sent_time)\n",
    "            except:\n",
    "                time_offset = \"NaN\"\n",
    "        else:\n",
    "            kernel_start_ts = row.loc[0, \"Start timestamp\"]\n",
    "            time_offset = kernel_start_ts - (exp_start_ts + marker_sent_time)\n",
    "        return float(time_offset)\n",
    "\n",
    "    def create_time_offset_dict(self) -> dict:\n",
    "        \"\"\"\n",
    "        Create a dictionary containing the time offset (in seconds) for each experiment.\n",
    "\n",
    "        Returns:\n",
    "            dict: Time offset dictionary.\n",
    "        \"\"\"\n",
    "        time_offset_dict = {}\n",
    "        for exp_name in self.par_behav.exp_order:\n",
    "            if (\n",
    "                exp_name == \"go_no_go\"\n",
    "            ):  # Go/No-go experiment is missing start timestamp marker\n",
    "                if np.isnan(self.calc_time_offset(exp_name)):\n",
    "                    session = self.par_behav.get_key_from_value(\n",
    "                        self.par_behav.session_dict, exp_name\n",
    "                    )\n",
    "                    session_exp_names = self.par_behav.session_dict[session]\n",
    "                    other_exp_names = [\n",
    "                        temp_exp_name\n",
    "                        for temp_exp_name in session_exp_names\n",
    "                        if temp_exp_name != \"go_no_go\"\n",
    "                    ]\n",
    "                    other_exp_time_offsets = []\n",
    "                    for temp_exp_name in other_exp_names:\n",
    "                        time_offset = self.calc_time_offset(temp_exp_name)\n",
    "                        other_exp_time_offsets.append(time_offset)\n",
    "                    avg_time_offset = np.mean(other_exp_time_offsets)\n",
    "                    time_offset_dict[exp_name] = avg_time_offset\n",
    "            else:\n",
    "                time_offset_dict[exp_name] = self.calc_time_offset(exp_name)\n",
    "        for session, exp_list in self.par_behav.session_dict.items():\n",
    "            session_offset = np.mean(\n",
    "                [time_offset_dict[exp_name] for exp_name in exp_list]\n",
    "            )\n",
    "            time_offset_dict[session] = session_offset\n",
    "        return time_offset_dict\n",
    "\n",
    "    def get_time_offset(self, exp_name: str) -> float:\n",
    "        \"\"\"\n",
    "        Get the time offset for an experiment.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Experiment name.\n",
    "\n",
    "        Returns:\n",
    "            float: Time offset (in seconds).\n",
    "        \"\"\"\n",
    "        return self.time_offset_dict[exp_name]\n",
    "\n",
    "    def offset_time_array(self, exp_name: str, time_array: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Offset a Kernel Flow datetime array for an experiment by the time-offset.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "            time_array (np.ndarray): Datetime array.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Time-offset datetime array.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            time_offset = self.get_time_offset(exp_name)\n",
    "        except KeyError:  # if experiment start time is missing, use avg of other session experiments\n",
    "            time_offset_list = []\n",
    "            for exp_name in self.par_behav.exp_order:\n",
    "                try:\n",
    "                    time_offset = self.get_time_offset(exp_name)\n",
    "                    time_offset_list.append(time_offset)\n",
    "                except KeyError:\n",
    "                    pass\n",
    "            time_offset = mean(time_offset_list)\n",
    "        time_offset_dt = datetime.timedelta(seconds=time_offset)\n",
    "        time_abs_dt_offset = time_array - time_offset_dt\n",
    "        return time_abs_dt_offset\n",
    "\n",
    "    def load_flow_session(\n",
    "        self, session: list[str | int], wrapper: bool = False\n",
    "    ) -> snirf.Snirf:\n",
    "        \"\"\"\n",
    "        Load Kernel Flow data for an experiment session.\n",
    "\n",
    "        Args:\n",
    "            session list[str | int]: Experiment session.\n",
    "            wrapper (bool, optional) Option to return Process_Flow-wrapped SNIRF file.\n",
    "                                     Defaults to false.\n",
    "\n",
    "        Raises:\n",
    "            Exception: Invalid session number argument.\n",
    "\n",
    "        Returns:\n",
    "            snirf.Snirf: SNIRF file object.\n",
    "            -or-\n",
    "            Process_Flow object for each experiment session.\n",
    "        \"\"\"\n",
    "        if isinstance(session, str):\n",
    "            if \"session\" not in session:\n",
    "                session = f\"session_{session}\"\n",
    "        elif isinstance(session, int):\n",
    "            session = f\"session_{session}\"\n",
    "        try:\n",
    "            session_dir = os.path.join(self.flow_raw_data_dir, session)\n",
    "            filename = os.listdir(session_dir)[0]\n",
    "            filepath = os.path.join(session_dir, filename)\n",
    "            if wrapper:\n",
    "                return Process_Flow(filepath)\n",
    "            else:\n",
    "                return Process_Flow(filepath).snirf_file\n",
    "        except:\n",
    "            print(\"Invalid session number.\")\n",
    "            raise\n",
    "\n",
    "    def load_flow_exp(self, exp_name: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load Kernel Flow data for the time frame of a specified experiment.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Kernel Flow data for an experiment.\n",
    "        \"\"\"\n",
    "        session = self.par_behav.get_key_from_value(\n",
    "            self.par_behav.session_dict, exp_name\n",
    "        )\n",
    "        flow_session = self.load_flow_session(session, wrapper=True)\n",
    "\n",
    "        start_dt = self.par_behav.get_start_dt(exp_name, self.adj_ts_markers)\n",
    "        end_dt = self.par_behav.get_end_dt(exp_name, self.adj_ts_markers)\n",
    "        time_abs_dt = flow_session.get_time_abs(\"datetime\")\n",
    "        time_abs_dt_offset = self.offset_time_array(exp_name, time_abs_dt)\n",
    "        start_idx = self.par_behav.get_start_index_dt(time_abs_dt_offset, start_dt)\n",
    "        end_idx = self.par_behav.get_end_index_dt(time_abs_dt_offset, end_dt)\n",
    "\n",
    "        flow_data = flow_session.get_data(\"dataframe\")\n",
    "        flow_data.insert(0, \"datetime\", time_abs_dt_offset)\n",
    "        return flow_data.iloc[start_idx:end_idx, :]\n",
    "\n",
    "    def create_flow_session_dict(self, wrapper: bool = False) -> dict:\n",
    "        \"\"\"\n",
    "        Create a dictionary of Kernel Flow data for all experiment sessions.\n",
    "\n",
    "        wrapper (bool, optional) Option to return Process_Flow-wrapped SNIRF file.\n",
    "                                 Default to false.\n",
    "\n",
    "        Returns:\n",
    "            dict: Kernel Flow data for all experiment sessions.\n",
    "                keys:\n",
    "                    \"session_1001\", \"session_1002\", \"session_1003\"\n",
    "                values:\n",
    "                    SNIRF file object for each experiment session\n",
    "                    -or-\n",
    "                    Process_Flow object for each experiment session\n",
    "        \"\"\"\n",
    "        flow_session_dict = {}\n",
    "        for session in self.par_behav.session_dict.keys():\n",
    "            flow_session_dict[session] = self.load_flow_session(session, wrapper)\n",
    "        return flow_session_dict\n",
    "\n",
    "    def create_abs_marker_df(self, session: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Convert the \"stim\" marker DataFrame into absolute time.\n",
    "\n",
    "        Args:\n",
    "            session (str): Experiment session.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Marker \"stim\" data in absolute time.\n",
    "        \"\"\"\n",
    "        marker_df = self.flow_session_dict[session].get_marker_df()\n",
    "        time_origin_ts = self.flow_session_dict[session].get_time_origin(\"timestamp\")\n",
    "        marker_df[\"Timestamp\"] = marker_df[\"Timestamp\"] + time_origin_ts\n",
    "        marker_df.rename({\"Timestamp\": \"Start timestamp\"}, axis=1, inplace=True)\n",
    "\n",
    "        for idx, row in marker_df.iterrows():\n",
    "            end_ts = row[\"Start timestamp\"] + row[\"Duration\"]\n",
    "            marker_df.at[idx, \"End timestamp\"] = end_ts\n",
    "            exp_num = int(row[\"Experiment\"])\n",
    "            exp_name = self.par_behav.marker_dict[exp_num]\n",
    "            marker_df.at[idx, \"Experiment\"] = exp_name\n",
    "\n",
    "        marker_df.rename({\"Experiment\": \"Marker\"}, axis=1, inplace=True)\n",
    "        marker_df.drop([\"Value\"], axis=1, inplace=True)\n",
    "        marker_df = marker_df[\n",
    "            [\"Marker\", \"Start timestamp\", \"Duration\", \"End timestamp\"]\n",
    "        ]\n",
    "        return marker_df\n",
    "\n",
    "    def create_exp_stim_response_dict(self, exp_name: str) -> dict:\n",
    "        \"\"\"\n",
    "        Create a dictionary that contains the processed Kernel Flow data in response\n",
    "        to a stimulus. It is organized by block (keys) and for each block, the value is\n",
    "        a list of Pandas series.  Each series is normalized, averaged, Kernel Flow data\n",
    "        during a presented stimulus duration for each channel.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "\n",
    "        Returns:\n",
    "            dict:\n",
    "                keys:\n",
    "                    \"block 1\", \"block 2\", ... \"block N\"\n",
    "                values:\n",
    "                    dicts:\n",
    "                        keys:\n",
    "                            \"trial 1\", \"trial 2\", ... \"trial N\"\n",
    "                        values:\n",
    "                            lists of averaged, normalized Kernel Flow data series for each\n",
    "                            channel during the stimulus duration\n",
    "        \"\"\"\n",
    "        exp_results = load_results(\n",
    "            self.par_behav.processed_data_dir, exp_name, self.par_behav.par_num\n",
    "        )\n",
    "        flow_exp = self.load_flow_exp(exp_name)\n",
    "        session = self.par_behav.get_key_from_value(\n",
    "            self.par_behav.session_dict, exp_name\n",
    "        )\n",
    "        ts_list = self.flow_session_dict[session].get_time_abs(\"timestamp\")\n",
    "        exp_time_offset = self.time_offset_dict[exp_name]\n",
    "        blocks = list(exp_results[\"block\"].unique())\n",
    "        exp_stim_resp_dict = {\n",
    "            block: {} for block in blocks\n",
    "        }  # initialize with unique blocks\n",
    "        for _, row in exp_results.iterrows():\n",
    "            stim_start_ts = row[\"stim_start\"]\n",
    "            stim_start_ts_offset = stim_start_ts + exp_time_offset\n",
    "            start_idx, _ = self.data_fun.find_closest_ts(stim_start_ts_offset, ts_list)\n",
    "            stim_end_ts = row[\"stim_end\"]\n",
    "            stim_end_ts_offset = stim_end_ts + exp_time_offset\n",
    "            end_idx, _ = self.data_fun.find_closest_ts(stim_end_ts_offset, ts_list)\n",
    "\n",
    "            baseline_row = flow_exp.loc[start_idx, 0:]\n",
    "            stim_rows = flow_exp.loc[start_idx:end_idx, 0:]\n",
    "            avg_norm_rows = (stim_rows - baseline_row).mean()  # all channels for a stim\n",
    "\n",
    "            block = row[\"block\"]\n",
    "            trial = row[\"trial\"]\n",
    "            if trial not in exp_stim_resp_dict[block].keys():\n",
    "                exp_stim_resp_dict[block][trial] = []\n",
    "            exp_stim_resp_dict[block][trial].append(\n",
    "                avg_norm_rows\n",
    "            )  # add to a block in dict\n",
    "        return exp_stim_resp_dict\n",
    "\n",
    "    def create_exp_stim_response_df(self, exp_name: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create a DataFrame that contains the processed Kernel Flow data in response\n",
    "        to each stimulus in an experiment. Each channel is normalized and averaged.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Processed Kernel Flow data.\n",
    "        \"\"\"\n",
    "\n",
    "        def split_col(row: pd.Series) -> pd.Series:\n",
    "            \"\"\"\n",
    "            Split a column containing an array into separate columns for each\n",
    "            element in the array.\n",
    "\n",
    "            Args:\n",
    "                row (pd.Series): DataFrame row.\n",
    "\n",
    "            Returns:\n",
    "                pd.Series: DataFrame row with split column.\n",
    "            \"\"\"\n",
    "            arr = row[\"Channels\"]\n",
    "            num_elements = len(arr)\n",
    "            col_names = [i for i in range(num_elements)]\n",
    "            return pd.Series(arr, index=col_names)\n",
    "\n",
    "        exp_baseline_avg_dict = self.create_exp_stim_response_dict(exp_name)\n",
    "        rows = []\n",
    "        for block, block_data in sorted(exp_baseline_avg_dict.items()):\n",
    "            for trial, stim_resp_data in block_data.items():\n",
    "                trial_avg = np.mean(stim_resp_data, axis=0)\n",
    "                row = {\n",
    "                    \"Participant\": self.par_num,\n",
    "                    \"Block\": block,\n",
    "                    \"Channels\": trial_avg,\n",
    "                }\n",
    "                rows.append(row)\n",
    "\n",
    "        stim_resp_df = pd.DataFrame(rows)\n",
    "        channel_cols = stim_resp_df.apply(split_col, axis=1)\n",
    "        stim_resp_df = pd.concat(\n",
    "            [stim_resp_df, channel_cols], axis=1\n",
    "        )  # merge with original DataFrame\n",
    "        stim_resp_df = stim_resp_df.drop(\n",
    "            \"Channels\", axis=1\n",
    "        )  # drop the original \"Channels\" column\n",
    "        return stim_resp_df\n",
    "\n",
    "    def lowpass_filter(\n",
    "        self, data: list[np.ndarray | pd.DataFrame]\n",
    "    ) -> Union[np.ndarray, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Lowpass filter input data.\n",
    "\n",
    "        Args:\n",
    "            data list([np.ndarray | pd.DataFrame]): Data to filter.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Lowpass filtered data.\n",
    "            -or-\n",
    "            pd.DataFrame: Lowpass filtered data.\n",
    "        \"\"\"\n",
    "        order = 20  # filter order\n",
    "        fs = 1.0  # sampling frequency (Hz)\n",
    "        cutoff = 0.1  # cut-off frequency (Hz)\n",
    "        nyq = 0.5 * fs  # nyquist\n",
    "        taps = firwin(order + 1, cutoff / nyq)\n",
    "\n",
    "        if type(data) == pd.DataFrame:\n",
    "            data_out = data.apply(\n",
    "                lambda x: lfilter(taps, 1.0, x)\n",
    "            )  # apply lowpass filter\n",
    "        else:\n",
    "            data_out = lfilter(taps, 1.0, data)  # apply lowpass filter\n",
    "        return data_out\n",
    "\n",
    "    def plot_flow_session(\n",
    "        self, session: str, channels: list[int | list | tuple], filter_type: str = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Plot Kernel flow session data.\n",
    "\n",
    "        Args:\n",
    "            session (str): Session number.\n",
    "            channels (list[int | list | tuple]): Kernel Flow channels to plot.\n",
    "            filter_type (str, optional): Filter type to apply. Defaults to None.\n",
    "        \"\"\"\n",
    "        flow_session = self.flow_session_dict[session]\n",
    "        sel_flow_data = flow_session.get_data(\"dataframe\", channels)  # TODO\n",
    "        if filter_type == \"lowpass\":\n",
    "            sel_flow_data = self.lowpass_filter(sel_flow_data)\n",
    "        session_time_offset = self.time_offset_dict[session]\n",
    "        time_abs_dt = flow_session.get_time_abs(\"datetime\")\n",
    "        time_abs_dt_offset = time_abs_dt - datetime.timedelta(\n",
    "            seconds=session_time_offset\n",
    "        )\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(15, 6))\n",
    "\n",
    "        data_traces = []\n",
    "        data_labels = []\n",
    "        for channel_num in channels:\n",
    "            flow_data = sel_flow_data.iloc[:, channel_num]\n",
    "            data_type_label = self.flow_session_dict[session].get_data_type_label(\n",
    "                channel_num\n",
    "            )\n",
    "            legend_label = f\"Ch {channel_num} ({data_type_label})\"\n",
    "            if data_type_label == \"HbO\":\n",
    "                color = \"red\"\n",
    "            elif data_type_label == \"HbR\":\n",
    "                color = \"blue\"\n",
    "            (data_trace,) = ax.plot(\n",
    "                time_abs_dt_offset, flow_data, color=color, label=legend_label\n",
    "            )\n",
    "            data_traces.append(data_trace)\n",
    "            data_labels.append(legend_label)\n",
    "\n",
    "        exp_spans = []\n",
    "        for exp_name in self.par_behav.session_dict[session]:\n",
    "            exp_start_dt = self.par_behav.get_start_dt(exp_name)\n",
    "            exp_end_dt = self.par_behav.get_end_dt(exp_name)\n",
    "            ax.axvline(exp_start_dt, linestyle=\"dashed\", color=\"k\", alpha=0.75)\n",
    "            ax.axvline(exp_end_dt, linestyle=\"dashed\", color=\"k\", alpha=0.75)\n",
    "            exp_span = ax.axvspan(\n",
    "                exp_start_dt,\n",
    "                exp_end_dt,\n",
    "                color=self.par_behav.exp_color_dict[exp_name],\n",
    "                alpha=0.4,\n",
    "                label=exp_name,\n",
    "            )\n",
    "            exp_spans.append(exp_span)\n",
    "\n",
    "        data_legend = ax.legend(\n",
    "            handles=data_traces,\n",
    "            bbox_to_anchor=(1.0, 1.0),\n",
    "            facecolor=\"white\",\n",
    "            framealpha=1,\n",
    "            title=\"Kernel Flow Data\",\n",
    "        )\n",
    "        handles, labels = plt.gca().get_legend_handles_labels()\n",
    "        uni_labels = dict(zip(labels, handles))\n",
    "        [uni_labels.pop(data_label) for data_label in data_labels]\n",
    "\n",
    "        stim_legend = ax.legend(\n",
    "            uni_labels.values(),\n",
    "            uni_labels.keys(),\n",
    "            bbox_to_anchor=(1.0, 0.75),\n",
    "            facecolor=\"white\",\n",
    "            framealpha=1,\n",
    "            title=\"Experiment\",\n",
    "        )\n",
    "        ax.add_artist(data_legend)\n",
    "        session_split = session.split(\"_\")\n",
    "        exp_title = session_split[0].capitalize() + \" \" + session_split[1]\n",
    "        ax.set_title(exp_title)\n",
    "        datetime_fmt = mdates.DateFormatter(\"%H:%M:%S\")\n",
    "        ax.xaxis.set_major_formatter(datetime_fmt)\n",
    "        ax.set_xlabel(\"Time\", fontsize=16, color=\"k\")\n",
    "\n",
    "    def plot_flow_exp(\n",
    "        self, exp_name: str, channels: list, filter_type: str = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Plot Kernel Flow experiment data.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "            channels (list): Kernel Flow channels to plot.\n",
    "            filter_type (str, optional): Filter type to apply. Defaults to None.\n",
    "        \"\"\"\n",
    "        flow_exp = self.load_flow_exp(exp_name)\n",
    "        session = self.par_behav.get_key_from_value(\n",
    "            self.par_behav.session_dict, exp_name\n",
    "        )\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(15, 6))\n",
    "\n",
    "        data_traces = []\n",
    "        data_labels = []\n",
    "        for channel_num in channels:\n",
    "            timeseries = flow_exp[\"datetime\"]\n",
    "            flow_data = flow_exp.iloc[:, channel_num + 1]\n",
    "            if filter_type == \"lowpass\":\n",
    "                flow_data = self.lowpass_filter(flow_data)\n",
    "            data_type_label = self.flow_session_dict[session].get_data_type_label(\n",
    "                channel_num\n",
    "            )\n",
    "            legend_label = f\"Ch {channel_num} ({data_type_label})\"\n",
    "            if data_type_label == \"HbO\":\n",
    "                color = \"red\"\n",
    "            elif data_type_label == \"HbR\":\n",
    "                color = \"blue\"\n",
    "            (data_trace,) = ax.plot(\n",
    "                timeseries, flow_data, color=color, label=legend_label\n",
    "            )\n",
    "            data_traces.append(data_trace)\n",
    "            data_labels.append(legend_label)\n",
    "\n",
    "        exp_start_dt = self.par_behav.get_start_dt(exp_name, self.adj_ts_markers)\n",
    "        exp_end_dt = self.par_behav.get_end_dt(exp_name, self.adj_ts_markers)\n",
    "        ax.axvline(exp_start_dt, linestyle=\"dashed\", color=\"k\", alpha=0.75)\n",
    "        ax.axvline(exp_end_dt, linestyle=\"dashed\", color=\"k\", alpha=0.75)\n",
    "        results_dir = r\"C:\\Users\\zackg\\OneDrive\\Ayaz Lab\\KernelFlow_Analysis\\processed_data\\behavioral\"  # NOTE: temporary\n",
    "        exp_results = load_results(results_dir, exp_name, self.par_num)\n",
    "        exp_title = self.par_behav.format_exp_name(exp_name)\n",
    "\n",
    "        stim_spans = []\n",
    "        for _, row in exp_results.iterrows():\n",
    "            try:\n",
    "                uni_stim_dict = self.par_behav.create_unique_stim_dict(\n",
    "                    exp_results, \"stim\"\n",
    "                )\n",
    "                stim = row[\"stim\"]\n",
    "                legend_label = self.par_behav.format_exp_name(row[\"stim\"])\n",
    "            except KeyError:\n",
    "                uni_stim_dict = self.par_behav.create_unique_stim_dict(\n",
    "                    exp_results, \"block\"\n",
    "                )\n",
    "                stim = row[\"block\"]\n",
    "                legend_label = self.par_behav.format_exp_name(row[\"block\"])\n",
    "            color_index = uni_stim_dict[stim]\n",
    "            stim_start = datetime.datetime.fromtimestamp(row[\"stim_start\"])\n",
    "            try:\n",
    "                stim_end = datetime.datetime.fromtimestamp(row[\"stim_end\"])\n",
    "            except ValueError:\n",
    "                if exp_name == \"go_no_go\":\n",
    "                    stim_time = 0.5  # seconds\n",
    "                stim_end = datetime.datetime.fromtimestamp(\n",
    "                    row[\"stim_start\"] + stim_time\n",
    "                )\n",
    "            stim_span = ax.axvspan(\n",
    "                stim_start,\n",
    "                stim_end,\n",
    "                color=self.plot_color_dict[color_index],\n",
    "                alpha=0.4,\n",
    "                label=legend_label,\n",
    "            )\n",
    "            stim_spans.append(stim_span)\n",
    "\n",
    "        data_legend = ax.legend(\n",
    "            handles=data_traces,\n",
    "            bbox_to_anchor=(1.0, 1.0),\n",
    "            facecolor=\"white\",\n",
    "            framealpha=1,\n",
    "            title=\"Kernel Flow Data\",\n",
    "        )\n",
    "        handles, labels = plt.gca().get_legend_handles_labels()\n",
    "        uni_labels = dict(zip(labels, handles))\n",
    "        [uni_labels.pop(data_label) for data_label in data_labels]\n",
    "\n",
    "        stim_legend = ax.legend(\n",
    "            uni_labels.values(),\n",
    "            uni_labels.keys(),\n",
    "            bbox_to_anchor=(1.0, 0.75),\n",
    "            facecolor=\"white\",\n",
    "            framealpha=1,\n",
    "            title=\"Stimulus\",\n",
    "        )\n",
    "        ax.add_artist(data_legend)\n",
    "        ax.set_title(exp_title)\n",
    "        datetime_fmt = mdates.DateFormatter(\"%H:%M:%S\")\n",
    "        ax.xaxis.set_major_formatter(datetime_fmt)\n",
    "        ax.set_xlabel(\"Time\", fontsize=16, color=\"k\")\n",
    "\n",
    "\n",
    "def create_flow_results_tables(num_pars: int) -> None:\n",
    "    \"\"\"\n",
    "    Generate an Excel file that contains the Kernel Flow stimulus response data\n",
    "    for all experiments and participants.\n",
    "\n",
    "    Args:\n",
    "        num_pars (int): Number of participants in the study.\n",
    "    \"\"\"\n",
    "    exp_order = [\n",
    "        \"audio_narrative\",\n",
    "        \"go_no_go\",\n",
    "        \"king_devick\",\n",
    "        \"n_back\",\n",
    "        \"resting_state\",\n",
    "        \"tower_of_london\",\n",
    "        \"vSAT\",\n",
    "        \"video_narrative_cmiyc\",\n",
    "        \"video_narrative_sherlock\",\n",
    "    ]\n",
    "    all_exp_results_list = []\n",
    "    for par_num in range(1, num_pars + 1):\n",
    "        print(f\"Processing participant {par_num} ...\")\n",
    "        par = Participant_Flow(par_num)\n",
    "        exp_results_list = []\n",
    "        for exp_name in exp_order:\n",
    "            stim_resp_df = par.create_exp_stim_response_df(exp_name)\n",
    "            exp_results_list.append(stim_resp_df)\n",
    "        all_exp_results_list.append(exp_results_list)\n",
    "\n",
    "    filedir = par.flow_processed_data_dir\n",
    "    if not os.path.exists(os.path.dirname(filedir)):\n",
    "        os.mkdir(os.path.dirname(filedir))\n",
    "\n",
    "    for i, exp_name in enumerate(exp_order):\n",
    "        exp_rows = [exp_results_list[i] for exp_results_list in all_exp_results_list]\n",
    "        exp_df = pd.concat(exp_rows, axis=0, ignore_index=True)\n",
    "        filepath = os.path.join(filedir, f\"{exp_name}_flow.csv\")\n",
    "        exp_df.to_csv(filepath, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "participant_01\n"
     ]
    }
   ],
   "source": [
    "# SNIRF file loading\n",
    "par_num = 1\n",
    "par = Participant_Flow(par_num)\n",
    "print(par.par_ID)\n",
    "# exp_name = \"king_devick\"\n",
    "# par.plot_flow_exp(exp_name, [0, 1], \"lowpass\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment order: ['resting_state', 'go_no_go', 'video_narrative_sherlock', 'king_devick', 'vSAT', 'audio_narrative', 'n_back', 'tower_of_london', 'video_narrative_cmiyc']\n",
      "\n",
      "Experiment time origin: 2022-06-15 14:43:17.132986\n",
      "Start marker sent time: 11.6911\n",
      "Start marker sent time (absolute): 2022-06-15 14:43:28.824086\n",
      "Flow time origin: 2022-06-15 18:43:22\n",
      "\n",
      "Kernel marker data (original):\n",
      "     Timestamp    Duration  Value  Experiment\n",
      "0    19.105127  441.473551    1.0        51.0\n",
      "1  1120.861606  514.570869    1.0        81.0\n",
      "\n",
      "Kernel marker data (absolute):\n",
      "                           Marker  Start timestamp    Duration  End timestamp\n",
      "0             resting_state_start     1.655319e+09  441.473551   1.655319e+09\n",
      "1  video_narrative_sherlock_start     1.655320e+09  514.570869   1.655320e+09\n",
      "\n",
      "Time offset: 12.281\n",
      "\n",
      "Time difference:\n",
      "------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'resting_state': 12.281041622161865,\n",
       " 'go_no_go': 12.282827615737915,\n",
       " 'video_narrative_sherlock': 12.284613609313965,\n",
       " 'king_devick': 11.980223417282104,\n",
       " 'vSAT': 11.980556726455688,\n",
       " 'audio_narrative': 11.982890844345093,\n",
       " 'n_back': 12.57655382156372,\n",
       " 'tower_of_london': 12.580034732818604,\n",
       " 'video_narrative_cmiyc': 12.58246636390686}"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time offset issue\n",
    "# Kernel Flow PC - Behavioral Task PC\n",
    "\n",
    "print(f\"Experiment order: {par.par_behav.exp_order}\\n\")\n",
    "\n",
    "exp_name = \"resting_state\"\n",
    "exp = par.par_behav.get_exp(exp_name)\n",
    "exp_time_origin_ts = exp.start_ts\n",
    "exp_time_origin_dt = datetime.datetime.fromtimestamp(exp_time_origin_ts)\n",
    "print(f\"Experiment time origin: {exp_time_origin_dt}\")\n",
    "start_marker_sent_time = float(exp.marker_data[\"start_marker\"][\"sent_time\"])\n",
    "print(f\"Start marker sent time: {start_marker_sent_time}\")\n",
    "start_marker_sent_time_abs = exp_time_origin_dt + datetime.timedelta(seconds=start_marker_sent_time)\n",
    "print(f\"Start marker sent time (absolute): {start_marker_sent_time_abs}\")\n",
    "\n",
    "# The behavioral start marker sent time (absolute) and \n",
    "# kernel start marker receive time (absolute) should be identical.\n",
    "\n",
    "flow_time_origin = par.flow_session_dict[\"session_1001\"].get_time_origin(offset=False)\n",
    "print(f\"Flow time origin: {flow_time_origin}\\n\")\n",
    "session = par.par_behav.get_key_from_value(par.par_behav.session_dict, exp_name)\n",
    "marker_df = par.flow_session_dict[session].get_marker_df()\n",
    "print(f\"Kernel marker data (original):\\n{marker_df}\\n\")\n",
    "marker_df_abs = par.create_abs_marker_df(session)\n",
    "print(f\"Kernel marker data (absolute):\\n{marker_df_abs}\")\n",
    "\n",
    "row = marker_df_abs.loc[marker_df_abs[\"Marker\"].str.startswith(exp_name)].reset_index()\n",
    "kernel_start_marker_ts = row.loc[0, \"Start timestamp\"]\n",
    "time_offset = kernel_start_marker_ts - (exp_time_origin_ts + start_marker_sent_time)\n",
    "print(f\"\\nTime offset: {round(time_offset, 3)}\\n\")\n",
    "\n",
    "par_num = 1\n",
    "par = Participant_Flow(par_num)\n",
    "time_offset_list = []\n",
    "print(\"Time difference:\\n------------\")\n",
    "par.time_offset_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9c1e24752b3065052c27c07c0a22748a6118d05dd89b50e77c79e7ce74e5970"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
