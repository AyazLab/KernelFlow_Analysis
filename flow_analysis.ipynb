{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import snirf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from typing import Union\n",
    "from behav_analysis import Data_Functions, Participant_Behav, load_results\n",
    "\n",
    "\n",
    "def sort_dict(dictionary: dict, sort_by: str) -> dict:\n",
    "    \"\"\"\n",
    "    Sort a dictionary by keys or values.\n",
    "\n",
    "    Args:\n",
    "        dictionary (dict): Dictionary to sort.\n",
    "        sort_by (str): How to sort to dictionary: by \"keys\" or by \"values\".\n",
    "\n",
    "    Raises:\n",
    "        Exception: Invalid sort_by argument.\n",
    "\n",
    "    Returns:\n",
    "        dict: Sorted dictionary.\n",
    "    \"\"\"\n",
    "    if \"key\" in sort_by.lower():\n",
    "        return dict(sorted(dictionary.items(), key=lambda item: item[0]))\n",
    "    elif \"value\" in sort_by.lower():\n",
    "        return dict(sorted(dictionary.items(), key=lambda item: item[1]))\n",
    "    else:\n",
    "        raise Exception(\"Invalid 'sort_by' argument. Must be 'key' or 'value'.\")\n",
    "\n",
    "\n",
    "class Flow_Processing():\n",
    "    \"\"\"\n",
    "    This class contains functions for processing Kernel Flow data.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_snirf(self, filepath: str) -> snirf.Snirf:\n",
    "        \"\"\"\n",
    "        Load SNIRF file.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            snirf.Snirf: SNIRF file object.\n",
    "        \"\"\"\n",
    "        return snirf.Snirf(filepath, \"r+\", dynamic_loading=True)\n",
    "\n",
    "\n",
    "    def get_time_origin(\n",
    "        self, fmt: str = \"datetime\"\n",
    "    ) -> Union[datetime.datetime, float]:\n",
    "        \"\"\"\n",
    "        Get the time origin (start time) from the SNIRF file.\n",
    "\n",
    "        Args:\n",
    "            fmt (str, optional): Format to get the time origin in: \"datetime\" or \"timestamp\". Defaults to \"datetime\".\n",
    "\n",
    "        Raises:\n",
    "            Exception: Invalid fmt argument.\n",
    "\n",
    "        Returns:\n",
    "            Union[datetime.datetime, float]:\n",
    "                datetime.datetime: Time origin datetime.\n",
    "                -or-\n",
    "                float: Time origin timestamp.\n",
    "        \"\"\"\n",
    "        start_date = self.snirf_file.nirs[0].metaDataTags.MeasurementDate\n",
    "        start_time = self.snirf_file.nirs[0].metaDataTags.MeasurementTime\n",
    "        start_str = start_date + \" \" + start_time\n",
    "        time_origin = datetime.datetime.strptime(start_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "        if fmt.lower() == \"datetime\":\n",
    "            return time_origin\n",
    "        elif fmt.lower() == \"timestamp\":\n",
    "            return datetime.datetime.timestamp(time_origin)\n",
    "        else:\n",
    "            raise Exception(\"Invalid 'fmt' argument. Must be 'datetime' or 'timestamp'.\")\n",
    "\n",
    "\n",
    "    def get_subject_ID(self) -> str:\n",
    "        \"\"\"\n",
    "        Get the subject ID from the SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            str: Subject ID.\n",
    "        \"\"\"\n",
    "        return self.snirf_file.nirs[0].metaDataTags.SubjectID\n",
    "\n",
    "\n",
    "    def get_time_rel(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get the relative time array from the SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Relative time array.\n",
    "        \"\"\"\n",
    "        return self.snirf_file.nirs[0].data[0].time\n",
    "\n",
    "\n",
    "    def get_time_abs(self, fmt: str = \"datetime\") -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Convert relative time array into an absolute time array.\n",
    "\n",
    "        Args:\n",
    "            fmt (str, optional): Format to get the time array in: \"datetime\" or \"timestamp\". Defaults to \"datetime\".\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Absolute time array.\n",
    "        \"\"\"\n",
    "        time_rel = self.get_time_rel()\n",
    "        if fmt.lower() == \"datetime\":\n",
    "            time_origin_dt = self.get_time_origin(\"datetime\")\n",
    "            return np.array(\n",
    "                [\n",
    "                    datetime.timedelta(seconds=time_rel[i]) + time_origin_dt\n",
    "                    for i in range(len(time_rel))\n",
    "                ]\n",
    "            )\n",
    "        elif fmt.lower() == \"timestamp\":\n",
    "            time_origin_ts = self.get_time_origin(\"timestamp\")\n",
    "            return time_rel + time_origin_ts\n",
    "\n",
    "\n",
    "    def get_data(self, cols: list[int | list | tuple]) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get timeseries data from the SNIRF file.\n",
    "\n",
    "        Args:\n",
    "            cols (list[int  |  list  |  tuple]): Data cols to select. Single col, list of cols, or slice of cols.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Timeseries data array.\n",
    "        \"\"\"\n",
    "        if isinstance(cols, tuple):\n",
    "            return self.snirf_file.nirs[0].data[0].dataTimeSeries[:, cols[0] : cols[1]]\n",
    "        else:\n",
    "            return self.snirf_file.nirs[0].data[0].dataTimeSeries[:, cols]\n",
    "\n",
    "\n",
    "    def get_unique_data_types(self) -> list:\n",
    "        \"\"\"\n",
    "        Get unique data types from the SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            list: Unique data types.\n",
    "        \"\"\"\n",
    "        data_types = []\n",
    "        for i in range(len(self.snirf_file.nirs[0].data[0].measurementList)):\n",
    "            data_type = self.snirf_file.nirs[0].data[0].measurementList[i].dataType\n",
    "            if data_type not in data_types:\n",
    "                data_types.append(data_type)\n",
    "        return data_types\n",
    "\n",
    "\n",
    "    def get_unique_data_type_labels(self) -> list:\n",
    "        \"\"\"\n",
    "        Get unique data type labels from the SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            list: Unique data type labels.\n",
    "        \"\"\"\n",
    "        data_type_labels = []\n",
    "        for i in range(len(self.snirf_file.nirs[0].data[0].measurementList)):\n",
    "            data_type_label = self.snirf_file.nirs[0].data[0].measurementList[i].dataTypeLabel\n",
    "            if data_type_label not in data_type_labels:\n",
    "                data_type_labels.append(data_type_label)\n",
    "        return data_type_labels\n",
    "\n",
    "\n",
    "    def create_source_dict(self) -> dict:\n",
    "        \"\"\"\n",
    "        Count the occurrences of each source index.\n",
    "\n",
    "        Returns:\n",
    "            dict: Counts for each source index.\n",
    "        \"\"\"\n",
    "        source_dict = {}\n",
    "        for i in range(len(self.snirf_file.nirs[0].data[0].measurementList)):\n",
    "            source = self.snirf_file.nirs[0].data[0].measurementList[i].sourceIndex\n",
    "            source_dict[source] = source_dict.get(source, 0) + 1\n",
    "        source_dict = sort_dict(source_dict, \"keys\")\n",
    "        return source_dict\n",
    "\n",
    "\n",
    "    def create_detector_dict(self) -> dict:\n",
    "        \"\"\"\n",
    "        Count the occurrences of each detector index.\n",
    "\n",
    "        Returns:\n",
    "            dict: Counts for each detector index.\n",
    "        \"\"\"\n",
    "        detector_dict = {}\n",
    "        for i in range(len(self.snirf_file.nirs[0].data[0].measurementList)):\n",
    "            detector = self.snirf_file.nirs[0].data[0].measurementList[i].detectorIndex\n",
    "            detector_dict[detector] = detector_dict.get(detector, 0) + 1\n",
    "        detector_dict = sort_dict(detector_dict, \"keys\")\n",
    "        return detector_dict\n",
    "\n",
    "class Participant_Flow(Flow_Processing):\n",
    "    \"\"\"\n",
    "    This class contains functions, data structures, and info necessary for\n",
    "    processing Kernel Flow data from the experiments.\n",
    "\n",
    "    Args:\n",
    "        Flow_Processing (class): Kernel Flow processing functions.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, par_num):\n",
    "        super().__init__()\n",
    "        self.data_fun = Data_Functions()\n",
    "        self.par = Participant_Behav(par_num)\n",
    "        self.par_num, self.par_ID = self.data_fun.process_par(par_num)\n",
    "        data_dir = r\"C:\\Kernel\\participants\"\n",
    "        self.flow_data_dir = os.path.join(data_dir, self.par_ID, \"flow_data\")\n",
    "        self.session_list = [\"1001\", \"1002\", \"1003\"]\n",
    "\n",
    "    def load_flow_session(self, session_num: int) -> snirf.Snirf:\n",
    "        \"\"\"\n",
    "        Load Kernel Flow data for an experiment session. \n",
    "\n",
    "        Args:\n",
    "            session_num (int): Experiment session number.\n",
    "\n",
    "        Raises:\n",
    "            Exception: Invalid session number argument.\n",
    "\n",
    "        Returns:\n",
    "            snirf.Snirf: SNIRF file object.\n",
    "        \"\"\"\n",
    "        if isinstance(session_num, str):\n",
    "            session_num = int(session_num)\n",
    "        elif isinstance(session_num, int):\n",
    "            pass\n",
    "        else:\n",
    "            raise Exception(\"Invalid session number.\")\n",
    "        session_num_str = f\"session_{session_num}\"\n",
    "        filepath = os.path.join(self.flow_data_dir, session_num_str)\n",
    "        return snirf.Snirf(filepath, \"r+\", dynamic_loading=True)\n",
    "        \n",
    "\n",
    "    def load_flow_exp(self, exp_name: str) -> snirf.Snirf:\n",
    "        # TODO: load kernel flow data from start to end timestamp of an experiment\n",
    "        pass\n",
    "\n",
    "    def create_flow_session_dict(self) -> dict:\n",
    "        \"\"\"\n",
    "        Create a dictionary of Kernel Flow data for all experiment sessions.\n",
    "\n",
    "        Returns:\n",
    "            dict: Kernel Flow data for all experiment sessions.\n",
    "                keys: \n",
    "                    \"session_1001\", \"session_1002\", \"session_1003\"\n",
    "                values:\n",
    "                    SNIRF file object for each experiment session\n",
    "        \"\"\"\n",
    "        flow_session_dict = {}\n",
    "        for session in self.session_list:\n",
    "            flow_session_dict[session] = self.load_flow_session(session)\n",
    "        return flow_session_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "participant_01\n"
     ]
    }
   ],
   "source": [
    "# SNIRF file loading\n",
    "filepath = r\"C:\\Kernel\\participants\\participant_01\\flow_data\\session_1001\\Cog1_S001_2163c20_5.snirf\"\n",
    "flow = Flow_Processing()\n",
    "par = Participant_Flow(par_num=1)\n",
    "print(par.par_ID)\n",
    "# snirf_file = flow.load_snirf(filepath=filepath)\n",
    "# print(snirf_file.nirs[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SNIRF file processing\n",
    "subject_ID = flow.get_subject_ID()\n",
    "time_origin_dt = flow.get_time_origin(\"datetime\")\n",
    "time_origin_ts = flow.get_time_origin(\"timestamp\")\n",
    "source_dict = flow.create_source_dict()\n",
    "detector_dict = flow.create_detector_dict()\n",
    "uni_data_types = flow.get_unique_data_types()\n",
    "uni_data_type_labels = flow.get_unique_data_type_labels()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time and data processing\n",
    "time_rel = flow.get_time_rel()\n",
    "time_abs_dt = flow.get_time_abs(\"datetime\")\n",
    "time_abs_ts = flow.get_time_abs(\"timestamp\")\n",
    "data = flow.get_data(0)\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(15, 6))\n",
    "# ax.plot(time_abs_dt[0:10000], data[0:10000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = load_results(r\"C:\\Users\\zackg\\OneDrive\\Ayaz Lab\\KernelFlow_Analysis\\results\\behavioral\", exp_name=\"king_devick\", par=(3, 6))\n",
    "# x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9c1e24752b3065052c27c07c0a22748a6118d05dd89b50e77c79e7ce74e5970"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
