{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from flow_analysis import Participant_Flow, Flow_Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import snirf\n",
    "import ctypes\n",
    "import datetime\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pingouin as pg\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.axes import Axes\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from adjustText import adjust_text\n",
    "from PIL import Image\n",
    "from statistics import mean\n",
    "from scipy.signal import butter, filtfilt, sosfiltfilt\n",
    "from typing import Union, Tuple, List, Literal\n",
    "from behav_analysis import Participant_Behav\n",
    "from data_functions import Data_Functions, load_results, exp_name_to_title\n",
    "\n",
    "hllDll = ctypes.WinDLL(\n",
    "    r\"C:\\Program Files\\R\\R-4.2.3\\bin\\x64\\R.dll\"\n",
    ")  # path to R DLL file\n",
    "import rpy2.robjects as robjects\n",
    "\n",
    "\n",
    "class Flow_Coordinates:\n",
    "    \"\"\"\n",
    "    Convert Kernel Flow XYZ coordinates into the MRIcron AAL template XYZ and MNI coordinates. All units in mm.\n",
    "    Brain anatomical directions:\n",
    "        x-pos = right\n",
    "        x-neg = left\n",
    "        y-pos = anterior\n",
    "        y-neg = posterior\n",
    "        z-pos = superior\n",
    "        z-neg = inferior\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, source_detector_df: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            source_detector_df (pd.DataFrame): DataFrame with inter-module source and detector XYZ coordinates.\n",
    "        \"\"\"\n",
    "        self.sd_df = source_detector_df\n",
    "\n",
    "        # measurements from: https://doi.org/10.1002/jbio.201900175\n",
    "        self.SCALP_THICKNESS = 3\n",
    "        self.SKULL_THICKNESS = 7\n",
    "        self.CSF_THICKNESS = 2\n",
    "        self.BRAIN_SURFACE_DEPTH = (\n",
    "            self.SCALP_THICKNESS + self.SKULL_THICKNESS + self.CSF_THICKNESS\n",
    "        )\n",
    "\n",
    "        # XYZ and MNI measurements from MRIcron AAL template: https://www.nitrc.org/projects/mricron\n",
    "        self.XYZ_ORIGIN = (91, 126, 72)\n",
    "        self.MNI_ORIGIN = (0, 0, 0)\n",
    "\n",
    "        self.X_MIN = 18\n",
    "        self.Y_MIN = 21\n",
    "        self.Z_MIN = 11\n",
    "\n",
    "        self.X_MAX = 163\n",
    "        self.Y_MAX = 200\n",
    "        self.Z_MAX = 156\n",
    "\n",
    "        self.MNI_X_MIN = -73\n",
    "        self.MNI_Y_MIN = -105\n",
    "        self.MNI_Z_MIN = -61\n",
    "\n",
    "        self.MNI_X_MAX = 72\n",
    "        self.MNI_Y_MAX = 74\n",
    "        self.MNI_Z_MAX = 84\n",
    "\n",
    "        self.X_MIN_ADJ = self.X_MIN - self.BRAIN_SURFACE_DEPTH\n",
    "        self.Y_MIN_ADJ = self.Y_MIN - self.BRAIN_SURFACE_DEPTH\n",
    "        self.Z_MIN_ADJ = self.Z_MIN - self.BRAIN_SURFACE_DEPTH\n",
    "        self.X_MAX_ADJ = self.X_MAX + self.BRAIN_SURFACE_DEPTH\n",
    "        self.Y_MAX_ADJ = self.Y_MAX + self.BRAIN_SURFACE_DEPTH\n",
    "        self.Z_MAX_ADJ = self.Z_MAX + self.BRAIN_SURFACE_DEPTH\n",
    "\n",
    "        self.MNI_X_MIN_ADJ = self.MNI_X_MIN - self.BRAIN_SURFACE_DEPTH\n",
    "        self.MNI_Y_MIN_ADJ = self.MNI_Y_MIN - self.BRAIN_SURFACE_DEPTH\n",
    "        self.MNI_Z_MIN_ADJ = self.MNI_Z_MIN - self.BRAIN_SURFACE_DEPTH\n",
    "        self.MNI_X_MAX_ADJ = self.MNI_X_MAX + self.BRAIN_SURFACE_DEPTH\n",
    "        self.MNI_Y_MAX_ADJ = self.MNI_Y_MAX + self.BRAIN_SURFACE_DEPTH\n",
    "        self.MNI_Z_MAX_ADJ = self.MNI_Z_MAX + self.BRAIN_SURFACE_DEPTH\n",
    "\n",
    "        # Kernel Flow source/detector midpoint min and max for each coordinate dimension\n",
    "        self.FLOW_X_MIN = self._get_flow_pos(\"x\", \"min\")  # -87.26267567763952\n",
    "        self.FLOW_Y_MIN = self._get_flow_pos(\"y\", \"min\")  # -121.1857849385022\n",
    "        self.FLOW_Z_MIN = self._get_flow_pos(\"z\", \"min\")  # -18.382019226737423\n",
    "        self.FLOW_X_MAX = self._get_flow_pos(\"x\", \"max\")  # 87.32753031777712\n",
    "        self.FLOW_Y_MAX = self._get_flow_pos(\"y\", \"max\")  # 86.61967154031478\n",
    "        self.FLOW_Z_MAX = self._get_flow_pos(\"z\", \"max\")  # 104.75086014260755\n",
    "\n",
    "        self._run_basic_checks()\n",
    "\n",
    "    def _get_flow_pos(\n",
    "        self, dim: Literal[\"x\", \"y\", \"z\"], pos_type: Literal[\"min\", \"max\"]\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Get the min or max Kernel Flow XYZ dimension coordinate.\n",
    "\n",
    "        Args:\n",
    "            dim (Literal[\"x\", \"y\", \"z\"]): Coordinate dimension.\n",
    "            pos_type (Literal[\"min\", \"max\"]): Min or max coordinate in that dimension.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: Invalid dimension.\n",
    "            ValueError: Invalid position type.\n",
    "\n",
    "        Returns:\n",
    "            float: Min or max coordinate value.\n",
    "        \"\"\"\n",
    "        if dim.lower() == \"x\":\n",
    "            pos_cols = self.sd_df[\"midpoint_x_pos\"]\n",
    "        elif dim.lower() == \"y\":\n",
    "            pos_cols = self.sd_df[\"midpoint_y_pos\"]\n",
    "        elif dim.lower() == \"z\":\n",
    "            pos_cols = self.sd_df[\"midpoint_z_pos\"]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid dimension. Must be 'x', 'y', or 'z'.\")\n",
    "\n",
    "        if pos_type.lower() == \"max\":\n",
    "            pos = pos_cols.max()\n",
    "        elif pos_type.lower() == \"min\":\n",
    "            pos = pos_cols.min()\n",
    "        else:\n",
    "            raise ValueError(\"Invalid position type. Must be 'min' or 'max'.\")\n",
    "        return pos\n",
    "\n",
    "    def _get_range(\n",
    "        self, dim: Literal[\"x\", \"y\", \"z\"], source: Literal[\"template\", \"flow\"]\n",
    "    ) -> float:\n",
    "        \"\"\"\n",
    "        Get the dimension range for the template AAL XYZ coordinates or Kernel Flow XYZ coordinates.\n",
    "\n",
    "        Args:\n",
    "            dim (Literal[\"x\", \"y\", \"z\"]): Coordinate dimension.\n",
    "            source (Literal[\"template\", \"flow\"]): Coordinate system measurement source.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: Invalid measurement source.\n",
    "            ValueError: Invalid dimension.\n",
    "\n",
    "        Returns:\n",
    "            float: Range of a coordinate system dimension.\n",
    "        \"\"\"\n",
    "        if dim.lower() == \"x\":\n",
    "            if source.lower() == \"template\":\n",
    "                range_out = self.X_MAX_ADJ - self.X_MIN_ADJ\n",
    "            elif source.lower() == \"flow\":\n",
    "                range_out = self.FLOW_X_MAX - self.FLOW_X_MIN\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Invalid measurement source. Must be 'template' or 'flow'.\"\n",
    "                )\n",
    "        elif dim.lower() == \"y\":\n",
    "            if source.lower() == \"template\":\n",
    "                range_out = self.Y_MAX_ADJ - self.Y_MIN_ADJ\n",
    "            elif source.lower() == \"flow\":\n",
    "                range_out = self.FLOW_Y_MAX - self.FLOW_Y_MIN\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Invalid measurement source. Must be 'template' or 'flow'.\"\n",
    "                )\n",
    "        elif dim.lower() == \"z\":\n",
    "            if source.lower() == \"template\":\n",
    "                range_out = self.Z_MAX_ADJ - self.Z_MIN_ADJ\n",
    "            elif source.lower() == \"flow\":\n",
    "                range_out = self.FLOW_Z_MAX - self.FLOW_Z_MIN\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Invalid measurement source. Must be 'template' or 'flow'.\"\n",
    "                )\n",
    "        else:\n",
    "            raise ValueError(\"Invalid dimension. Must be 'x', 'y', or 'z'.\")\n",
    "        return range_out\n",
    "\n",
    "    def _get_scaling_factor(self, dim: Literal[\"x\", \"y\", \"z\"]) -> float:\n",
    "        \"\"\"\n",
    "        Get the scaling factor to equalize a Kernel Flow XYZ coordinate dimension range to an\n",
    "        AAL template XYZ coordinate dimension range.\n",
    "\n",
    "        Args:\n",
    "             dim (Literal[\"x\", \"y\", \"z\"]): Coordinate dimension.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: Invalid dimension.\n",
    "\n",
    "        Returns:\n",
    "            float: Scaling factor.\n",
    "        \"\"\"\n",
    "        if dim.lower() == \"x\":\n",
    "            scaling_factor = self._get_range(\"x\", \"template\") / self._get_range(\n",
    "                \"x\", \"flow\"\n",
    "            )\n",
    "        elif dim.lower() == \"y\":\n",
    "            scaling_factor = self._get_range(\"y\", \"template\") / self._get_range(\n",
    "                \"y\", \"flow\"\n",
    "            )\n",
    "        elif dim.lower() == \"z\":\n",
    "            scaling_factor = self._get_range(\"z\", \"template\") / self._get_range(\n",
    "                \"z\", \"flow\"\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"Invalid dimension. Must be 'x', 'y', or 'z'.\")\n",
    "        return scaling_factor\n",
    "\n",
    "    def _scale_kernel_XYZ(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Scale Kernel Flow XYZ coordinate dimensions by the scaling factor such that the range of each\n",
    "        dimension equals the AAL template XYZ coordinate dimension.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Scaled Kernel Flow XYZ coordinate dimensions.\n",
    "        \"\"\"\n",
    "        sd_df_scaled = pd.DataFrame()\n",
    "        sd_df_scaled[\"midpoint_x_pos\"] = self.sd_df[\n",
    "            \"midpoint_x_pos\"\n",
    "        ] * self._get_scaling_factor(\"x\")\n",
    "        sd_df_scaled[\"midpoint_y_pos\"] = self.sd_df[\n",
    "            \"midpoint_y_pos\"\n",
    "        ] * self._get_scaling_factor(\"y\")\n",
    "        sd_df_scaled[\"midpoint_z_pos\"] = self.sd_df[\n",
    "            \"midpoint_z_pos\"\n",
    "        ] * self._get_scaling_factor(\"z\")\n",
    "        return sd_df_scaled\n",
    "\n",
    "    def _align_kernel_XYZ(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Align Kernel Flow XYZ coordinates with the AAL template XYZ coordinates.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Kernel Flow XYZ coordinate dimensions.\n",
    "        \"\"\"\n",
    "        sd_df_scaled = self._scale_kernel_XYZ()\n",
    "        sd_df_aligned = pd.DataFrame()\n",
    "        x_align = sd_df_scaled[\"midpoint_x_pos\"].max() - self.X_MAX_ADJ\n",
    "        y_align = sd_df_scaled[\"midpoint_y_pos\"].max() - self.Y_MAX_ADJ\n",
    "        z_align = sd_df_scaled[\"midpoint_z_pos\"].max() - self.Z_MAX_ADJ\n",
    "        sd_df_aligned[\"midpoint_x_XYZ\"] = sd_df_scaled[\"midpoint_x_pos\"] - x_align\n",
    "        sd_df_aligned[\"midpoint_y_XYZ\"] = sd_df_scaled[\"midpoint_y_pos\"] - y_align\n",
    "        sd_df_aligned[\"midpoint_z_XYZ\"] = sd_df_scaled[\"midpoint_z_pos\"] - z_align\n",
    "        return sd_df_aligned\n",
    "\n",
    "    def _get_depth(self, depth: Union[int, float] = None) -> Union[int, float]:\n",
    "        \"\"\"\n",
    "        Get the Kernel Flow coordinate translation depth (in mm).\n",
    "\n",
    "        Args:\n",
    "            depth (Union[int, float], optional): Depth into the brain. Defaults to None (brain surface).\n",
    "\n",
    "        Returns:\n",
    "            depth (Union[int, float], optional): Depth into the brain.\n",
    "        \"\"\"\n",
    "        if depth is None:\n",
    "            depth_out = self.BRAIN_SURFACE_DEPTH\n",
    "        else:\n",
    "            depth_out = self.BRAIN_SURFACE_DEPTH + depth\n",
    "        return depth_out\n",
    "\n",
    "    def _translate_kernel_XYZ(self, depth: Union[int, float] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Translate Kernel Flow XYZ coordinate dimensions to the brain surface or to a specified depth.\n",
    "\n",
    "        Args:\n",
    "            depth (Union[int, float], optional): Depth into the brain. Defaults to None (brain surface).\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Translated Kernel Flow DataFrame.\n",
    "        \"\"\"\n",
    "        depth = self._get_depth(depth)\n",
    "        sd_df_aligned = self._align_kernel_XYZ()\n",
    "        sd_df_translated = pd.DataFrame()\n",
    "        if depth:\n",
    "            for col in sd_df_aligned.columns:\n",
    "                min_val = sd_df_aligned[col].min()\n",
    "                max_val = sd_df_aligned[col].max()\n",
    "                new_min = min_val + depth\n",
    "                new_max = max_val - depth\n",
    "                sd_df_translated[col] = (sd_df_aligned[col] - min_val) * (\n",
    "                    new_max - new_min\n",
    "                ) / (max_val - min_val) + new_min\n",
    "            self._run_translation_check(sd_df_translated, depth)\n",
    "        else:\n",
    "            sd_df_translated = sd_df_aligned\n",
    "        return sd_df_translated\n",
    "\n",
    "    def get_kernel_XYZ(self, depth: Union[int, float] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get a DataFrame with Kernel Flow XYZ coordinates scaled and aligned with the AAL template XYZ coordinates.\n",
    "\n",
    "        Args:\n",
    "            depth (Union[int, float], optional): Depth into the brain. Defaults to None (brain surface).\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Kernel Flow XYZ coordinate dimensions.\n",
    "        \"\"\"\n",
    "        kernel_XYZ = self._translate_kernel_XYZ(depth)\n",
    "        return kernel_XYZ\n",
    "\n",
    "    def get_kernel_MNI(self, depth: Union[int, float] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get a DataFrame with Kernel Flow MNI coordinates scaled and aligned with the AAL template MNI coordinates.\n",
    "\n",
    "        Args:\n",
    "            depth (Union[int, float], optional): Depth into the brain. Defaults to None (brain surface).\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Kernel Flow MNI coordinate dimensions.\n",
    "        \"\"\"\n",
    "        kernel_XYZ = self.get_kernel_XYZ(depth)\n",
    "        kernel_MNI = pd.DataFrame()\n",
    "        kernel_MNI[\"midpoint_x_MNI\"] = kernel_XYZ[\"midpoint_x_XYZ\"] - self.XYZ_ORIGIN[0]\n",
    "        kernel_MNI[\"midpoint_y_MNI\"] = kernel_XYZ[\"midpoint_y_XYZ\"] - self.XYZ_ORIGIN[1]\n",
    "        kernel_MNI[\"midpoint_z_MNI\"] = kernel_XYZ[\"midpoint_z_XYZ\"] - self.XYZ_ORIGIN[2]\n",
    "        return kernel_MNI\n",
    "\n",
    "    def create_source_detector_adj(\n",
    "        self, depth: Union[int, float] = None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Add the new Kernel Flow XYZ and MNI coordinates to the source/detector DataFrame.\n",
    "\n",
    "        Args:\n",
    "            depth (Union[int, float], optional): Depth into the brain. Defaults to None (brain surface).\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Adjusted source/detector DataFrame.\n",
    "        \"\"\"\n",
    "        kernel_XYZ = self.get_kernel_XYZ(depth)\n",
    "        kernel_MNI = self.get_kernel_MNI(depth)\n",
    "        sd_df_adj = self.sd_df\n",
    "        sd_df_adj[\"midpoint_x_XYZ\"] = kernel_XYZ[\"midpoint_x_XYZ\"]\n",
    "        sd_df_adj[\"midpoint_y_XYZ\"] = kernel_XYZ[\"midpoint_y_XYZ\"]\n",
    "        sd_df_adj[\"midpoint_z_XYZ\"] = kernel_XYZ[\"midpoint_z_XYZ\"]\n",
    "        sd_df_adj[\"midpoint_x_MNI\"] = kernel_MNI[\"midpoint_x_MNI\"]\n",
    "        sd_df_adj[\"midpoint_y_MNI\"] = kernel_MNI[\"midpoint_y_MNI\"]\n",
    "        sd_df_adj[\"midpoint_z_MNI\"] = kernel_MNI[\"midpoint_z_MNI\"]\n",
    "        return sd_df_adj\n",
    "\n",
    "    def plot_coordinates(\n",
    "        self, coord_sys: Literal[\"XYZ\", \"MNI\"], depth: Union[int, float] = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Plot the Kernel Flow XYZ or MNI coordinate system scaled to and aligned with the AAL template.\n",
    "        The bounding surfaces indicate the min and max x, y, and z values of the respective AAL template coordinate system.\n",
    "        A \"depth\" argument is accepted which projects the Kernel Flow coordinates that number of mm into the cortex.\n",
    "\n",
    "        Args:\n",
    "            coord_sys (Literal[\"XYZ\", \"MNI\"]): Coordinate system. \"XYZ\" or \"MNI\".\n",
    "            depth (Union[int, float], optional): Depth into the brain\n",
    "                                                Defaults to None (brain surface).\n",
    "        \"\"\"\n",
    "\n",
    "        def _create_surface(\n",
    "            ax: Axes, val: Union[int, float], dim: Literal[\"x\", \"y\", \"z\"], color: str\n",
    "        ) -> None:\n",
    "            \"\"\"\n",
    "            Create a surface at the specified dimension value.\n",
    "\n",
    "            Args:\n",
    "                ax (pd.Axes): Plot axis.\n",
    "                val (Union[int, float]): Surface value.\n",
    "                dim (Literal[\"x\", \"y\", \"z\"]): Coordinate dimension.\n",
    "                color (str): Surface color.\n",
    "\n",
    "            Raises:\n",
    "                ValueError: Invalid coordinate dimension.\n",
    "            \"\"\"\n",
    "            if coord_sys.lower() == \"xyz\":\n",
    "                x_vals = np.linspace(self.X_MIN - 10, self.X_MAX + 10, 110)\n",
    "                y_vals = np.linspace(self.Y_MIN - 10, self.Y_MAX + 10, 110)\n",
    "                z_vals = np.linspace(self.Z_MIN - 10, self.Z_MAX + 10, 110)\n",
    "            elif coord_sys.lower() == \"mni\":\n",
    "                x_vals = np.linspace(self.MNI_X_MIN - 10, self.MNI_X_MAX + 10, 110)\n",
    "                y_vals = np.linspace(self.MNI_Y_MIN - 10, self.MNI_Y_MAX + 10, 110)\n",
    "                z_vals = np.linspace(self.MNI_Z_MIN - 10, self.MNI_Z_MAX + 10, 110)\n",
    "\n",
    "            if dim == \"x\":\n",
    "                y_grid, z_grid = np.meshgrid(y_vals, z_vals)\n",
    "                x_grid = np.ones_like(y_grid) * val\n",
    "            elif dim == \"y\":\n",
    "                x_grid, z_grid = np.meshgrid(x_vals, z_vals)\n",
    "                y_grid = np.ones_like(x_grid) * val\n",
    "            elif dim == \"z\":\n",
    "                x_grid, y_grid = np.meshgrid(x_vals, y_vals)\n",
    "                z_grid = np.ones_like(x_grid) * val\n",
    "            else:\n",
    "                raise ValueError(\n",
    "                    \"Invalid coordinate dimension. Choose 'x', 'y', or 'z'.\"\n",
    "                )\n",
    "\n",
    "            ax.plot_surface(x_grid, y_grid, z_grid, alpha=0.4, color=color)\n",
    "\n",
    "        gs = gridspec.GridSpec(1, 2, width_ratios=[1, 1.3])\n",
    "        fig = plt.figure(figsize=(18, 12))\n",
    "        ax1 = fig.add_subplot(gs[0], projection=\"3d\")\n",
    "        ax1.view_init(azim=0, elev=270)\n",
    "        ax2 = fig.add_subplot(gs[1], projection=\"3d\")\n",
    "        ax2.view_init(azim=0, elev=0)\n",
    "\n",
    "        if coord_sys.lower() == \"xyz\":\n",
    "            plot_df = self.get_kernel_XYZ(depth)\n",
    "            _create_surface(ax1, self.X_MIN, \"x\", \"r\")\n",
    "            _create_surface(ax1, self.X_MAX, \"x\", \"r\")\n",
    "            _create_surface(ax1, self.Y_MIN, \"y\", \"g\")\n",
    "            _create_surface(ax1, self.Y_MAX, \"y\", \"g\")\n",
    "            _create_surface(ax2, self.Y_MIN, \"y\", \"g\")\n",
    "            _create_surface(ax2, self.Y_MAX, \"y\", \"g\")\n",
    "            _create_surface(ax2, self.Z_MIN, \"z\", \"b\")\n",
    "            _create_surface(ax2, self.Z_MAX, \"z\", \"b\")\n",
    "        elif coord_sys.lower() == \"mni\":\n",
    "            plot_df = self.get_kernel_MNI(depth)\n",
    "            _create_surface(ax1, self.MNI_X_MIN, \"x\", \"r\")\n",
    "            _create_surface(ax1, self.MNI_X_MAX, \"x\", \"r\")\n",
    "            _create_surface(ax1, self.MNI_Y_MIN, \"y\", \"g\")\n",
    "            _create_surface(ax1, self.MNI_Y_MAX, \"y\", \"g\")\n",
    "            _create_surface(ax2, self.MNI_Y_MIN, \"y\", \"g\")\n",
    "            _create_surface(ax2, self.MNI_Y_MAX, \"y\", \"g\")\n",
    "            _create_surface(ax2, self.MNI_Z_MIN, \"z\", \"b\")\n",
    "            _create_surface(ax2, self.MNI_Z_MAX, \"z\", \"b\")\n",
    "        else:\n",
    "            raise ValueError(\"Invalid coordinate system type. Must be 'XYZ' or 'MNI'.\")\n",
    "\n",
    "        ax1.scatter(\n",
    "            list(plot_df[f\"midpoint_x_{coord_sys.upper()}\"].iloc[::2]),\n",
    "            list(plot_df[f\"midpoint_y_{coord_sys.upper()}\"].iloc[::2]),\n",
    "            list(plot_df[f\"midpoint_z_{coord_sys.upper()}\"].iloc[::2]),\n",
    "            color=\"blue\",\n",
    "            s=20,\n",
    "            alpha=1,\n",
    "        )\n",
    "        ax2.scatter(\n",
    "            list(plot_df[f\"midpoint_x_{coord_sys.upper()}\"].iloc[::2]),\n",
    "            list(plot_df[f\"midpoint_y_{coord_sys.upper()}\"].iloc[::2]),\n",
    "            list(plot_df[f\"midpoint_z_{coord_sys.upper()}\"].iloc[::2]),\n",
    "            color=\"blue\",\n",
    "            s=20,\n",
    "        )\n",
    "        ax1.set_xlabel(\n",
    "            f\"{coord_sys.upper()} x-axis (mm)\",\n",
    "            fontweight=\"bold\",\n",
    "            fontsize=14,\n",
    "            labelpad=10,\n",
    "        )\n",
    "        ax1.set_ylabel(\n",
    "            f\"{coord_sys.upper()} y-axis (mm)\",\n",
    "            fontweight=\"bold\",\n",
    "            fontsize=14,\n",
    "            labelpad=15,\n",
    "        )\n",
    "        ax2.set_ylabel(\n",
    "            f\"{coord_sys.upper()} y-axis (mm)\",\n",
    "            fontweight=\"bold\",\n",
    "            fontsize=14,\n",
    "            labelpad=15,\n",
    "        )\n",
    "        ax2.set_zlabel(\n",
    "            f\"{coord_sys.upper()} z-axis (mm)\",\n",
    "            fontweight=\"bold\",\n",
    "            fontsize=14,\n",
    "            labelpad=5,\n",
    "        )\n",
    "        ax1.set_zticks([])\n",
    "        ax1.set_zticklabels([])\n",
    "        ax2.set_xticks([])\n",
    "        ax2.set_xticklabels([])\n",
    "        ax1.set_title(\"Top View\", fontweight=\"bold\", fontsize=18, y=0.96)\n",
    "        ax2.set_title(\"Right View\", fontweight=\"bold\", fontsize=18, y=0.85)\n",
    "        fig.text(\n",
    "            0.43,\n",
    "            0.5,\n",
    "            \"Anterior\",\n",
    "            fontweight=\"bold\",\n",
    "            fontsize=14,\n",
    "            rotation=90,\n",
    "            va=\"center\",\n",
    "        )\n",
    "        fig.text(\n",
    "            0.84,\n",
    "            0.5,\n",
    "            \"Anterior\",\n",
    "            fontweight=\"bold\",\n",
    "            fontsize=14,\n",
    "            rotation=90,\n",
    "            va=\"center\",\n",
    "        )\n",
    "        fig.suptitle(\n",
    "            f\"Kernel Flow {coord_sys.upper()} Coordinates (cortical depth: {depth} mm)\",\n",
    "            fontweight=\"bold\",\n",
    "            fontsize=20,\n",
    "            x=0.465,\n",
    "            y=0.78,\n",
    "        )\n",
    "        fig.subplots_adjust(wspace=0)\n",
    "        plt.show()\n",
    "\n",
    "    def _run_basic_checks(self) -> None:\n",
    "        \"\"\"\n",
    "        Run assertion tests to ensure all steps were successful.\n",
    "        Tolerance = 0.00001 mm (round 5).\n",
    "        \"\"\"\n",
    "        # kernel position scaling\n",
    "        assert (\n",
    "            round(\n",
    "                (self.FLOW_X_MAX - self.FLOW_X_MIN) * self._get_scaling_factor(\"x\"), 5\n",
    "            )\n",
    "        ) == self.X_MAX_ADJ - self.X_MIN_ADJ, (\n",
    "            \"Test failed: FLOW_X did not scale correctly.\"\n",
    "        )\n",
    "        assert (\n",
    "            round(\n",
    "                (self.FLOW_Y_MAX - self.FLOW_Y_MIN) * self._get_scaling_factor(\"y\"), 5\n",
    "            )\n",
    "        ) == self.Y_MAX_ADJ - self.Y_MIN_ADJ, (\n",
    "            \"Test failed: FLOW_Y did not scale correctly.\"\n",
    "        )\n",
    "        assert (\n",
    "            round(\n",
    "                (self.FLOW_Z_MAX - self.FLOW_Z_MIN) * self._get_scaling_factor(\"z\"), 5\n",
    "            )\n",
    "        ) == self.Z_MAX_ADJ - self.Z_MIN_ADJ, (\n",
    "            \"Test failed: FLOW_Z did not scale correctly.\"\n",
    "        )\n",
    "\n",
    "        # kernel DataFrame scaling\n",
    "        sd_df_scaled = self._scale_kernel_XYZ()\n",
    "        assert (\n",
    "            round(\n",
    "                sd_df_scaled[\"midpoint_x_pos\"].max()\n",
    "                - sd_df_scaled[\"midpoint_x_pos\"].min(),\n",
    "                5,\n",
    "            )\n",
    "            == self.X_MAX_ADJ - self.X_MIN_ADJ\n",
    "        ), \"Test failed. Flow DataFrame x-pos did not scale correctly.\"\n",
    "        assert (\n",
    "            round(\n",
    "                sd_df_scaled[\"midpoint_y_pos\"].max()\n",
    "                - sd_df_scaled[\"midpoint_y_pos\"].min(),\n",
    "                5,\n",
    "            )\n",
    "            == self.Y_MAX_ADJ - self.Y_MIN_ADJ\n",
    "        ), \"Test failed. Flow DataFrame y-pos did not scale correctly.\"\n",
    "        assert (\n",
    "            round(\n",
    "                sd_df_scaled[\"midpoint_z_pos\"].max()\n",
    "                - sd_df_scaled[\"midpoint_z_pos\"].min(),\n",
    "                5,\n",
    "            )\n",
    "            == self.Z_MAX_ADJ - self.Z_MIN_ADJ\n",
    "        ), \"Test failed. Flow DataFrame z-pos did not scale correctly.\"\n",
    "\n",
    "        # kernel DataFrame alignment\n",
    "        sd_df_aligned = self._align_kernel_XYZ()\n",
    "        assert (\n",
    "            round(sd_df_aligned[\"midpoint_x_XYZ\"].min(), 5) == self.X_MIN_ADJ\n",
    "        ), \"Test failed. Flow DataFrame x-pos min did not align correctly.\"\n",
    "        assert (\n",
    "            round(sd_df_aligned[\"midpoint_y_XYZ\"].min(), 5) == self.Y_MIN_ADJ\n",
    "        ), \"Test failed. Flow DataFrame y-pos min did not align correctly.\"\n",
    "        assert (\n",
    "            round(sd_df_aligned[\"midpoint_z_XYZ\"].min(), 5) == self.Z_MIN_ADJ\n",
    "        ), \"Test failed. Flow DataFrame z-pos min did not align correctly.\"\n",
    "        assert (\n",
    "            round(sd_df_aligned[\"midpoint_x_XYZ\"].max(), 5) == self.X_MAX_ADJ\n",
    "        ), \"Test failed. Flow DataFrame x-pos max did not align correctly.\"\n",
    "        assert (\n",
    "            round(sd_df_aligned[\"midpoint_y_XYZ\"].max(), 5) == self.Y_MAX_ADJ\n",
    "        ), \"Test failed. Flow DataFrame y-pos max did not align correctly.\"\n",
    "        assert (\n",
    "            round(sd_df_aligned[\"midpoint_z_XYZ\"].max(), 5) == self.Z_MAX_ADJ\n",
    "        ), \"Test failed. Flow DataFrame z-pos max did not align correctly.\"\n",
    "\n",
    "    def _run_translation_check(\n",
    "        self, df: pd.DataFrame, depth: Tuple[int, float] = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Run an assertion test to check Kernel XYZ translation.\n",
    "        \"\"\"\n",
    "        assert (\n",
    "            round(df[\"midpoint_x_XYZ\"].min(), 5) == self.X_MIN_ADJ + depth\n",
    "        ), \"Test failed. Flow DataFrame x-pos min did not translate correctly.\"\n",
    "        assert (\n",
    "            round(df[\"midpoint_y_XYZ\"].min(), 5) == self.Y_MIN_ADJ + depth\n",
    "        ), \"Test failed. Flow DataFrame y-pos min did not translate correctly.\"\n",
    "        assert (\n",
    "            round(df[\"midpoint_z_XYZ\"].min(), 5) == self.Z_MIN_ADJ + depth\n",
    "        ), \"Test failed. Flow DataFrame z-pos min did not translate correctly.\"\n",
    "        assert (\n",
    "            round(df[\"midpoint_x_XYZ\"].max(), 5) == self.X_MAX_ADJ - depth\n",
    "        ), \"Test failed. Flow DataFrame x-pos max did not translate correctly.\"\n",
    "        assert (\n",
    "            round(df[\"midpoint_y_XYZ\"].max(), 5) == self.Y_MAX_ADJ - depth\n",
    "        ), \"Test failed. Flow DataFrame y-pos max did not translate correctly.\"\n",
    "        assert (\n",
    "            round(df[\"midpoint_z_XYZ\"].max(), 5) == self.Z_MAX_ADJ - depth\n",
    "        ), \"Test failed. Flow DataFrame z-pos max did not translate correctly.\"\n",
    "\n",
    "\n",
    "class Process_Flow:\n",
    "    \"\"\"\n",
    "    This class contains functions for processing Kernel Flow data.\n",
    "    Wrapper around an snirf.Snirf object.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, filepath: str) -> None:\n",
    "        \"\"\"\n",
    "        Initialize by loading SNIRF file.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to SNIRF file.\n",
    "        \"\"\"\n",
    "        self.data_fun = Data_Functions()\n",
    "        self.snirf_file = self.load_snirf(filepath)\n",
    "\n",
    "        self.missing_detector_pos_2d = [\n",
    "            [0.2700519522879849, 0.92534462173171],\n",
    "            [0.2100404073350992, 0.9599923033647436],\n",
    "            [0.1500288623822143, 0.92534462173171],\n",
    "            [0.1500288623822143, 0.856049258465643],\n",
    "            [0.2100404073350992, 0.8214015768326095],\n",
    "            [0.2700519522879849, 0.856049258465643],\n",
    "        ]\n",
    "        self.missing_source_pos_2d = [0.2100404073350983, 0.8906969400986755]\n",
    "        self.missing_detector_pos_3d = [\n",
    "            [34.18373257128052, 83.84749436111261, -3.421772079425661],\n",
    "            [24.89193921324638, 87.59280827807989, -3.877662542873584],\n",
    "            [19.49960518952535, 88.52633022589306, 4.53462776618961],\n",
    "            [23.69484819349888, 86.5963118571706, 13.38774165295894],\n",
    "            [32.93421777049451, 82.87888296072012, 13.83928277924401],\n",
    "            [37.86338484008788, 80.87503761567585, 5.394829563438814],\n",
    "        ]\n",
    "        self.missing_source_pos_3d = [\n",
    "            28.65886271209007,\n",
    "            84.52123706248807,\n",
    "            4.746746612880643,\n",
    "        ]\n",
    "        self.missing_measurement_list_data = {\n",
    "            \"measurement_list_index\": [float(\"NaN\")] * 12,\n",
    "            \"data_type\": [99999] * 12,\n",
    "            \"data_type_index\": [\"HbO\", \"HbR\"] * 6,\n",
    "            \"detector_index\": [\n",
    "                307,\n",
    "                307,\n",
    "                308,\n",
    "                308,\n",
    "                309,\n",
    "                309,\n",
    "                310,\n",
    "                310,\n",
    "                311,\n",
    "                311,\n",
    "                312,\n",
    "                312,\n",
    "            ],\n",
    "            \"source_index\": [0] * 12,\n",
    "        }\n",
    "\n",
    "    def load_snirf(self, filepath: str) -> snirf.Snirf:\n",
    "        \"\"\"\n",
    "        Load SNIRF file.\n",
    "\n",
    "        Args:\n",
    "            filepath (str): Path to SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            snirf.Snirf: SNIRF file object.\n",
    "        \"\"\"\n",
    "        return snirf.Snirf(filepath, \"r+\", dynamic_loading=True)\n",
    "\n",
    "    def get_time_origin(\n",
    "        self, fmt: str = \"datetime\", offset=True\n",
    "    ) -> Union[datetime.datetime, float]:\n",
    "        \"\"\"\n",
    "        Get the time origin (start time) from the SNIRF file.\n",
    "\n",
    "        Args:\n",
    "            fmt (str, optional): Format to get the time origin in: \"datetime\" or \"timestamp\". Defaults to \"datetime\".\n",
    "            offset (bool): Offset the datetime by 4 hours. Defaults to True.\n",
    "\n",
    "        Raises:\n",
    "            Exception: Invalid fmt argument.\n",
    "\n",
    "        Returns:\n",
    "            Union[datetime.datetime, float]:\n",
    "                datetime.datetime: Time origin datetime.\n",
    "                -or-\n",
    "                float: Time origin timestamp.\n",
    "        \"\"\"\n",
    "        start_date = self.snirf_file.nirs[0].metaDataTags.MeasurementDate\n",
    "        start_time = self.snirf_file.nirs[0].metaDataTags.MeasurementTime\n",
    "        start_str = start_date + \" \" + start_time\n",
    "        if offset:\n",
    "            time_origin = datetime.datetime.strptime(\n",
    "                start_str, \"%Y-%m-%d %H:%M:%S\"\n",
    "            ) - datetime.timedelta(\n",
    "                hours=4\n",
    "            )  # 4 hour offset\n",
    "        else:\n",
    "            time_origin = datetime.datetime.strptime(start_str, \"%Y-%m-%d %H:%M:%S\")\n",
    "        if fmt.lower() == \"datetime\":\n",
    "            return time_origin\n",
    "        elif fmt.lower() == \"timestamp\":\n",
    "            return datetime.datetime.timestamp(time_origin)\n",
    "        else:\n",
    "            raise Exception(\n",
    "                \"Invalid 'fmt' argument. Must be 'datetime' or 'timestamp'.\"\n",
    "            )\n",
    "\n",
    "    def get_subject_ID(self) -> str:\n",
    "        \"\"\"\n",
    "        Get the subject ID from the SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            str: Subject ID.\n",
    "        \"\"\"\n",
    "        return self.snirf_file.nirs[0].metaDataTags.SubjectID\n",
    "\n",
    "    def get_time_rel(self) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get the relative time array from the SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Relative time array.\n",
    "        \"\"\"\n",
    "        return self.snirf_file.nirs[0].data[0].time\n",
    "\n",
    "    def get_time_abs(self, fmt: str = \"datetime\") -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Convert relative time array into an absolute time array.\n",
    "\n",
    "        Args:\n",
    "            fmt (str, optional): Format to get the time array in: \"datetime\" or \"timestamp\". Defaults to \"datetime\".\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Absolute time array.\n",
    "        \"\"\"\n",
    "        time_rel = self.get_time_rel()\n",
    "        if fmt.lower() == \"datetime\":\n",
    "            time_origin_dt = self.get_time_origin(\"datetime\")\n",
    "            return np.array(\n",
    "                [\n",
    "                    datetime.timedelta(seconds=time_rel[i]) + time_origin_dt\n",
    "                    for i in range(len(time_rel))\n",
    "                ]\n",
    "            )\n",
    "        elif fmt.lower() == \"timestamp\":\n",
    "            time_origin_ts = self.get_time_origin(\"timestamp\")\n",
    "            return time_rel + time_origin_ts\n",
    "\n",
    "    def get_data(\n",
    "        self, fmt: str = \"array\", cols: list[int | list | tuple] = None\n",
    "    ) -> Union[np.ndarray, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Get timeseries data from the SNIRF file.\n",
    "\n",
    "        Args:\n",
    "            fmt (str): Format of data (np.ndarray or pd.DataFrame). Defaults to \"array\".\n",
    "            cols (list[int | list | tuple]): Data cols to select. Single col, list of cols, or slice of cols.\n",
    "                                             Defaults to None (all columns).\n",
    "\n",
    "        Raises:\n",
    "            Exception: Invalid fmt argument.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Timeseries data array.\n",
    "        \"\"\"\n",
    "        if cols or cols == 0:\n",
    "            if isinstance(cols, tuple):\n",
    "                data = (\n",
    "                    self.snirf_file.nirs[0].data[0].dataTimeSeries[:, cols[0] : cols[1]]\n",
    "                )\n",
    "            else:\n",
    "                data = self.snirf_file.nirs[0].data[0].dataTimeSeries[:, cols]\n",
    "        else:\n",
    "            data = self.snirf_file.nirs[0].data[0].dataTimeSeries\n",
    "\n",
    "        if \"array\" in fmt.lower():\n",
    "            return data\n",
    "        elif \"dataframe\" in fmt.lower():\n",
    "            return pd.DataFrame(data)\n",
    "        else:\n",
    "            raise Exception(\"Invalid fmt argument. Must be 'array' or 'dataframe'.\")\n",
    "\n",
    "    def get_source_pos(self, dim: str, add_missing: bool = False) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get the 2D or 3D source position array.\n",
    "\n",
    "        Args:\n",
    "            dim (str): Position data dimension \"2D\" or \"3D\".\n",
    "            add_missing (bool): Add missing source data. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: 2D or 3D source position array.\n",
    "        \"\"\"\n",
    "        if dim.lower() == \"2d\":\n",
    "            source_pos_array_og = self.snirf_file.nirs[0].probe.sourcePos2D\n",
    "            if add_missing:\n",
    "                source_pos_array = np.vstack(\n",
    "                    [np.array(self.missing_source_pos_2d), source_pos_array_og]\n",
    "                )\n",
    "                return source_pos_array\n",
    "            else:\n",
    "                return source_pos_array_og\n",
    "        elif dim.lower() == \"3d\":\n",
    "            source_pos_array_og = self.snirf_file.nirs[0].probe.sourcePos3D\n",
    "            if add_missing:\n",
    "                source_pos_array = np.vstack(\n",
    "                    [np.array(self.missing_source_pos_3d), source_pos_array_og]\n",
    "                )\n",
    "                return source_pos_array\n",
    "            else:\n",
    "                return source_pos_array_og\n",
    "\n",
    "    def get_detector_pos(self, dim: str, add_missing: bool = False) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Get the 2D or 3D detector position array.\n",
    "\n",
    "        Args:\n",
    "            dim (str): Position data dimension \"2D\" or \"3D\".\n",
    "            add_missing (bool): Add missing detector data. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: 2D or 3D detector position array.\n",
    "        \"\"\"\n",
    "        if dim.lower() == \"2d\":\n",
    "            detector_pos_array_og = self.snirf_file.nirs[0].probe.detectorPos2D\n",
    "            if add_missing:\n",
    "                detector_pos_array = np.vstack(\n",
    "                    [np.array(self.missing_detector_pos_2d), detector_pos_array_og]\n",
    "                )\n",
    "                return detector_pos_array\n",
    "            else:\n",
    "                return detector_pos_array_og\n",
    "        elif dim.lower() == \"3d\":\n",
    "            detector_pos_array_og = self.snirf_file.nirs[0].probe.detectorPos3D\n",
    "            if add_missing:\n",
    "                detector_pos_array_og = self.snirf_file.nirs[0].probe.detectorPos3D\n",
    "                detector_pos_array = np.vstack(\n",
    "                    [np.array(self.missing_detector_pos_3d), detector_pos_array_og]\n",
    "                )\n",
    "                return detector_pos_array\n",
    "            else:\n",
    "                return detector_pos_array_og\n",
    "\n",
    "    def get_measurement_list(self) -> np.array:\n",
    "        \"\"\"\n",
    "        Get the data measurement list.\n",
    "\n",
    "        Returns:\n",
    "            np.array: Data measurement list array.\n",
    "        \"\"\"\n",
    "        return self.snirf_file.nirs[0].data[0].measurementList\n",
    "\n",
    "    def get_source_labels(self, add_missing: bool = False) -> np.array:\n",
    "        \"\"\"\n",
    "        Get the source labels.\n",
    "\n",
    "        Args:\n",
    "            add_missing (bool): Add missing source label. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            np.array: Source label array.\n",
    "        \"\"\"\n",
    "        source_labels_og = self.snirf_file.nirs[0].probe.sourceLabels\n",
    "        if add_missing:\n",
    "            missing_source_label = \"S00\"\n",
    "            source_labels = np.insert(source_labels_og, 0, missing_source_label)\n",
    "            return source_labels\n",
    "        else:\n",
    "            return source_labels_og\n",
    "\n",
    "    def get_detector_labels(self, add_missing: bool = False) -> np.array:\n",
    "        \"\"\"\n",
    "        Get the detector labels.\n",
    "\n",
    "        Args:\n",
    "            add_missing (bool): Add missing detector labels. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            np.array: Detector label array.\n",
    "        \"\"\"\n",
    "        detector_labels_og = self.snirf_file.nirs[0].probe.detectorLabels\n",
    "        if add_missing:\n",
    "            missing_detector_labels = [\n",
    "                \"D00d0\",\n",
    "                \"D00d1\",\n",
    "                \"D00d2\",\n",
    "                \"D00d3\",\n",
    "                \"D00d4\",\n",
    "                \"D00d5\",\n",
    "            ]\n",
    "            detector_labels = np.insert(detector_labels_og, 0, missing_detector_labels)\n",
    "            return detector_labels\n",
    "        else:\n",
    "            return detector_labels_og\n",
    "\n",
    "    def get_marker_df(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get a DataFrame of marker data from the \"stim\" part of the SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Marker \"stim\" data.\n",
    "        \"\"\"\n",
    "        marker_data = self.snirf_file.nirs[0].stim[0].data\n",
    "        marker_data_cols = self.snirf_file.nirs[0].stim[0].dataLabels\n",
    "        return pd.DataFrame(marker_data, columns=marker_data_cols)\n",
    "\n",
    "    def get_unique_data_types(self) -> list:\n",
    "        \"\"\"\n",
    "        Get unique data types from the SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            list: Unique data types.\n",
    "        \"\"\"\n",
    "        data_types = []\n",
    "        for i in range(len(self.snirf_file.nirs[0].data[0].measurementList)):\n",
    "            data_type = self.snirf_file.nirs[0].data[0].measurementList[i].dataType\n",
    "            if data_type not in data_types:\n",
    "                data_types.append(data_type)\n",
    "        return data_types\n",
    "\n",
    "    def get_data_type_label(self, channel_num: int) -> str:\n",
    "        \"\"\"\n",
    "        Get the data type label for a channel(s).\n",
    "\n",
    "        Args:\n",
    "            channel_num (int): Channel number to get the data type label of.\n",
    "\n",
    "        Returns:\n",
    "            str: Data type label of the channel.\n",
    "        \"\"\"\n",
    "        return (\n",
    "            self.snirf_file.nirs[0].data[0].measurementList[channel_num].dataTypeLabel\n",
    "        )\n",
    "\n",
    "    def get_unique_data_type_labels(self) -> list:\n",
    "        \"\"\"\n",
    "        Get unique data type labels from the SNIRF file.\n",
    "\n",
    "        Returns:\n",
    "            list: Unique data type labels.\n",
    "        \"\"\"\n",
    "        data_type_labels = []\n",
    "        for i in range(len(self.snirf_file.nirs[0].data[0].measurementList)):\n",
    "            data_type_label = (\n",
    "                self.snirf_file.nirs[0].data[0].measurementList[i].dataTypeLabel\n",
    "            )\n",
    "            if data_type_label not in data_type_labels:\n",
    "                data_type_labels.append(data_type_label)\n",
    "        return data_type_labels\n",
    "\n",
    "    def create_source_dict(self) -> dict:\n",
    "        \"\"\"\n",
    "        Count the occurrences of each source index.\n",
    "\n",
    "        Returns:\n",
    "            dict: Counts for each source index.\n",
    "        \"\"\"\n",
    "        source_dict = {}\n",
    "        for i in range(len(self.snirf_file.nirs[0].data[0].measurementList)):\n",
    "            source = self.snirf_file.nirs[0].data[0].measurementList[i].sourceIndex\n",
    "            source_dict[source] = source_dict.get(source, 0) + 1\n",
    "        source_dict = self.data_fun.sort_dict(source_dict, \"keys\")\n",
    "        return source_dict\n",
    "\n",
    "    def create_detector_dict(self) -> dict:\n",
    "        \"\"\"\n",
    "        Count the occurrences of each detector index.\n",
    "\n",
    "        Returns:\n",
    "            dict: Counts for each detector index.\n",
    "        \"\"\"\n",
    "        detector_dict = {}\n",
    "        for i in range(len(self.snirf_file.nirs[0].data[0].measurementList)):\n",
    "            detector = self.snirf_file.nirs[0].data[0].measurementList[i].detectorIndex\n",
    "            detector_dict[detector] = detector_dict.get(detector, 0) + 1\n",
    "        detector_dict = self.data_fun.sort_dict(detector_dict, \"keys\")\n",
    "        return detector_dict\n",
    "\n",
    "    def create_measurement_list_df(self, add_missing: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create a DataFrame with all the data measurement list information.\n",
    "\n",
    "        Args:\n",
    "            add_missing (bool): Add missing detector data. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Data measurement list DataFrame.\n",
    "        \"\"\"\n",
    "        measurement_list = self.get_measurement_list()\n",
    "        dict_list = []\n",
    "\n",
    "        for i in range(len(measurement_list)):\n",
    "            measurement_list_i = measurement_list[i]\n",
    "            measurement_dict = {}\n",
    "            measurement_dict[\"measurement_list_index\"] = i + 1\n",
    "            measurement_dict[\"data_type\"] = measurement_list_i.dataType\n",
    "            measurement_dict[\"data_type_index\"] = measurement_list_i.dataTypeLabel\n",
    "            measurement_dict[\"detector_index\"] = measurement_list_i.detectorIndex\n",
    "            measurement_dict[\"source_index\"] = measurement_list_i.sourceIndex\n",
    "            dict_list.append(measurement_dict)\n",
    "\n",
    "        measurement_list_df = pd.DataFrame(dict_list)\n",
    "\n",
    "        if add_missing:\n",
    "            missing_data_df = pd.DataFrame(self.missing_measurement_list_data)\n",
    "            measurement_list_df = pd.concat(\n",
    "                [missing_data_df, measurement_list_df], ignore_index=True\n",
    "            )\n",
    "            measurement_list_df[\"measurement_list_index\"] = measurement_list_df[\n",
    "                \"measurement_list_index\"\n",
    "            ].astype(pd.Int64Dtype())\n",
    "        return measurement_list_df\n",
    "\n",
    "    def create_source_df(self, dim: str, add_missing: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create a DataFrame with the source labels and 2D or 3D source positions.\n",
    "\n",
    "        Args:\n",
    "            dim (str): Position data dimension \"2D\" or \"3D\".\n",
    "            add_missing (bool): Add missing detector data. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Source labels and positions.\n",
    "        \"\"\"\n",
    "        source_labels = self.get_source_labels(add_missing)\n",
    "        if dim.lower() == \"2d\":\n",
    "            source_pos_2d = self.get_source_pos(dim, add_missing)\n",
    "            source_data = [\n",
    "                (label, *lst) for label, lst in zip(source_labels, source_pos_2d)\n",
    "            ]\n",
    "            source_df = pd.DataFrame(\n",
    "                source_data, columns=[\"source_label\", \"source_x_pos\", \"source_y_pos\"]\n",
    "            )\n",
    "        elif dim.lower() == \"3d\":\n",
    "            source_pos_3d = self.get_source_pos(dim, add_missing)\n",
    "            source_data = [\n",
    "                (label, *lst) for label, lst in zip(source_labels, source_pos_3d)\n",
    "            ]\n",
    "            source_df = pd.DataFrame(\n",
    "                source_data,\n",
    "                columns=[\n",
    "                    \"source_label\",\n",
    "                    \"source_x_pos\",\n",
    "                    \"source_y_pos\",\n",
    "                    \"source_z_pos\",\n",
    "                ],\n",
    "            )\n",
    "        # NOTE: Kernel changed source and detector label formats after a certain date\n",
    "        try:\n",
    "            f = lambda x: int(x.lstrip(\"S\"))\n",
    "            source_df.insert(1, \"source_index\", source_df[\"source_label\"].apply(f))\n",
    "        except ValueError:  # Format changed for participants 12+\n",
    "            f = lambda x: int(x[1:4].lstrip(\"0\"))\n",
    "            source_df.insert(1, \"source_index\", source_df[\"source_label\"].apply(f))\n",
    "        return source_df\n",
    "\n",
    "    def create_detector_df(self, dim: str, add_missing: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create a DataFrame with the detector labels and 2D or 3D detector positions.\n",
    "\n",
    "        Args:\n",
    "            dim (str): Position data dimension \"2D\" or \"3D\".\n",
    "            add_missing (bool): Add missing detector data. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Detector labels and positions.\n",
    "        \"\"\"\n",
    "        detector_labels = self.get_detector_labels(add_missing)\n",
    "        if dim.lower() == \"2d\":\n",
    "            detector_pos_2d = self.get_detector_pos(dim, add_missing)\n",
    "            detector_data = [\n",
    "                (label, *lst) for label, lst in zip(detector_labels, detector_pos_2d)\n",
    "            ]\n",
    "            detector_df = pd.DataFrame(\n",
    "                detector_data,\n",
    "                columns=[\"detector_label\", \"detector_x_pos\", \"detector_y_pos\"],\n",
    "            )\n",
    "        elif dim.lower() == \"3d\":\n",
    "            detector_pos_3d = self.get_detector_pos(dim, add_missing)\n",
    "            detector_data = [\n",
    "                (label, *lst) for label, lst in zip(detector_labels, detector_pos_3d)\n",
    "            ]\n",
    "            detector_df = pd.DataFrame(\n",
    "                detector_data,\n",
    "                columns=[\n",
    "                    \"detector_label\",\n",
    "                    \"detector_x_pos\",\n",
    "                    \"detector_y_pos\",\n",
    "                    \"detector_z_pos\",\n",
    "                ],\n",
    "            )\n",
    "        # NOTE: Kernel changed source and detector label formats after a certain date\n",
    "        if len(detector_df[\"detector_label\"][7]) == 5:\n",
    "            f = lambda x: int(x[1:3])\n",
    "        elif (\n",
    "            len(detector_df[\"detector_label\"][7]) == 7\n",
    "        ):  # Format changed for participants 12+\n",
    "            f = lambda x: int(x[2:4])\n",
    "\n",
    "        detector_df.insert(1, \"source_index\", detector_df[\"detector_label\"].apply(f))\n",
    "        if add_missing:\n",
    "            detector_index_col = []\n",
    "            for i in range(307, 313):\n",
    "                detector_index_col.append(i)\n",
    "            for i in range(1, detector_df.shape[0] - 5):\n",
    "                detector_index_col.append(i)\n",
    "            detector_df.insert(1, \"detector_index\", detector_index_col)\n",
    "        else:\n",
    "            detector_df.insert(1, \"detector_index\", range(1, detector_df.shape[0] + 1))\n",
    "        return detector_df\n",
    "\n",
    "    def create_source_detector_df(\n",
    "        self,\n",
    "        dim: str,\n",
    "        add_missing: bool = False,\n",
    "        midpoint_only: bool = False,\n",
    "        MNI: bool = False,\n",
    "        brain_regions: bool = False,\n",
    "        channels: Union[List[int], int] = None,\n",
    "        depth: Union[int, float] = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create a DataFrame with the source and detector information for the inter-module channels.\n",
    "\n",
    "        Args:\n",
    "            dim (str): Position data dimension \"2D\" or \"3D\".\n",
    "            add_missing (bool): Add missing detector data. Defaults to False.\n",
    "            midpoint_only (bool): Include only source/detector midpoint coordinate dimensions. Default to False.\n",
    "            MNI (bool): Include MNI coordinate system columns. Defaults to False.\n",
    "            brain_regions (bool): Include AAL and BA brain region columns. Defaults to False.\n",
    "            channels (Union[List[int], int]): Return only specific channel(s). Defaults to None.\n",
    "            depth (Union[int, float], optional): Depth into the brain. Defaults to None (brain surface).\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Source and detector information for inter-module channels.\n",
    "        \"\"\"\n",
    "        measurement_list_df = self.create_measurement_list_df(add_missing)\n",
    "        if dim.lower() == \"2d\":\n",
    "            source_df = self.create_source_df(\"2D\", add_missing)\n",
    "            detector_df = self.create_detector_df(\"2D\", add_missing)\n",
    "        elif dim.lower() == \"3d\":\n",
    "            source_df = self.create_source_df(\"3D\", add_missing)\n",
    "            detector_df = self.create_detector_df(\"3D\", add_missing)\n",
    "        source_merge = pd.merge(measurement_list_df, source_df, on=\"source_index\")\n",
    "        merged_source_detector_df = pd.merge(\n",
    "            source_merge, detector_df, on=[\"detector_index\", \"source_index\"]\n",
    "        )\n",
    "        source_detector_df = merged_source_detector_df.copy()\n",
    "        source_detector_df.insert(\n",
    "            0, \"channel_num\", source_detector_df[\"measurement_list_index\"] - 1\n",
    "        )\n",
    "\n",
    "        if dim.lower() == \"2d\":\n",
    "            if isinstance(channels, int):\n",
    "                source_detector_df = source_detector_df[\n",
    "                    source_detector_df[\"channel_num\"] == channels\n",
    "                ].copy()\n",
    "            elif isinstance(channels, list):\n",
    "                source_detector_df = source_detector_df[\n",
    "                    source_detector_df[\"channel_num\"].isin(channels)\n",
    "                ].copy()\n",
    "        elif dim.lower() == \"3d\":\n",
    "            source_detector_df[\n",
    "                [\"midpoint_x_pos\", \"midpoint_y_pos\", \"midpoint_z_pos\"]\n",
    "            ] = source_detector_df.apply(\n",
    "                lambda row: self.get_midpoint(\n",
    "                    (row[\"source_x_pos\"], row[\"source_y_pos\"], row[\"source_z_pos\"]),\n",
    "                    (\n",
    "                        row[\"detector_x_pos\"],\n",
    "                        row[\"detector_y_pos\"],\n",
    "                        row[\"detector_z_pos\"],\n",
    "                    ),\n",
    "                ),\n",
    "                axis=1,\n",
    "                result_type=\"expand\",\n",
    "            )\n",
    "\n",
    "            if MNI or brain_regions:\n",
    "                # add source/detector MNI coordinates\n",
    "                FC = Flow_Coordinates(source_detector_df)\n",
    "                source_detector_df = FC.create_source_detector_adj(depth)\n",
    "            if isinstance(channels, int):\n",
    "                source_detector_df = source_detector_df[\n",
    "                    source_detector_df[\"channel_num\"] == channels\n",
    "                ].copy()\n",
    "            elif isinstance(channels, list):\n",
    "                source_detector_df = source_detector_df[\n",
    "                    source_detector_df[\"channel_num\"].isin(channels)\n",
    "                ].copy()\n",
    "            if brain_regions:\n",
    "                # load R script files here to improve performance\n",
    "                with open(\n",
    "                    os.path.join(\n",
    "                        os.getcwd(), \"label4MRI\", \"R\", \"mni_to_region_index.R\"\n",
    "                    ),\n",
    "                    \"r\",\n",
    "                ) as file:\n",
    "                    mni_to_region_index_code = \"\".join(file.readlines())\n",
    "                with open(\n",
    "                    os.path.join(os.getcwd(), \"label4MRI\", \"R\", \"mni_to_region_name.R\"),\n",
    "                    \"r\",\n",
    "                ) as file:\n",
    "                    mni_to_region_name_code = \"\".join(file.readlines())\n",
    "                # evaluate R code\n",
    "                metadata_path = os.path.join(\n",
    "                    os.getcwd(), \"label4MRI\", \"data\", \"metadata.RData\"\n",
    "                )\n",
    "                load_rdata = robjects.r[\"load\"]\n",
    "                load_rdata(metadata_path)\n",
    "                robjects.r(mni_to_region_index_code)\n",
    "                robjects.r(mni_to_region_name_code)\n",
    "                # R function as Python callable\n",
    "                self.mni_to_region_name = robjects.globalenv[\"mni_to_region_name\"]\n",
    "\n",
    "                source_detector_df[\n",
    "                    [\"AAL_distance\", \"AAL_region\", \"BA_distance\", \"BA_region\"]\n",
    "                ] = source_detector_df.apply(\n",
    "                    lambda row: self.MNI_to_region(\n",
    "                        row[\"midpoint_x_MNI\"],\n",
    "                        row[\"midpoint_y_MNI\"],\n",
    "                        row[\"midpoint_z_MNI\"],\n",
    "                    ),\n",
    "                    axis=1,\n",
    "                    result_type=\"expand\",\n",
    "                )\n",
    "            if midpoint_only:\n",
    "                source_detector_df = source_detector_df.drop(\n",
    "                    columns=[\n",
    "                        \"source_x_pos\",\n",
    "                        \"source_y_pos\",\n",
    "                        \"source_z_pos\",\n",
    "                        \"detector_x_pos\",\n",
    "                        \"detector_y_pos\",\n",
    "                        \"detector_z_pos\",\n",
    "                    ]\n",
    "                )\n",
    "        return source_detector_df\n",
    "\n",
    "    def create_source_detector_df_all(\n",
    "        self,\n",
    "        midpoint_only: bool = False,\n",
    "        channels: Union[List[int], int] = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create a DataFrame with the source and detector information for all channels.\n",
    "        NOTE: Currently only functional for 2D positions and without brain regions.\n",
    "\n",
    "        Args:\n",
    "            midpoint_only (bool): Include only source/detector midpoint coordinate dimensions. Default to False.\n",
    "            channels (Union[List[int], int]): Return only specific channel(s). Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Source and detector information for all channels.\n",
    "        \"\"\"\n",
    "        dim = \"2D\"\n",
    "        measurement_list_df = self.create_measurement_list_df()\n",
    "        source_df = self.create_source_df(dim)\n",
    "        detector_df = self.create_detector_df(dim)\n",
    "        source_merge = pd.merge(measurement_list_df, source_df, on=\"source_index\")\n",
    "        merged_source_detector_df = pd.merge(\n",
    "            source_merge, detector_df, on=\"detector_index\"\n",
    "        )\n",
    "        source_detector_df = merged_source_detector_df.copy()\n",
    "        source_detector_df.insert(\n",
    "            0, \"channel_num\", source_detector_df[\"measurement_list_index\"] - 1\n",
    "        )\n",
    "        source_detector_df.drop(\"source_index_y\", axis=1, inplace=True)\n",
    "        source_detector_df.rename(\n",
    "            columns={\"source_index_x\": \"source_index\"}, inplace=True\n",
    "        )\n",
    "        source_detector_df.sort_values(\"channel_num\", inplace=True)\n",
    "\n",
    "        if isinstance(channels, int):\n",
    "            source_detector_df = source_detector_df[\n",
    "                source_detector_df[\"channel_num\"] == channels\n",
    "            ].copy()\n",
    "        elif isinstance(channels, list):\n",
    "            source_detector_df = source_detector_df[\n",
    "                source_detector_df[\"channel_num\"].isin(channels)\n",
    "            ].copy()\n",
    "\n",
    "        source_detector_df[\n",
    "            [\"midpoint_x_pos\", \"midpoint_y_pos\"]\n",
    "        ] = source_detector_df.apply(\n",
    "            lambda row: self.get_midpoint(\n",
    "                (row[\"source_x_pos\"], row[\"source_y_pos\"]),\n",
    "                (\n",
    "                    row[\"detector_x_pos\"],\n",
    "                    row[\"detector_y_pos\"],\n",
    "                ),\n",
    "            ),\n",
    "            axis=1,\n",
    "            result_type=\"expand\",\n",
    "        )\n",
    "\n",
    "        if midpoint_only:\n",
    "            source_detector_df = source_detector_df.drop(\n",
    "                columns=[\n",
    "                    \"source_x_pos\",\n",
    "                    \"source_y_pos\",\n",
    "                    \"detector_x_pos\",\n",
    "                    \"detector_y_pos\",\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        return source_detector_df\n",
    "\n",
    "    def get_midpoint(\n",
    "        self,\n",
    "        point1: Union[Tuple[float, float], Tuple[float, float, float]],\n",
    "        point2: Union[Tuple[float, float], Tuple[float, float, float]],\n",
    "    ) -> Union[Tuple[float, float], Tuple[float, float, float]]:\n",
    "        \"\"\"\n",
    "        Get the midpoint between two coordinate points (source and detector) in 2D or 3D.\n",
    "\n",
    "        Args:\n",
    "            point1 (Union[Tuple[float, float], Tuple[float, float, float]]): x, y (and optional z) coordinates of the source.\n",
    "            point2 (Union[Tuple[float, float], Tuple[float, float, float]]): x, y (and optional z) coordinates of the detector.\n",
    "\n",
    "        Returns:\n",
    "            Union[Tuple[float, float], Tuple[float, float, float]]: x, y (and optional z) coordinates of the source/detector midpoint.\n",
    "        \"\"\"\n",
    "        if len(point1) != len(point2):\n",
    "            raise ValueError(\"Both points must have the same number of dimensions.\")\n",
    "\n",
    "        x_mid = (point1[0] + point2[0]) / 2\n",
    "        y_mid = (point1[1] + point2[1]) / 2\n",
    "\n",
    "        if len(point1) == 2:  # 2D coordinates\n",
    "            return x_mid, y_mid\n",
    "        else:  # 3D coordinates\n",
    "            z_mid = (point1[2] + point2[2]) / 2\n",
    "            return x_mid, y_mid, z_mid\n",
    "\n",
    "    def MNI_to_region(\n",
    "        self, mni_x: float, mni_y: float, mni_z: float, print_results: bool = False\n",
    "    ) -> Tuple[float, str, float, str]:\n",
    "        \"\"\"\n",
    "        Convert MNI coordinates to the corresponding Automated Anatomical Labeling (AAL) and\n",
    "        Brodmann area (BA) including the distance from the nearest brain region.\n",
    "        Adapted from https://github.com/yunshiuan/label4MRI.\n",
    "\n",
    "        Args:\n",
    "            mni_x (float): x MNI coordinate.\n",
    "            mni_y (float): y MNI coordinate.\n",
    "            mni_z (float): z MNI coordinate.\n",
    "            print_results (bool): Print the results. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            Tuple[float, str, float, str]: Distance from AAL brain region, AAL brain region,\n",
    "                                           distance from BA brain region, and BA region.\n",
    "        \"\"\"\n",
    "        if hasattr(self.__class__, \"mni_to_region_name\"):\n",
    "            mni_to_region_name = self.mni_to_region_name\n",
    "        else:\n",
    "            # load R script files\n",
    "            with open(\n",
    "                os.path.join(os.getcwd(), \"label4MRI\", \"R\", \"mni_to_region_index.R\"),\n",
    "                \"r\",\n",
    "            ) as file:\n",
    "                mni_to_region_index_code = \"\".join(file.readlines())\n",
    "            with open(\n",
    "                os.path.join(os.getcwd(), \"label4MRI\", \"R\", \"mni_to_region_name.R\"), \"r\"\n",
    "            ) as file:\n",
    "                mni_to_region_name_code = \"\".join(file.readlines())\n",
    "            # evaluate R code\n",
    "            metadata_path = os.path.join(\n",
    "                os.getcwd(), \"label4MRI\", \"data\", \"metadata.RData\"\n",
    "            )\n",
    "            load_rdata = robjects.r[\"load\"]\n",
    "            load_rdata(metadata_path)\n",
    "            robjects.r(mni_to_region_index_code)\n",
    "            robjects.r(mni_to_region_name_code)\n",
    "            # R function as Python callable\n",
    "            mni_to_region_name = robjects.globalenv[\"mni_to_region_name\"]\n",
    "\n",
    "        result = mni_to_region_name(float(mni_x), float(mni_y), float(mni_z))\n",
    "\n",
    "        aal_distance = result.rx2(\"aal.distance\")\n",
    "        aal_label = result.rx2(\"aal.label\")\n",
    "        ba_distance = result.rx2(\"ba.distance\")\n",
    "        ba_label = result.rx2(\"ba.label\")\n",
    "\n",
    "        # convert R vector objects\n",
    "        aal_distance = round(list(aal_distance)[0], 2)\n",
    "        aal_label = list(aal_label)[0]\n",
    "        ba_distance = round(list(ba_distance)[0], 2)\n",
    "        ba_label = list(ba_label)[0]\n",
    "\n",
    "        if print_results:\n",
    "            print(f\"AAL distance: {aal_distance}\")\n",
    "            print(f\"AAL region: {aal_label}\")\n",
    "            print(f\"BA distance: {ba_distance}\")\n",
    "            print(f\"BA region: {ba_label}\")\n",
    "\n",
    "        return aal_distance, aal_label, ba_distance, ba_label\n",
    "\n",
    "    def create_flow_atlas(self, depth: Union[int, float] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create an atlas of Kernel Flow source/detector locations with corresponding brain regions.\n",
    "\n",
    "        Args:\n",
    "            depth (Union[int, float], optional): Depth into the brain. Defaults to None (brain surface).\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame with brain regions for all sources and detectors\n",
    "        \"\"\"\n",
    "        if depth is None:\n",
    "            depth = 0\n",
    "        atlas_df = self.create_source_detector_df(\n",
    "            \"3D\", add_missing=True, brain_regions=True, depth=depth\n",
    "        )\n",
    "        filename = f\"kernel_flow_atlas_depth_{depth}.csv\"\n",
    "        filedir = os.path.join(os.getcwd(), \"processed_data\", \"flow\")\n",
    "        filepath = os.path.join(filedir, filename)\n",
    "        atlas_df.to_csv(filepath, index=False)\n",
    "        return atlas_df\n",
    "\n",
    "    def load_flow_atlas(\n",
    "        self,\n",
    "        depth: Union[int, float] = None,\n",
    "        minimal: bool = False,\n",
    "        channels: Union[List[int], int] = None,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load an atlas of Kernel Flow source/detector locations with corresponding brain regions.\n",
    "\n",
    "        Args:\n",
    "            depth (Union[int, float], optional): Depth into the brain. Defaults to None (brain surface).\n",
    "            minimal (bool): Load a minimal version with just channels and brain regions. Defaults to False (load all data).\n",
    "            channels (Union[List[int], int]): Return only specific channel(s). Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame with brain regions for all sources and detectors\n",
    "        \"\"\"\n",
    "        if depth is None:\n",
    "            depth = 0\n",
    "        filename = f\"kernel_flow_atlas_depth_{depth}.csv\"\n",
    "        filedir = os.path.join(os.getcwd(), \"processed_data\", \"flow\")\n",
    "        filepath = os.path.join(filedir, filename)\n",
    "        atlas_df = pd.read_csv(filepath, dtype={\"channel_num\": \"Int64\"})\n",
    "        if minimal:\n",
    "            atlas_df = atlas_df[\n",
    "                [\n",
    "                    \"channel_num\",\n",
    "                    \"AAL_distance\",\n",
    "                    \"AAL_region\",\n",
    "                    \"BA_distance\",\n",
    "                    \"BA_region\",\n",
    "                ]\n",
    "            ]\n",
    "        if isinstance(channels, int):\n",
    "            atlas_df = atlas_df[atlas_df[\"channel_num\"] == channels].copy()\n",
    "        elif isinstance(channels, list):\n",
    "            atlas_df = atlas_df[atlas_df[\"channel_num\"].isin(channels)].copy()\n",
    "        return atlas_df\n",
    "\n",
    "    def plot_pos(\n",
    "        self,\n",
    "        dim: str,\n",
    "        add_labels: bool = False,\n",
    "        minimal: bool = True,\n",
    "        hemo_type: str = \"HbO\",\n",
    "        add_missing: bool = True,\n",
    "        azim: int = 120,\n",
    "        view: str = None,\n",
    "        channels: Union[List[int], int] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Plot the detector and source 2D or 3D positions.\n",
    "\n",
    "        Args:\n",
    "            dim (str): Position data dimension \"2D\" or \"3D\".\n",
    "            add_labels (bool): Add a channel number label at each source position. Defaults to False.\n",
    "            minimal (bool): Show minimal plot elements. Defaults to False.\n",
    "            hemo_type (str): Hemodynamic type. \"HbO\" or \"HbR\". Defaults to \"HbO\".\n",
    "            add_missing (bool): Add missing detector/source positions. Defaults to True.\n",
    "            azim (int): 3D plot azimuth. Defaults to 120 degrees.\n",
    "            view: 3D plot view. \"Anterior\", \"Posterior\", \"Left\" or \"Right\". Defaults to None.\n",
    "            channels (Union[List[int], int]): Highlight specific channel(s). Defaults to None.\n",
    "        \"\"\"\n",
    "\n",
    "        def _get_highlight_channels(\n",
    "            plot_df: pd.DataFrame, channels: Union[List[int], int]\n",
    "        ) -> pd.DataFrame:\n",
    "            if isinstance(channels, int):\n",
    "                return plot_df[plot_df[\"channel_num\"] == channels]\n",
    "            elif isinstance(channels, list):\n",
    "                return plot_df[plot_df[\"channel_num\"].isin(channels)]\n",
    "\n",
    "        def _add_labels(\n",
    "            plot_df: pd.DataFrame,\n",
    "            dim: int,\n",
    "            opt_type: str = \"source\",\n",
    "            label_x_offset: int = 0,\n",
    "            label_y_offset: int = 0,\n",
    "            label_z_offset: int = 0,\n",
    "        ):\n",
    "            if dim.lower() == \"2d\":\n",
    "                labels = plot_df[\"channel_num\"]\n",
    "                if opt_type == \"source\":\n",
    "                    x_pos = list(plot_df[\"source_x_pos\"])\n",
    "                    y_pos = list(plot_df[\"source_y_pos\"])\n",
    "                elif opt_type == \"detector\":\n",
    "                    x_pos = list(plot_df[\"detector_x_pos\"])\n",
    "                    y_pos = list(plot_df[\"detector_y_pos\"])\n",
    "                for i, label in enumerate(labels):\n",
    "                    try:\n",
    "                        ax.annotate(\n",
    "                            label,\n",
    "                            (x_pos[i] - 0.007, y_pos[i] - 0.007),\n",
    "                            xytext=(label_x_offset, label_y_offset),\n",
    "                            textcoords=\"offset points\",\n",
    "                            fontsize=8,\n",
    "                            ha=\"left\",\n",
    "                            va=\"center\",\n",
    "                            bbox=dict(\n",
    "                                boxstyle=\"round,pad=0.15\",\n",
    "                                edgecolor=\"black\",\n",
    "                                facecolor=\"white\",\n",
    "                                alpha=1,\n",
    "                            ),\n",
    "                            arrowprops=dict(\n",
    "                                arrowstyle=\"-|>\",\n",
    "                                facecolor=\"black\",\n",
    "                                linewidth=2,\n",
    "                                shrinkA=0,\n",
    "                                shrinkB=0,\n",
    "                            ),\n",
    "                        )\n",
    "                    except TypeError:\n",
    "                        ax.annotate(\n",
    "                            \"NaN\",\n",
    "                            (x_pos[i] - 0.007, y_pos[i] - 0.007),\n",
    "                            xytext=(label_x_offset, label_y_offset),\n",
    "                            textcoords=\"offset points\",\n",
    "                            fontsize=8,\n",
    "                            ha=\"left\",\n",
    "                            va=\"center\",\n",
    "                            bbox=dict(\n",
    "                                boxstyle=\"round,pad=0.15\",\n",
    "                                edgecolor=\"black\",\n",
    "                                facecolor=\"white\",\n",
    "                                alpha=1,\n",
    "                            ),\n",
    "                            arrowprops=dict(\n",
    "                                arrowstyle=\"-|>\",\n",
    "                                facecolor=\"black\",\n",
    "                                linewidth=2,\n",
    "                                shrinkA=0,\n",
    "                                shrinkB=0,\n",
    "                            ),\n",
    "                        )\n",
    "            elif dim.lower() == \"3d\":\n",
    "                labels = plot_df[\"channel_num\"]\n",
    "                if opt_type == \"source\":\n",
    "                    x_pos = list(plot_df[\"source_x_pos\"])\n",
    "                    y_pos = list(plot_df[\"source_y_pos\"])\n",
    "                    z_pos = list(plot_df[\"source_z_pos\"])\n",
    "                elif opt_type == \"detector\":\n",
    "                    x_pos = list(plot_df[\"detector_x_pos\"])\n",
    "                    y_pos = list(plot_df[\"detector_y_pos\"])\n",
    "                    z_pos = list(plot_df[\"detector_z_pos\"])\n",
    "                for i, label in enumerate(labels):\n",
    "                    label_x = x_pos[i] + label_x_offset\n",
    "                    label_y = y_pos[i] + label_y_offset\n",
    "                    label_z = z_pos[i] + label_z_offset\n",
    "                    arrow_length = np.array(\n",
    "                        [label_x_offset, label_y_offset, label_z_offset]\n",
    "                    )\n",
    "                    ax.quiver(\n",
    "                        x_pos[i] + arrow_length[0],\n",
    "                        y_pos[i] + arrow_length[1],\n",
    "                        z_pos[i] + arrow_length[2],\n",
    "                        -arrow_length[0],\n",
    "                        -arrow_length[1],\n",
    "                        -arrow_length[2],\n",
    "                        color=\"black\",\n",
    "                        linewidth=1,\n",
    "                        arrow_length_ratio=0.3,\n",
    "                    )\n",
    "                    try:\n",
    "                        ax.text(\n",
    "                            label_x,\n",
    "                            label_y,\n",
    "                            label_z,\n",
    "                            label,\n",
    "                            fontsize=8,\n",
    "                            ha=\"left\",\n",
    "                            va=\"center\",\n",
    "                            bbox=dict(\n",
    "                                boxstyle=\"round,pad=0.15\",\n",
    "                                edgecolor=\"black\",\n",
    "                                facecolor=\"white\",\n",
    "                                alpha=1,\n",
    "                            ),\n",
    "                        )\n",
    "                    except TypeError:\n",
    "                        ax.text(\n",
    "                            label_x,\n",
    "                            label_y,\n",
    "                            label_z,\n",
    "                            \"NaN\",\n",
    "                            fontsize=8,\n",
    "                            ha=\"left\",\n",
    "                            va=\"center\",\n",
    "                            bbox=dict(\n",
    "                                boxstyle=\"round,pad=0.15\",\n",
    "                                edgecolor=\"black\",\n",
    "                                facecolor=\"white\",\n",
    "                                alpha=1,\n",
    "                            ),\n",
    "                        )\n",
    "\n",
    "        source_detector_df = self.create_source_detector_df(dim, add_missing)\n",
    "        source_detector_hemo = source_detector_df[\n",
    "            source_detector_df[\"data_type_index\"] == hemo_type\n",
    "        ]\n",
    "        uni_source_label_df = source_detector_hemo.drop_duplicates(\n",
    "            subset=\"source_index\"\n",
    "        )\n",
    "\n",
    "        if dim.lower() == \"2d\":\n",
    "            x_detector = list(source_detector_hemo[\"detector_x_pos\"])\n",
    "            y_detector = list(source_detector_hemo[\"detector_y_pos\"])\n",
    "            x_source = list(uni_source_label_df[\"source_x_pos\"])\n",
    "            y_source = list(uni_source_label_df[\"source_y_pos\"])\n",
    "            fig = plt.figure(figsize=(8, 8))\n",
    "            ax = fig.add_subplot(111)\n",
    "            ax.scatter(x_detector, y_detector, s=40)\n",
    "            ax.scatter(x_source, y_source, s=70)\n",
    "            if add_labels and not channels:\n",
    "                label_x_offset = 10\n",
    "                label_y_offset = 15\n",
    "                _add_labels(\n",
    "                    uni_source_label_df, dim, \"source\", label_x_offset, label_y_offset\n",
    "                )\n",
    "            if minimal:\n",
    "                ax.set_title(\"Anterior\", fontweight=\"bold\", fontsize=14)\n",
    "                ax.set_xticklabels([])\n",
    "                ax.set_yticklabels([])\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "                for spine in ax.spines.values():\n",
    "                    spine.set_visible(False)\n",
    "                ax.text(\n",
    "                    0.5,\n",
    "                    -0.06,\n",
    "                    \"Posterior\",\n",
    "                    fontweight=\"bold\",\n",
    "                    fontsize=14,\n",
    "                    ha=\"center\",\n",
    "                    va=\"bottom\",\n",
    "                    transform=ax.transAxes,\n",
    "                )\n",
    "                ax.text(\n",
    "                    -0.02,\n",
    "                    0.5,\n",
    "                    \"Left\",\n",
    "                    fontsize=14,\n",
    "                    fontweight=\"bold\",\n",
    "                    rotation=90,\n",
    "                    va=\"center\",\n",
    "                    ha=\"center\",\n",
    "                    transform=ax.transAxes,\n",
    "                )\n",
    "                ax.text(\n",
    "                    1.02,\n",
    "                    0.5,\n",
    "                    \"Right\",\n",
    "                    fontsize=14,\n",
    "                    fontweight=\"bold\",\n",
    "                    rotation=90,\n",
    "                    va=\"center\",\n",
    "                    ha=\"center\",\n",
    "                    transform=ax.transAxes,\n",
    "                )\n",
    "            else:\n",
    "                ax.set_title(\"Detector/Source 2D Plot\")\n",
    "                ax.set_xlabel(\"X-Position (mm)\")\n",
    "                ax.set_ylabel(\"Y-Position (mm)\")\n",
    "                ax.legend([\"Detector\", \"Source\"])\n",
    "            if channels:\n",
    "                label_x_offset = 12\n",
    "                label_y_offset = 12\n",
    "                highlight_rows = _get_highlight_channels(source_detector_hemo, channels)\n",
    "                _add_labels(\n",
    "                    highlight_rows, dim, \"detector\", label_x_offset, label_y_offset\n",
    "                )\n",
    "\n",
    "        elif dim.lower() == \"3d\":\n",
    "            fig = plt.figure(figsize=(8, 8))\n",
    "            ax = fig.add_subplot(111, projection=\"3d\", computed_zorder=False)\n",
    "            label_x_offset = 10\n",
    "            label_y_offset = 10\n",
    "            label_z_offset = 10\n",
    "            if not view:\n",
    "                x_detector = list(source_detector_hemo[\"detector_x_pos\"])\n",
    "                y_detector = list(source_detector_hemo[\"detector_y_pos\"])\n",
    "                z_detector = list(source_detector_hemo[\"detector_z_pos\"])\n",
    "                x_source = list(uni_source_label_df[\"source_x_pos\"])\n",
    "                y_source = list(uni_source_label_df[\"source_y_pos\"])\n",
    "                z_source = list(uni_source_label_df[\"source_z_pos\"])\n",
    "                ax.scatter(x_detector, y_detector, z_detector, s=30)\n",
    "                ax.scatter(x_source, y_source, z_source, s=55)\n",
    "                ax.view_init(azim=azim)\n",
    "                if add_labels and not channels:\n",
    "                    _add_labels(\n",
    "                        uni_source_label_df,\n",
    "                        dim,\n",
    "                        \"source\",\n",
    "                        label_x_offset,\n",
    "                        label_y_offset,\n",
    "                        label_z_offset,\n",
    "                    )\n",
    "                if channels:\n",
    "                    highlight_rows = _get_highlight_channels(\n",
    "                        source_detector_hemo, channels\n",
    "                    )\n",
    "                    _add_labels(\n",
    "                        highlight_rows,\n",
    "                        dim,\n",
    "                        \"detector\",\n",
    "                        label_x_offset,\n",
    "                        label_y_offset,\n",
    "                        label_z_offset,\n",
    "                    )\n",
    "            else:\n",
    "                views = {\n",
    "                    \"right\": 0,\n",
    "                    \"left\": 180,\n",
    "                    \"anterior\": 90,\n",
    "                    \"posterior\": 270,\n",
    "                }\n",
    "                ax.view_init(elev=0, azim=views[view])\n",
    "                if view == \"right\":\n",
    "                    source_plot_df = uni_source_label_df[\n",
    "                        uni_source_label_df[\"source_x_pos\"] >= 0\n",
    "                    ]\n",
    "                    detector_plot_df = source_detector_hemo[\n",
    "                        source_detector_hemo[\"detector_x_pos\"] >= 0\n",
    "                    ]\n",
    "                    ax.set_title(\"Right View\", fontweight=\"bold\", fontsize=18, y=0.85)\n",
    "                    fig.text(\n",
    "                        0.8,\n",
    "                        0.5,\n",
    "                        \"Anterior\",\n",
    "                        fontweight=\"bold\",\n",
    "                        fontsize=14,\n",
    "                        rotation=90,\n",
    "                        va=\"center\",\n",
    "                        ha=\"center\",\n",
    "                    )\n",
    "                elif view == \"left\":\n",
    "                    source_plot_df = uni_source_label_df[\n",
    "                        uni_source_label_df[\"source_x_pos\"] <= 0\n",
    "                    ]\n",
    "                    detector_plot_df = source_detector_hemo[\n",
    "                        source_detector_hemo[\"detector_x_pos\"] <= 0\n",
    "                    ]\n",
    "                    ax.set_title(\"Left View\", fontweight=\"bold\", fontsize=18, y=0.85)\n",
    "                    fig.text(\n",
    "                        0.25,\n",
    "                        0.5,\n",
    "                        \"Anterior\",\n",
    "                        fontweight=\"bold\",\n",
    "                        fontsize=14,\n",
    "                        rotation=90,\n",
    "                        va=\"center\",\n",
    "                        ha=\"center\",\n",
    "                    )\n",
    "                elif view == \"anterior\":\n",
    "                    source_plot_df = uni_source_label_df[\n",
    "                        uni_source_label_df[\"source_y_pos\"] > 0\n",
    "                    ]\n",
    "                    detector_plot_df = source_detector_hemo[\n",
    "                        source_detector_hemo[\"detector_y_pos\"] > 0\n",
    "                    ]\n",
    "                    ax.set_title(\n",
    "                        \"Anterior View\", fontweight=\"bold\", fontsize=18, y=0.85\n",
    "                    )\n",
    "                    fig.text(\n",
    "                        0.8,\n",
    "                        0.5,\n",
    "                        \"Left\",\n",
    "                        fontweight=\"bold\",\n",
    "                        fontsize=14,\n",
    "                        rotation=90,\n",
    "                        va=\"center\",\n",
    "                        ha=\"center\",\n",
    "                    )\n",
    "                elif view == \"posterior\":\n",
    "                    source_plot_df = uni_source_label_df[\n",
    "                        uni_source_label_df[\"source_y_pos\"] <= 0\n",
    "                    ]\n",
    "                    detector_plot_df = source_detector_hemo[\n",
    "                        source_detector_hemo[\"detector_y_pos\"] <= 0\n",
    "                    ]\n",
    "                    ax.set_title(\n",
    "                        \"Posterior View\", fontweight=\"bold\", fontsize=18, y=0.85\n",
    "                    )\n",
    "                    fig.text(\n",
    "                        0.25,\n",
    "                        0.5,\n",
    "                        \"Left\",\n",
    "                        fontweight=\"bold\",\n",
    "                        fontsize=14,\n",
    "                        rotation=90,\n",
    "                        va=\"center\",\n",
    "                        ha=\"center\",\n",
    "                    )\n",
    "\n",
    "                if add_labels and not channels:\n",
    "                    try:\n",
    "                        _add_labels(\n",
    "                            source_plot_df,\n",
    "                            dim,\n",
    "                            \"source\",\n",
    "                            label_x_offset,\n",
    "                            label_y_offset,\n",
    "                            label_z_offset,\n",
    "                        )\n",
    "                    except NameError:\n",
    "                        _add_labels(\n",
    "                            source_plot_df,\n",
    "                            dim,\n",
    "                            \"source\",\n",
    "                            label_x_offset,\n",
    "                            label_y_offset,\n",
    "                            label_z_offset,\n",
    "                        )\n",
    "                ax.scatter(\n",
    "                    detector_plot_df[\"detector_x_pos\"],\n",
    "                    detector_plot_df[\"detector_y_pos\"],\n",
    "                    detector_plot_df[\"detector_z_pos\"],\n",
    "                    s=30,\n",
    "                    alpha=1,\n",
    "                    zorder=2,\n",
    "                )\n",
    "                ax.scatter(\n",
    "                    source_plot_df[\"source_x_pos\"],\n",
    "                    source_plot_df[\"source_y_pos\"],\n",
    "                    source_plot_df[\"source_z_pos\"],\n",
    "                    s=55,\n",
    "                    alpha=1,\n",
    "                    zorder=1,\n",
    "                )\n",
    "                if channels:\n",
    "                    highlight_rows = _get_highlight_channels(detector_plot_df, channels)\n",
    "                    _add_labels(\n",
    "                        highlight_rows,\n",
    "                        dim,\n",
    "                        \"detector\",\n",
    "                        label_x_offset,\n",
    "                        label_y_offset,\n",
    "                        label_z_offset,\n",
    "                    )\n",
    "\n",
    "            if minimal:\n",
    "                ax.patch.set_alpha(0.0)\n",
    "                ax.xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "                ax.xaxis.line.set_color(\"none\")\n",
    "                ax.yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "                ax.yaxis.line.set_color(\"none\")\n",
    "                ax.zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "                ax.zaxis.line.set_color(\"none\")\n",
    "                ax.set_xticklabels([])\n",
    "                ax.set_yticklabels([])\n",
    "                ax.set_zticklabels([])\n",
    "                if not view:\n",
    "                    if azim > 180:\n",
    "                        ax.set_xlabel(\"Posterior\", fontweight=\"bold\", fontsize=14)\n",
    "                    else:\n",
    "                        ax.set_xlabel(\"Anterior\", fontweight=\"bold\", fontsize=14)\n",
    "                    if azim >= 270 or (azim >= 0 and azim <= 90):\n",
    "                        ax.set_ylabel(\"Right\", fontweight=\"bold\", fontsize=14)\n",
    "                    else:\n",
    "                        ax.set_ylabel(\"Left\", fontweight=\"bold\", fontsize=14)\n",
    "            else:\n",
    "                ax.set_title(\"Detector/Source 3D Plot\")\n",
    "                ax.set_xlabel(\"X-Position (mm)\")\n",
    "                ax.set_ylabel(\"Y-Position (mm)\")\n",
    "                ax.set_zlabel(\"Z-Position (mm)\")\n",
    "                ax.legend([\"Detector\", \"Source\"])\n",
    "\n",
    "\n",
    "class Participant_Flow:\n",
    "    \"\"\"\n",
    "    This class contains functions, data structures, and info necessary for\n",
    "    processing Kernel Flow data from the experiments.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, par_num: int = None):\n",
    "        if not par_num:\n",
    "            par_num = 1\n",
    "        self.data_fun = Data_Functions()\n",
    "        self.adj_ts_markers = True\n",
    "        self.par_behav = Participant_Behav(par_num, self.adj_ts_markers)\n",
    "        self.par_num, self.par_ID = self.data_fun.process_par(par_num)\n",
    "        self.flow_raw_data_dir = os.path.join(\n",
    "            self.par_behav.raw_data_dir, self.par_ID, \"kernel_data\"\n",
    "        )\n",
    "        self.flow_processed_data_dir = os.path.join(\n",
    "            os.getcwd(), \"processed_data\", \"flow\"\n",
    "        )\n",
    "        self.flow = self.load_flow_session(\"1001\", wrapper=True)\n",
    "        self.flow_session_dict = self.create_flow_session_dict(wrapper=True)\n",
    "        self.time_offset_dict = self.create_time_offset_dict()\n",
    "        self.plot_color_dict = {\n",
    "            0: \"purple\",\n",
    "            1: \"orange\",\n",
    "            2: \"green\",\n",
    "            3: \"yellow\",\n",
    "            4: \"pink\",\n",
    "            5: \"skyblue\",\n",
    "        }\n",
    "\n",
    "    def calc_time_offset(self, exp_name: str) -> float:\n",
    "        \"\"\"\n",
    "        Calculate the time offset (in seconds) between the behavioral and Kernel Flow data\n",
    "        files. Number of seconds that the Kernel Flow data is ahead of the behavioral data.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "\n",
    "        Returns:\n",
    "            float: Time offset (in seconds).\n",
    "        \"\"\"\n",
    "        exp = self.par_behav.get_exp(exp_name)\n",
    "        exp_start_ts = exp.start_ts\n",
    "        marker_sent_time = float(exp.marker_data[\"start_marker\"][\"sent_time\"])\n",
    "        session = self.par_behav.get_key_from_value(\n",
    "            self.par_behav.session_dict, exp_name\n",
    "        )\n",
    "        marker_df = self.create_abs_marker_df(session)\n",
    "        row = marker_df.loc[marker_df[\"Marker\"].str.startswith(exp_name)].reset_index()\n",
    "        if (\n",
    "            exp_name == \"go_no_go\"\n",
    "        ):  # Go/No-go experiment is missing start timestamp marker\n",
    "            try:\n",
    "                kernel_start_ts = row.loc[0, \"Start timestamp\"]\n",
    "                time_offset = kernel_start_ts - (exp_start_ts + marker_sent_time)\n",
    "            except:\n",
    "                time_offset = \"NaN\"\n",
    "        else:\n",
    "            kernel_start_ts = row.loc[0, \"Start timestamp\"]\n",
    "            time_offset = kernel_start_ts - (exp_start_ts + marker_sent_time)\n",
    "        return float(time_offset)\n",
    "\n",
    "    def create_time_offset_dict(self) -> dict:\n",
    "        \"\"\"\n",
    "        Create a dictionary containing the time offset (in seconds) for each experiment.\n",
    "\n",
    "        Returns:\n",
    "            dict: Time offset dictionary.\n",
    "        \"\"\"\n",
    "        time_offset_dict = {}\n",
    "        for exp_name in self.par_behav.exp_order:\n",
    "            if (\n",
    "                exp_name == \"go_no_go\"\n",
    "            ):  # Go/No-go experiment is missing start timestamp marker\n",
    "                if np.isnan(self.calc_time_offset(exp_name)):\n",
    "                    session = self.par_behav.get_key_from_value(\n",
    "                        self.par_behav.session_dict, exp_name\n",
    "                    )\n",
    "                    session_exp_names = self.par_behav.session_dict[session]\n",
    "                    other_exp_names = [\n",
    "                        temp_exp_name\n",
    "                        for temp_exp_name in session_exp_names\n",
    "                        if temp_exp_name != \"go_no_go\"\n",
    "                    ]\n",
    "                    other_exp_time_offsets = []\n",
    "                    for temp_exp_name in other_exp_names:\n",
    "                        time_offset = self.calc_time_offset(temp_exp_name)\n",
    "                        other_exp_time_offsets.append(time_offset)\n",
    "                    avg_time_offset = np.mean(other_exp_time_offsets)\n",
    "                    time_offset_dict[exp_name] = avg_time_offset\n",
    "            else:\n",
    "                time_offset_dict[exp_name] = self.calc_time_offset(exp_name)\n",
    "        for session, exp_list in self.par_behav.session_dict.items():\n",
    "            session_offset = np.mean(\n",
    "                [time_offset_dict[exp_name] for exp_name in exp_list]\n",
    "            )\n",
    "            time_offset_dict[session] = session_offset\n",
    "        return time_offset_dict\n",
    "\n",
    "    def get_time_offset(self, exp_name: str) -> float:\n",
    "        \"\"\"\n",
    "        Get the time offset for an experiment.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Experiment name.\n",
    "\n",
    "        Returns:\n",
    "            float: Time offset (in seconds).\n",
    "        \"\"\"\n",
    "        return self.time_offset_dict[exp_name]\n",
    "\n",
    "    def offset_time_array(self, exp_name: str, time_array: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Offset a Kernel Flow datetime array for an experiment by the time-offset.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "            time_array (np.ndarray): Datetime array.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Time-offset datetime array.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            time_offset = self.get_time_offset(exp_name)\n",
    "        except KeyError:  # if experiment start time is missing, use avg of other session experiments\n",
    "            time_offset_list = []\n",
    "            for exp_name in self.par_behav.exp_order:\n",
    "                try:\n",
    "                    time_offset = self.get_time_offset(exp_name)\n",
    "                    time_offset_list.append(time_offset)\n",
    "                except KeyError:\n",
    "                    pass\n",
    "            time_offset = mean(time_offset_list)\n",
    "        time_offset_dt = datetime.timedelta(seconds=time_offset)\n",
    "        time_abs_dt_offset = time_array - time_offset_dt\n",
    "        return time_abs_dt_offset\n",
    "\n",
    "    def load_flow_session(\n",
    "        self, session: list[str | int], wrapper: bool = False\n",
    "    ) -> snirf.Snirf:\n",
    "        \"\"\"\n",
    "        Load Kernel Flow data for an experiment session.\n",
    "\n",
    "        Args:\n",
    "            session list[str | int]: Experiment session.\n",
    "            wrapper (bool, optional) Option to return Process_Flow-wrapped SNIRF file.\n",
    "                                     Defaults to False.\n",
    "\n",
    "        Raises:\n",
    "            Exception: Invalid session number argument.\n",
    "\n",
    "        Returns:\n",
    "            snirf.Snirf: SNIRF file object.\n",
    "            -or-\n",
    "            Process_Flow object for each experiment session.\n",
    "        \"\"\"\n",
    "        if isinstance(session, str):\n",
    "            if \"session\" not in session:\n",
    "                session = f\"session_{session}\"\n",
    "        elif isinstance(session, int):\n",
    "            session = f\"session_{session}\"\n",
    "        try:\n",
    "            session_dir = os.path.join(self.flow_raw_data_dir, session)\n",
    "            filename = os.listdir(session_dir)[0]\n",
    "            filepath = os.path.join(session_dir, filename)\n",
    "            if wrapper:\n",
    "                return Process_Flow(filepath)\n",
    "            else:\n",
    "                return Process_Flow(filepath).snirf_file\n",
    "        except:\n",
    "            print(\"Invalid session number.\")\n",
    "            raise\n",
    "\n",
    "    def load_flow_exp(self, exp_name: str, filter_type: str = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load Kernel Flow data for the time frame of a specified experiment.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "            filter_type (str): Filter to apply to the data. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Kernel Flow data for an experiment.\n",
    "        \"\"\"\n",
    "        session = self.par_behav.get_key_from_value(\n",
    "            self.par_behav.session_dict, exp_name\n",
    "        )\n",
    "        flow_session = self.load_flow_session(session, wrapper=True)\n",
    "\n",
    "        start_dt = self.par_behav.get_start_dt(exp_name, self.adj_ts_markers)\n",
    "        end_dt = self.par_behav.get_end_dt(exp_name, self.adj_ts_markers)\n",
    "        time_abs_dt = flow_session.get_time_abs(\"datetime\")\n",
    "        time_abs_dt_offset = self.offset_time_array(exp_name, time_abs_dt)\n",
    "        start_idx = self.par_behav.get_start_index_dt(time_abs_dt_offset, start_dt)\n",
    "        end_idx = self.par_behav.get_end_index_dt(time_abs_dt_offset, end_dt)\n",
    "\n",
    "        flow_data = flow_session.get_data(\"dataframe\")\n",
    "        if filter_type.lower() == \"lowpass\":\n",
    "            flow_data = flow_data.apply(lambda x: self.lowpass_filter(x), axis=0)\n",
    "        elif filter_type.lower() == \"bandpass\":\n",
    "            flow_data = flow_data.apply(lambda x: self.bandpass_filter(x), axis=0)\n",
    "        flow_data.insert(0, \"datetime\", time_abs_dt_offset)\n",
    "        return flow_data.iloc[start_idx:end_idx, :]\n",
    "\n",
    "    def create_flow_session_dict(self, wrapper: bool = False) -> dict:\n",
    "        \"\"\"\n",
    "        Create a dictionary of Kernel Flow data for all experiment sessions.\n",
    "\n",
    "        wrapper (bool, optional) Option to return Process_Flow-wrapped SNIRF file.\n",
    "                                 Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            dict: Kernel Flow data for all experiment sessions.\n",
    "                keys:\n",
    "                    \"session_1001\", \"session_1002\", \"session_1003\"\n",
    "                values:\n",
    "                    SNIRF file object for each experiment session\n",
    "                    -or-\n",
    "                    Process_Flow object for each experiment session\n",
    "        \"\"\"\n",
    "        flow_session_dict = {}\n",
    "        for session in self.par_behav.session_dict.keys():\n",
    "            flow_session_dict[session] = self.load_flow_session(session, wrapper)\n",
    "        return flow_session_dict\n",
    "\n",
    "    def create_abs_marker_df(self, session: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Convert the \"stim\" marker DataFrame into absolute time.\n",
    "\n",
    "        Args:\n",
    "            session (str): Experiment session.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Marker \"stim\" data in absolute time.\n",
    "        \"\"\"\n",
    "        marker_df = self.flow_session_dict[session].get_marker_df()\n",
    "        time_origin_ts = self.flow_session_dict[session].get_time_origin(\"timestamp\")\n",
    "        marker_df[\"Timestamp\"] = marker_df[\"Timestamp\"] + time_origin_ts\n",
    "        marker_df.rename({\"Timestamp\": \"Start timestamp\"}, axis=1, inplace=True)\n",
    "\n",
    "        for idx, row in marker_df.iterrows():\n",
    "            end_ts = row[\"Start timestamp\"] + row[\"Duration\"]\n",
    "            marker_df.at[idx, \"End timestamp\"] = end_ts\n",
    "            exp_num = int(row[\"Experiment\"])\n",
    "            exp_name = self.par_behav.marker_dict[exp_num]\n",
    "            marker_df.at[idx, \"Experiment\"] = exp_name\n",
    "\n",
    "        marker_df.rename({\"Experiment\": \"Marker\"}, axis=1, inplace=True)\n",
    "        marker_df.drop([\"Value\"], axis=1, inplace=True)\n",
    "        marker_df = marker_df[\n",
    "            [\"Marker\", \"Start timestamp\", \"Duration\", \"End timestamp\"]\n",
    "        ]\n",
    "        return marker_df\n",
    "\n",
    "    def create_exp_stim_response_dict(\n",
    "        self, exp_name: str, filter_type: str = None\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Create a dictionary that contains the processed Kernel Flow data in response\n",
    "        to a stimulus. It is organized by block (keys) and for each block, the value is\n",
    "        a list of Pandas Series. Each Series is normalized, averaged, Kernel Flow data\n",
    "        during a presented stimulus duration for each channel. Each block is baselined\n",
    "        to the first 5 seconds, and the stim response is averaged over the stimulus\n",
    "        presentation duration.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "            filter_type (str): Filter to apply to the data. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            dict:\n",
    "                keys:\n",
    "                    \"block 1\", \"block 2\", ... \"block N\"\n",
    "                values:\n",
    "                    dicts:\n",
    "                        keys:\n",
    "                            \"trial 1\", \"trial 2\", ... \"trial N\"\n",
    "                        values:\n",
    "                            lists of averaged, normalized Kernel Flow data Series for each\n",
    "                            channel during the stimulus duration\n",
    "        \"\"\"\n",
    "        exp_results = load_results(\n",
    "            self.par_behav.processed_data_dir, exp_name, self.par_behav.par_num\n",
    "        )\n",
    "        flow_exp = self.load_flow_exp(exp_name, filter_type)\n",
    "        session = self.par_behav.get_key_from_value(\n",
    "            self.par_behav.session_dict, exp_name\n",
    "        )\n",
    "        ts_list = self.flow_session_dict[session].get_time_abs(\"timestamp\")\n",
    "        exp_time_offset = self.time_offset_dict[exp_name]\n",
    "        exp_by_block = self.par_behav.by_block_ts_dict[exp_name]\n",
    "\n",
    "        blocks = list(exp_results[\"block\"].unique())\n",
    "        exp_stim_resp_dict = {\n",
    "            block: {} for block in blocks\n",
    "        }  # initialize with unique blocks\n",
    "        processed_blocks = []\n",
    "\n",
    "        if exp_name == \"king_devick\":  # normalize all blocks to the first block\n",
    "            (first_block_start_ts, first_block_end_ts) = next(\n",
    "                iter(exp_by_block.keys())\n",
    "            )  # start/end of first block\n",
    "            first_block_start_ts_offset = first_block_start_ts + exp_time_offset\n",
    "            first_block_start_idx, _ = self.data_fun.find_closest_ts(\n",
    "                first_block_start_ts_offset, ts_list\n",
    "            )\n",
    "            first_block_end_ts_offset = first_block_end_ts + exp_time_offset\n",
    "            first_block_end_idx, _ = self.data_fun.find_closest_ts(\n",
    "                first_block_end_ts_offset, ts_list\n",
    "            )\n",
    "            baseline_rows = flow_exp.loc[\n",
    "                first_block_start_idx : first_block_start_idx + 35, 0:\n",
    "            ]  # first 5 seconds of a block\n",
    "            baseline = pd.DataFrame(baseline_rows.mean()).T\n",
    "\n",
    "            for (\n",
    "                block_start_ts,\n",
    "                block_end_ts,\n",
    "            ) in exp_by_block.keys():  # for each block in the experiment\n",
    "                block_start_ts_offset = block_start_ts + exp_time_offset\n",
    "                block_start_idx, _ = self.data_fun.find_closest_ts(\n",
    "                    block_start_ts_offset, ts_list\n",
    "                )\n",
    "                block_end_ts_offset = block_end_ts + exp_time_offset\n",
    "                block_end_idx, _ = self.data_fun.find_closest_ts(\n",
    "                    block_end_ts_offset, ts_list\n",
    "                )\n",
    "                block_rows = flow_exp.loc[\n",
    "                    block_start_idx:block_end_idx, 0:\n",
    "                ]  # rows from block start to end\n",
    "\n",
    "                baseline_df = pd.concat(\n",
    "                    [baseline] * block_rows.shape[0], ignore_index=True\n",
    "                )\n",
    "                baseline_df = baseline_df.set_index(\n",
    "                    pd.Index(range(block_start_idx, block_start_idx + len(baseline_df)))\n",
    "                )\n",
    "\n",
    "                block_rows_norm = block_rows.subtract(\n",
    "                    baseline_df, fill_value=0\n",
    "                )  # normalize the block rows\n",
    "                processed_blocks.append(block_rows_norm)\n",
    "        else:  # normalize each block to the start of the block\n",
    "            for (\n",
    "                block_start_ts,\n",
    "                block_end_ts,\n",
    "            ) in exp_by_block.keys():  # for each block in the experiment\n",
    "                block_start_ts_offset = block_start_ts + exp_time_offset\n",
    "                block_start_idx, _ = self.data_fun.find_closest_ts(\n",
    "                    block_start_ts_offset, ts_list\n",
    "                )\n",
    "                block_end_ts_offset = block_end_ts + exp_time_offset\n",
    "                block_end_idx, _ = self.data_fun.find_closest_ts(\n",
    "                    block_end_ts_offset, ts_list\n",
    "                )\n",
    "                block_rows = flow_exp.loc[\n",
    "                    block_start_idx:block_end_idx, 0:\n",
    "                ]  # rows from block start to end\n",
    "\n",
    "                baseline_rows = flow_exp.loc[\n",
    "                    block_start_idx : block_start_idx + 35, 0:\n",
    "                ]  # first 5 seconds of a block\n",
    "                baseline = pd.DataFrame(baseline_rows.mean()).T\n",
    "                baseline_df = pd.concat(\n",
    "                    [baseline] * block_rows.shape[0], ignore_index=True\n",
    "                )\n",
    "                baseline_df = baseline_df.set_index(\n",
    "                    pd.Index(range(block_start_idx, block_start_idx + len(baseline_df)))\n",
    "                )\n",
    "\n",
    "                block_rows_norm = block_rows.subtract(\n",
    "                    baseline_df, fill_value=0\n",
    "                )  # normalize the block rows\n",
    "                processed_blocks.append(block_rows_norm)\n",
    "\n",
    "        processed_block_df = pd.concat(\n",
    "            processed_blocks\n",
    "        )  # all processed blocks for an experiment\n",
    "\n",
    "        for _, row in exp_results.iterrows():\n",
    "            stim_start_ts = row[\"stim_start\"]\n",
    "            stim_start_ts_offset = stim_start_ts + exp_time_offset\n",
    "            start_idx, _ = self.data_fun.find_closest_ts(stim_start_ts_offset, ts_list)\n",
    "            stim_end_ts = row[\"stim_end\"]\n",
    "            stim_end_ts_offset = stim_end_ts + exp_time_offset\n",
    "            end_idx, _ = self.data_fun.find_closest_ts(stim_end_ts_offset, ts_list)\n",
    "\n",
    "            stim_rows = processed_block_df.loc[start_idx:end_idx, 0:]\n",
    "            avg_stim_rows = stim_rows.mean()  # all channels for a stim\n",
    "\n",
    "            block = row[\"block\"]\n",
    "            trial = row[\"trial\"]\n",
    "\n",
    "            if trial not in exp_stim_resp_dict[block].keys():\n",
    "                exp_stim_resp_dict[block][trial] = []\n",
    "            exp_stim_resp_dict[block][trial].append(\n",
    "                avg_stim_rows\n",
    "            )  # add to a block in dict\n",
    "\n",
    "        return exp_stim_resp_dict\n",
    "\n",
    "    def _compute_hemo_type_df(\n",
    "        self, hemo_type: str, df_in: pd.DataFrame\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create HbTot and HbDiff DataFrames.\n",
    "\n",
    "        Args:\n",
    "            hemo_type (str): \"HbTot\" or \"HbDiff\".\n",
    "            df_in (pd.DataFrame): DataFrame to create hemodynamic types for.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: HbTot or HbDiff DataFrame.\n",
    "        \"\"\"\n",
    "        HbO_df = df_in.iloc[:, np.r_[0, 1, 2 : len(df_in.columns) : 2]]\n",
    "        HbO_data_cols = HbO_df.iloc[:, 2:]\n",
    "        HbR_df = df_in.iloc[:, np.r_[0, 1, 3 : len(df_in.columns) : 2]]\n",
    "        HbR_data_cols = HbR_df.iloc[:, 2:]\n",
    "        cols_dict = {}\n",
    "        for i, col_name in enumerate(HbO_data_cols.columns):\n",
    "            if hemo_type.lower() == \"hbtot\":\n",
    "                cols_dict[col_name] = (\n",
    "                    HbO_data_cols.iloc[:, i] + HbR_data_cols.iloc[:, i]\n",
    "                )\n",
    "            elif hemo_type.lower() == \"hbdiff\":\n",
    "                cols_dict[col_name] = (\n",
    "                    HbO_data_cols.iloc[:, i] - HbR_data_cols.iloc[:, i]\n",
    "                )\n",
    "        df_out = pd.DataFrame(cols_dict)\n",
    "        df_out.insert(0, \"block\", HbO_df[\"block\"])\n",
    "        df_out.insert(0, \"participant\", HbO_df[\"participant\"])\n",
    "        return df_out\n",
    "\n",
    "    def create_exp_stim_response_df(\n",
    "        self, exp_name: str, hemo_type: str = None, filter_type: str = None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create a DataFrame that contains the processed Kernel Flow data in response\n",
    "        to each stimulus in an experiment. Each channel is normalized and averaged.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "            hemo_type (str, optional): \"HbO\", \"HbR\", \"HbTot\", or \"HbDiff\" channels.\n",
    "                                       Defaults to None (all channels).\n",
    "            filter_type (str): Filter to apply to the data. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Processed Kernel Flow data.\n",
    "        \"\"\"\n",
    "\n",
    "        def _split_col(row: pd.Series) -> pd.Series:\n",
    "            \"\"\"\n",
    "            Split a column containing an array into separate columns for each\n",
    "            element in the array.\n",
    "\n",
    "            Args:\n",
    "                row (pd.Series): DataFrame row.\n",
    "\n",
    "            Returns:\n",
    "                pd.Series: DataFrame row with split column.\n",
    "            \"\"\"\n",
    "            arr = row[\"channels\"]\n",
    "            num_elements = len(arr)\n",
    "            col_names = [i for i in range(num_elements)]\n",
    "            return pd.Series(arr, index=col_names)\n",
    "\n",
    "        exp_baseline_avg_dict = self.create_exp_stim_response_dict(\n",
    "            exp_name, filter_type\n",
    "        )\n",
    "        rows = []\n",
    "        for block, block_data in sorted(exp_baseline_avg_dict.items()):\n",
    "            for trial, stim_resp_data in block_data.items():\n",
    "                trial_avg = np.mean(stim_resp_data, axis=0)\n",
    "                row = {\n",
    "                    \"participant\": self.par_num,\n",
    "                    \"block\": block,\n",
    "                    \"channels\": trial_avg,\n",
    "                }\n",
    "                rows.append(row)\n",
    "\n",
    "        stim_resp_df = pd.DataFrame(rows)\n",
    "        channel_cols = stim_resp_df.apply(_split_col, axis=1)\n",
    "        stim_resp_df = pd.concat(\n",
    "            [stim_resp_df, channel_cols], axis=1\n",
    "        )  # merge with original DataFrame\n",
    "        stim_resp_df = stim_resp_df.drop(\n",
    "            \"channels\", axis=1\n",
    "        )  # drop the original \"channels\" column\n",
    "        if hemo_type:\n",
    "            if hemo_type.lower() == \"hbo\":  # HbO\n",
    "                HbO_df = stim_resp_df.iloc[\n",
    "                    :, np.r_[0, 1, 2 : len(stim_resp_df.columns) : 2]\n",
    "                ]\n",
    "                return HbO_df\n",
    "            elif hemo_type.lower() == \"hbr\":  # HbR\n",
    "                HbR_df = stim_resp_df.iloc[\n",
    "                    :, np.r_[0, 1, 3 : len(stim_resp_df.columns) : 2]\n",
    "                ]\n",
    "                return HbR_df\n",
    "            elif hemo_type.lower() == \"hbtot\":  # HbTot\n",
    "                HbTot_df = self._compute_hemo_type_df(hemo_type, df_in=stim_resp_df)\n",
    "                return HbTot_df\n",
    "            elif hemo_type.lower() == \"hbdiff\":  # HbDiff\n",
    "                HbDiff_df = self._compute_hemo_type_df(hemo_type, df_in=stim_resp_df)\n",
    "                return HbDiff_df\n",
    "        else:\n",
    "            return stim_resp_df\n",
    "\n",
    "    def create_inter_module_exp_results_df(\n",
    "        self, exp_name: str, hemo_type: str = None, filter_type: str = None\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create a DataFrame with the inter-module channels for an experiment.\n",
    "        This DataFrame can include both HbO and HbR channels in alternating columns\n",
    "        or just \"HbO\", \"HbR\", \"HbTot\", or \"HbDiff\" channels.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "            hemo_type (str, optional): \"HbO\", \"HbR\", \"HbTot\", or \"HbDiff\" channels.\n",
    "                                 Defaults to None (all inter-module channels).\n",
    "            filter_type (str): Filter to apply to the data. Defaults to None.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Inter-module channels for an experiment.\n",
    "        \"\"\"\n",
    "        if filter_type:\n",
    "            exp_results = load_results(\n",
    "                os.path.join(self.flow_processed_data_dir, \"all_channels\", filter_type),\n",
    "                exp_name,\n",
    "            )\n",
    "        else:\n",
    "            exp_results = load_results(\n",
    "                os.path.join(\n",
    "                    self.flow_processed_data_dir, \"all_channels\", \"unfiltered\"\n",
    "                ),\n",
    "                exp_name,\n",
    "            )\n",
    "        session = self.par_behav.get_key_from_value(\n",
    "            self.par_behav.session_dict, exp_name\n",
    "        )\n",
    "        measurement_list_df = self.flow_session_dict[session].create_source_detector_df(\n",
    "            \"3D\"\n",
    "        )\n",
    "        channels = (measurement_list_df[\"measurement_list_index\"] - 1).tolist()\n",
    "        cols_to_select = [\"participant\", \"block\"] + [str(chan) for chan in channels]\n",
    "        inter_module_df = exp_results.loc[:, cols_to_select]\n",
    "        if hemo_type:\n",
    "            if hemo_type.lower() == \"hbo\":  # HbO\n",
    "                HbO_df = inter_module_df.iloc[\n",
    "                    :, np.r_[0, 1, 2 : len(inter_module_df.columns) : 2]\n",
    "                ]\n",
    "                return HbO_df\n",
    "            elif hemo_type.lower() == \"hbr\":  # HbR\n",
    "                HbR_df = inter_module_df.iloc[\n",
    "                    :, np.r_[0, 1, 3 : len(inter_module_df.columns) : 2]\n",
    "                ]\n",
    "                return HbR_df\n",
    "            elif hemo_type.lower() == \"hbtot\":  # HbTot\n",
    "                HbTot_df = self._compute_hemo_type_df(hemo_type, df_in=inter_module_df)\n",
    "                return HbTot_df\n",
    "            elif hemo_type.lower() == \"hbdiff\":  # HbDiff\n",
    "                HbDiff_df = self._compute_hemo_type_df(hemo_type, df_in=inter_module_df)\n",
    "                return HbDiff_df\n",
    "        else:\n",
    "            return inter_module_df\n",
    "\n",
    "    def lowpass_filter(\n",
    "        self,\n",
    "        data: Union[np.ndarray, pd.Series, pd.DataFrame],\n",
    "        cutoff: float = 0.1,\n",
    "        fs: float = 7.1,\n",
    "        order: int = 80,\n",
    "        sos: bool = True,\n",
    "    ) -> Union[np.ndarray, pd.Series, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Apply an IIR lowpass Butterworth filter.\n",
    "\n",
    "        Args:\n",
    "            data (Union[np.ndarray, pd.DataFrame]): Data to filter. Array, Series, or DataFrame.\n",
    "            cutoff (float): Cutoff frequency (Hz). Defaults to 0.1.\n",
    "            fs (float): System sampling frequency (Hz). Defaults to 7.1.\n",
    "            order (int): Filter order. Defaults to 80. NOTE: this is the doubled filtfilt order.\n",
    "            sos (bool): Use 'sos' or 'b, a' output. Defaults to True ('sos').\n",
    "\n",
    "        Returns:\n",
    "            Union[np.ndarray, pd.Series, pd.DataFrame]: Filtered data. Array, Series, or DataFrame.\n",
    "        \"\"\"\n",
    "        if sos:\n",
    "            sos = butter(\n",
    "                N=order / 2,\n",
    "                Wn=cutoff,\n",
    "                fs=fs,\n",
    "                btype=\"lowpass\",\n",
    "                output=\"sos\",\n",
    "                analog=False,\n",
    "            )\n",
    "            pad = int(len(data) * 0.8)\n",
    "            if type(data) == pd.DataFrame:\n",
    "                data_out = data.apply(\n",
    "                    lambda x: sosfiltfilt(sos, data, padlen=pad), axis=0\n",
    "                )  # apply lowpass filter\n",
    "            elif type(data) == np.ndarray or type(data) == pd.Series:\n",
    "                data_out = sosfiltfilt(sos, data, padlen=pad)  # apply lowpass filter\n",
    "        else:\n",
    "            b, a = butter(N=order / 2, Wn=cutoff, fs=fs, btype=\"lowpass\", analog=False)\n",
    "            pad = 3 * (max(len(b), len(a)) - 1)\n",
    "            if type(data) == pd.DataFrame:\n",
    "                data_out = data.apply(\n",
    "                    lambda x: filtfilt(b, a, data, padlen=pad), axis=0\n",
    "                )  # apply lowpass filter\n",
    "            elif type(data) == np.ndarray or type(data) == pd.Series:\n",
    "                data_out = filtfilt(b, a, data, padlen=pad)  # apply lowpass filter\n",
    "        return data_out\n",
    "\n",
    "    def bandpass_filter(\n",
    "        self,\n",
    "        data: Union[np.ndarray, pd.Series, pd.DataFrame],\n",
    "        cutoff_low: float = 0.01,\n",
    "        cutoff_high: float = 0.1,\n",
    "        fs: float = 7.1,\n",
    "        order: int = 20,\n",
    "        sos: bool = True,\n",
    "    ) -> Union[np.ndarray, pd.Series, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Apply an IIR bandpass Butterworth filter.\n",
    "\n",
    "        Args:\n",
    "            data (Union[np.ndarray, pd.DataFrame]): Data to filter. Array, Series, or DataFrame.\n",
    "            cutoff_low (float): Low cutoff frequency (Hz). Defaults to 0.01.\n",
    "            cutoff_high (float): High cutoff frequency (Hz). Defaults to 0.1.\n",
    "            fs (float): System sampling frequency (Hz). Defaults to 7.1.\n",
    "            order (int): Filter order. Defaults to 20. NOTE: this is the doubled filtfilt order.\n",
    "            sos (bool): Use 'sos' or 'b, a' output. Defaults to True ('sos').\n",
    "\n",
    "        Returns:\n",
    "            Union[np.ndarray, pd.Series, pd.DataFrame]: Filtered data. Array, Series, or DataFrame.\n",
    "        \"\"\"\n",
    "        if sos:\n",
    "            sos = butter(\n",
    "                N=order,\n",
    "                Wn=[cutoff_low, cutoff_high],\n",
    "                fs=fs,\n",
    "                btype=\"bandpass\",\n",
    "                output=\"sos\",\n",
    "                analog=False,\n",
    "            )\n",
    "            pad = int(len(data) * 0.8)\n",
    "            if type(data) == pd.DataFrame:\n",
    "                data_out = data.apply(\n",
    "                    lambda x: sosfiltfilt(sos, data, padlen=pad), axis=0\n",
    "                )  # apply bandpass filter\n",
    "            elif type(data) == np.ndarray or type(data) == pd.Series:\n",
    "                data_out = sosfiltfilt(sos, data, padlen=pad)  # apply bandpass filter\n",
    "        else:\n",
    "            b, a = butter(\n",
    "                N=order,\n",
    "                Wn=[cutoff_low, cutoff_high],\n",
    "                fs=fs,\n",
    "                btype=\"bandpass\",\n",
    "                analog=False,\n",
    "            )\n",
    "            pad = 3 * (max(len(b), len(a)) - 1)\n",
    "            if type(data) == pd.DataFrame:\n",
    "                data_out = data.apply(\n",
    "                    lambda x: filtfilt(b, a, data, padlen=pad), axis=0\n",
    "                )  # apply bandpass filter\n",
    "            elif type(data) == np.ndarray or type(data) == pd.Series:\n",
    "                data_out = filtfilt(b, a, data, padlen=pad)  # apply bandpass filter\n",
    "        return data_out\n",
    "\n",
    "    def plot_flow_session(\n",
    "        self, session: str, channels: Union[int, list, tuple], filter_type: str = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Plot Kernel flow session data.\n",
    "\n",
    "        Args:\n",
    "            session (str): Session number.\n",
    "            channels (Union[int, list, tuple]): Kernel Flow channels to plot.\n",
    "            filter_type (str, optional): Filter type to apply. Defaults to None.\n",
    "        \"\"\"\n",
    "        flow_session = self.flow_session_dict[session]\n",
    "        sel_flow_data = flow_session.get_data(\"dataframe\", channels)  # TODO\n",
    "        if filter_type == \"lowpass\":\n",
    "            sel_flow_data = self.lowpass_filter(sel_flow_data)\n",
    "        elif filter_type == \"bandpass\":\n",
    "            flow_data = flow_data.apply(lambda x: self.bandpass_filter(x), axis=0)\n",
    "        session_time_offset = self.time_offset_dict[session]\n",
    "        time_abs_dt = flow_session.get_time_abs(\"datetime\")\n",
    "        time_abs_dt_offset = time_abs_dt - datetime.timedelta(\n",
    "            seconds=session_time_offset\n",
    "        )\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(15, 6))\n",
    "\n",
    "        data_traces = []\n",
    "        data_labels = []\n",
    "        for channel_num in channels:\n",
    "            flow_data = sel_flow_data.iloc[:, channel_num]\n",
    "            data_type_label = self.flow_session_dict[session].get_data_type_label(\n",
    "                channel_num\n",
    "            )\n",
    "            legend_label = f\"Ch {channel_num} ({data_type_label})\"\n",
    "            if data_type_label == \"HbO\":\n",
    "                color = \"red\"\n",
    "            elif data_type_label == \"HbR\":\n",
    "                color = \"blue\"\n",
    "            (data_trace,) = ax.plot(\n",
    "                time_abs_dt_offset, flow_data, color=color, label=legend_label\n",
    "            )\n",
    "            data_traces.append(data_trace)\n",
    "            data_labels.append(legend_label)\n",
    "\n",
    "        exp_spans = []\n",
    "        for exp_name in self.par_behav.session_dict[session]:\n",
    "            exp_start_dt = self.par_behav.get_start_dt(exp_name)\n",
    "            exp_end_dt = self.par_behav.get_end_dt(exp_name)\n",
    "            ax.axvline(exp_start_dt, linestyle=\"dashed\", color=\"k\", alpha=0.75)\n",
    "            ax.axvline(exp_end_dt, linestyle=\"dashed\", color=\"k\", alpha=0.75)\n",
    "            exp_span = ax.axvspan(\n",
    "                exp_start_dt,\n",
    "                exp_end_dt,\n",
    "                color=self.par_behav.exp_color_dict[exp_name],\n",
    "                alpha=0.4,\n",
    "                label=exp_name,\n",
    "            )\n",
    "            exp_spans.append(exp_span)\n",
    "\n",
    "        data_legend = ax.legend(\n",
    "            handles=data_traces,\n",
    "            bbox_to_anchor=(1.0, 1.0),\n",
    "            facecolor=\"white\",\n",
    "            framealpha=1,\n",
    "            title=\"Kernel Flow Data\",\n",
    "        )\n",
    "        handles, labels = plt.gca().get_legend_handles_labels()\n",
    "        uni_labels = dict(zip(labels, handles))\n",
    "        [uni_labels.pop(data_label) for data_label in data_labels]\n",
    "\n",
    "        stim_legend = ax.legend(\n",
    "            uni_labels.values(),\n",
    "            uni_labels.keys(),\n",
    "            bbox_to_anchor=(1.0, 0.75),\n",
    "            facecolor=\"white\",\n",
    "            framealpha=1,\n",
    "            title=\"Experiment\",\n",
    "        )\n",
    "        ax.add_artist(data_legend)\n",
    "        session_split = session.split(\"_\")\n",
    "        exp_title = session_split[0].capitalize() + \" \" + session_split[1]\n",
    "        ax.set_title(exp_title)\n",
    "        datetime_fmt = mdates.DateFormatter(\"%H:%M:%S\")\n",
    "        ax.xaxis.set_major_formatter(datetime_fmt)\n",
    "        ax.set_xlabel(\"Time\", fontsize=16, color=\"k\")\n",
    "\n",
    "    def plot_flow_exp(\n",
    "        self,\n",
    "        exp_name: str,\n",
    "        channels: list,\n",
    "        filter_type: str = None,\n",
    "        filter_order: int = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Plot Kernel Flow experiment data.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "            channels (list): Kernel Flow channels to plot.\n",
    "            filter_type (str, optional): Filter type to apply. Defaults to None.\n",
    "            filter_order (int): Filter order. Defaults to None (default filter order value).\n",
    "        \"\"\"\n",
    "        flow_exp = self.load_flow_exp(exp_name)\n",
    "        session = self.par_behav.get_key_from_value(\n",
    "            self.par_behav.session_dict, exp_name\n",
    "        )\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(15, 6))\n",
    "\n",
    "        data_traces = []\n",
    "        data_labels = []\n",
    "        for channel_num in channels:\n",
    "            timeseries = flow_exp[\"datetime\"]\n",
    "            flow_data = flow_exp.iloc[:, channel_num + 1]\n",
    "            if filter_type.lower() == \"lowpass\":\n",
    "                if filter_order:\n",
    "                    flow_data = self.lowpass_filter(flow_data, order=filter_order)\n",
    "                else:\n",
    "                    flow_data = self.lowpass_filter(flow_data)\n",
    "            elif filter_type.lower() == \"bandpass\":\n",
    "                if filter_order:\n",
    "                    flow_data = self.bandpass_filter(flow_data, order=filter_order)\n",
    "                else:\n",
    "                    flow_data = self.bandpass_filter(flow_data)\n",
    "            data_type_label = self.flow_session_dict[session].get_data_type_label(\n",
    "                channel_num\n",
    "            )\n",
    "            # legend_label = f\"Ch {channel_num} ({data_type_label})\"\n",
    "            legend_label = f\"{data_type_label}\"\n",
    "            if data_type_label == \"HbO\":\n",
    "                color = \"red\"\n",
    "            elif data_type_label == \"HbR\":\n",
    "                color = \"blue\"\n",
    "            (data_trace,) = ax.plot(\n",
    "                timeseries, flow_data, color=color, label=legend_label\n",
    "            )\n",
    "            data_traces.append(data_trace)\n",
    "            data_labels.append(legend_label)\n",
    "\n",
    "        exp_start_dt = self.par_behav.get_start_dt(exp_name, self.adj_ts_markers)\n",
    "        exp_end_dt = self.par_behav.get_end_dt(exp_name, self.adj_ts_markers)\n",
    "        ax.axvline(exp_start_dt, linestyle=\"dashed\", color=\"k\", alpha=0.75)\n",
    "        ax.axvline(exp_end_dt, linestyle=\"dashed\", color=\"k\", alpha=0.75)\n",
    "        results_dir = os.path.join(os.getcwd(), \"processed_data\", \"behavioral\")\n",
    "        exp_results = load_results(results_dir, exp_name, self.par_num)\n",
    "        exp_title = self.par_behav.format_exp_name(exp_name)\n",
    "\n",
    "        stim_spans = []\n",
    "        for _, row in exp_results.iterrows():\n",
    "            try:\n",
    "                uni_stim_dict = self.par_behav.create_unique_stim_dict(\n",
    "                    exp_results, \"stim\"\n",
    "                )\n",
    "                stim = row[\"stim\"]\n",
    "                legend_label = self.par_behav.format_exp_name(row[\"stim\"])\n",
    "            except KeyError:\n",
    "                uni_stim_dict = self.par_behav.create_unique_stim_dict(\n",
    "                    exp_results, \"block\"\n",
    "                )\n",
    "                stim = row[\"block\"]\n",
    "                legend_label = self.par_behav.format_exp_name(row[\"block\"])\n",
    "            color_index = uni_stim_dict[stim]\n",
    "            stim_start = datetime.datetime.fromtimestamp(row[\"stim_start\"])\n",
    "            try:\n",
    "                stim_end = datetime.datetime.fromtimestamp(row[\"stim_end\"])\n",
    "            except ValueError:\n",
    "                if exp_name == \"go_no_go\":\n",
    "                    stim_time = 0.5  # seconds\n",
    "                stim_end = datetime.datetime.fromtimestamp(\n",
    "                    row[\"stim_start\"] + stim_time\n",
    "                )\n",
    "            stim_span = ax.axvspan(\n",
    "                stim_start,\n",
    "                stim_end,\n",
    "                color=self.plot_color_dict[color_index],\n",
    "                alpha=0.4,\n",
    "                label=legend_label,\n",
    "            )\n",
    "            stim_spans.append(stim_span)\n",
    "\n",
    "        data_legend = ax.legend(\n",
    "            handles=data_traces,\n",
    "            bbox_to_anchor=(1.0, 1.0),\n",
    "            facecolor=\"white\",\n",
    "            framealpha=1,\n",
    "            title=\"fNIRS data\",\n",
    "        )\n",
    "        handles, labels = plt.gca().get_legend_handles_labels()\n",
    "        uni_labels = dict(zip(labels, handles))\n",
    "        [uni_labels.pop(data_label) for data_label in data_labels]\n",
    "\n",
    "        stim_legend = ax.legend(\n",
    "            uni_labels.values(),\n",
    "            uni_labels.keys(),\n",
    "            bbox_to_anchor=(1.0, 0.75),\n",
    "            facecolor=\"white\",\n",
    "            framealpha=1,\n",
    "            title=\"Stimulus\",\n",
    "        )\n",
    "        ax.add_artist(data_legend)\n",
    "        ax.set_title(exp_title)\n",
    "        datetime_fmt = mdates.DateFormatter(\"%H:%M:%S\")\n",
    "        ax.xaxis.set_major_formatter(datetime_fmt)\n",
    "        ax.set_xlabel(\"Time\", fontsize=16, color=\"k\")\n",
    "        ax.set_ylabel(\"Concentration (\\u03bcM)\", fontsize=16, color=\"k\")\n",
    "\n",
    "\n",
    "class Flow_Results:\n",
    "    def __init__(self):\n",
    "        self.results_dir = os.path.join(os.getcwd(), \"results\")\n",
    "        self.exp_names = [\n",
    "            \"audio_narrative\",\n",
    "            \"go_no_go\",\n",
    "            \"king_devick\",\n",
    "            \"n_back\",\n",
    "            \"resting_state\",\n",
    "            \"tower_of_london\",\n",
    "            \"vSAT\",\n",
    "            \"video_narrative_cmiyc\",\n",
    "            \"video_narrative_sherlock\",\n",
    "        ]\n",
    "        self.hemo_types = [\"HbO\", \"HbR\", \"HbTot\", \"HbDiff\"]\n",
    "        self.par = Participant_Flow(1)\n",
    "        self.flow_session = self.par.flow_session_dict[\"session_1001\"]\n",
    "\n",
    "    def process_flow_data(\n",
    "        self, num_pars: int, filter_type: str = None, inter_module_only=True\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Generate a CSV file that contains the Kernel Flow stimulus response data\n",
    "        for all experiments and participants.\n",
    "\n",
    "        Args:\n",
    "            num_pars (int): Number of participants in the study.\n",
    "            filter_type (str): Filter to apply to the data. Defaults to None.\n",
    "            inter_module_only (bool): Select only inter-module channels. Defaults to True.\n",
    "        \"\"\"\n",
    "        if inter_module_only:\n",
    "            print(f\"Processing participants ...\")\n",
    "            for hemo_type in self.hemo_types:\n",
    "                all_exp_results_list = []\n",
    "                exp_results_list = []\n",
    "                for exp_name in self.exp_names:\n",
    "                    stim_resp_df = self.par.create_inter_module_exp_results_df(\n",
    "                        exp_name, hemo_type, filter_type\n",
    "                    )\n",
    "                    exp_results_list.append(stim_resp_df)\n",
    "                all_exp_results_list.append(exp_results_list)\n",
    "\n",
    "                if filter_type:\n",
    "                    filedir = os.path.join(\n",
    "                        self.par.flow_processed_data_dir,\n",
    "                        \"inter_module_channels\",\n",
    "                        filter_type,\n",
    "                        hemo_type,\n",
    "                    )\n",
    "                else:\n",
    "                    filedir = os.path.join(\n",
    "                        self.par.flow_processed_data_dir,\n",
    "                        \"inter_module_channels\",\n",
    "                        \"unfiltered\",\n",
    "                        hemo_type,\n",
    "                    )\n",
    "                if not os.path.exists(filedir):\n",
    "                    os.makedirs(filedir)\n",
    "\n",
    "                print(f\"Creating {hemo_type} CSV files ...\")\n",
    "                all_exp_filepath = os.path.join(\n",
    "                    filedir, f\"all_experiments_flow_{hemo_type}.csv\"\n",
    "                )\n",
    "                if os.path.exists(all_exp_filepath):\n",
    "                    os.remove(all_exp_filepath)\n",
    "                for i, exp_name in enumerate(self.exp_names):\n",
    "                    exp_rows = [\n",
    "                        exp_results_list[i] for exp_results_list in all_exp_results_list\n",
    "                    ]\n",
    "                    exp_df = pd.concat(exp_rows, axis=0, ignore_index=True)\n",
    "                    filepath = os.path.join(filedir, f\"{exp_name}_flow_{hemo_type}.csv\")\n",
    "                    exp_df.to_csv(filepath, index=False)\n",
    "                    all_exp_df = exp_df.copy(deep=True)\n",
    "                    exp_name_col = [exp_name] * len(all_exp_df.index)\n",
    "                    all_exp_df.insert(0, \"experiment\", exp_name_col)\n",
    "                    # TODO: add demographic data\n",
    "                    if i == 0:\n",
    "                        all_exp_df.to_csv(\n",
    "                            all_exp_filepath, mode=\"a\", header=True, index=False\n",
    "                        )\n",
    "                    else:\n",
    "                        all_exp_df.to_csv(\n",
    "                            all_exp_filepath, mode=\"a\", header=False, index=False\n",
    "                        )\n",
    "        else:\n",
    "            for hemo_type in self.hemo_types:\n",
    "                print(f\"Processing {hemo_type} data ...\")\n",
    "                all_exp_results_list = []\n",
    "                for par_num in range(1, num_pars + 1):\n",
    "                    par = Participant_Flow(par_num)\n",
    "                    exp_results_list = []\n",
    "                    for exp_name in self.exp_names:\n",
    "                        stim_resp_df = par.create_exp_stim_response_df(\n",
    "                            exp_name, hemo_type, filter_type\n",
    "                        )\n",
    "                        exp_results_list.append(stim_resp_df)\n",
    "                    all_exp_results_list.append(exp_results_list)\n",
    "\n",
    "                if filter_type:\n",
    "                    filedir = os.path.join(\n",
    "                        self.par.flow_processed_data_dir,\n",
    "                        \"all_channels\",\n",
    "                        filter_type,\n",
    "                        hemo_type,\n",
    "                    )\n",
    "                else:\n",
    "                    filedir = os.path.join(\n",
    "                        self.par.flow_processed_data_dir,\n",
    "                        \"all_channels\",\n",
    "                        \"unfiltered\",\n",
    "                        hemo_type,\n",
    "                    )\n",
    "                if not os.path.exists(filedir):\n",
    "                    os.makedirs(filedir)\n",
    "\n",
    "                print(\"Creating CSV files ...\")\n",
    "                all_exp_filepath = os.path.join(\n",
    "                    filedir, f\"all_experiments_flow_{hemo_type}.csv\"\n",
    "                )\n",
    "                if os.path.exists(all_exp_filepath):\n",
    "                    os.remove(all_exp_filepath)\n",
    "                for i, exp_name in enumerate(self.exp_names):\n",
    "                    exp_rows = [\n",
    "                        exp_results_list[i] for exp_results_list in all_exp_results_list\n",
    "                    ]\n",
    "                    exp_df = pd.concat(exp_rows, axis=0, ignore_index=True)\n",
    "                    filepath = os.path.join(filedir, f\"{exp_name}_flow_{hemo_type}.csv\")\n",
    "                    exp_df.to_csv(filepath, index=False)\n",
    "                    all_exp_df = exp_df.copy(deep=True)\n",
    "                    exp_name_col = [exp_name] * len(all_exp_df.index)\n",
    "                    all_exp_df.insert(0, \"experiment\", exp_name_col)\n",
    "                    if i == 0:\n",
    "                        all_exp_df.to_csv(\n",
    "                            all_exp_filepath, mode=\"a\", header=True, index=False\n",
    "                        )\n",
    "                    else:\n",
    "                        all_exp_df.to_csv(\n",
    "                            all_exp_filepath, mode=\"a\", header=False, index=False\n",
    "                        )\n",
    "\n",
    "    def load_processed_flow_data(\n",
    "        self,\n",
    "        exp_name: str,\n",
    "        hemo_type: str,\n",
    "        filter_type: str = None,\n",
    "        inter_module_only=True,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load processes Kernel Flow data into a DataFrame.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "            hemo_type (str): Hemodynamic type. \"HbO\", \"HbR\", \"HbTot\", or \"HbDiff\".\n",
    "            filter_type (str, optional): Filter to apply to the data. Defaults to None.\n",
    "            inter_module_only (bool): Select only inter-module channels. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Processed DataFrame.\n",
    "        \"\"\"\n",
    "        if inter_module_only:\n",
    "            read_filedir = os.path.join(\n",
    "                self.par.flow_processed_data_dir,\n",
    "                \"inter_module_channels\",\n",
    "                filter_type,\n",
    "                hemo_type,\n",
    "            )\n",
    "        else:\n",
    "            read_filedir = os.path.join(\n",
    "                self.par.flow_processed_data_dir,\n",
    "                \"all_channels\",\n",
    "                filter_type,\n",
    "                hemo_type,\n",
    "            )\n",
    "        read_filename = f\"{exp_name}_flow_{hemo_type}.csv\"\n",
    "        read_filepath = os.path.join(read_filedir, read_filename)\n",
    "        flow_df = pd.read_csv(read_filepath)\n",
    "\n",
    "        if exp_name == \"king_devick\":\n",
    "            flow_df = flow_df.drop(\n",
    "                flow_df[\n",
    "                    (flow_df[\"participant\"] == 15) & (flow_df[\"block\"] == \"card_1\")\n",
    "                ].index\n",
    "            )\n",
    "            flow_df.loc[flow_df[\"participant\"] == 15, \"block\"] = flow_df.loc[\n",
    "                flow_df[\"participant\"] == 15, \"block\"\n",
    "            ].apply(lambda x: x[:-1] + str(int(x[-1]) - 1))\n",
    "        return flow_df\n",
    "\n",
    "    def run_rm_anova(\n",
    "        self,\n",
    "        exp_name: str,\n",
    "        hemo_type: str,\n",
    "        filter_type: str = None,\n",
    "        corr: bool = True,\n",
    "        inter_module_only=True,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Run a repeated measures ANOVA on processed inter-module channels.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "            hemo_type (str): Hemodynamic type. \"HbO\", \"HbR\", \"HbTot\", or \"HbDiff\".\n",
    "            filter_type (str): Filter to apply to the data. Defaults to None.\n",
    "            corr (bool): Apply a Bonferroni correction to the p-values. Defaults to True.\n",
    "            inter_module_only (bool): Select only inter-module channels. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: ANOVA results.\n",
    "        \"\"\"\n",
    "        flow_df = self.load_processed_flow_data(\n",
    "            exp_name, hemo_type, filter_type, inter_module_only\n",
    "        )\n",
    "        channels = list(flow_df.columns[2:])\n",
    "        num_channels = len(channels)\n",
    "        aov_list = []\n",
    "        for channel in channels:\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\n",
    "                    \"ignore\", category=RuntimeWarning\n",
    "                )  # suppress RuntimeWarning warnings\n",
    "                try:\n",
    "                    aov = pg.rm_anova(\n",
    "                        data=flow_df,\n",
    "                        dv=channel,\n",
    "                        within=\"block\",\n",
    "                        subject=\"participant\",\n",
    "                        effsize=\"np2\",\n",
    "                    )\n",
    "                    aov_final = aov[[\"p-unc\", \"F\", \"ddof1\", \"ddof2\"]].copy()\n",
    "                    aov_final.rename(\n",
    "                        columns={\n",
    "                            \"p-unc\": \"p_value\",\n",
    "                            \"F\": \"F_value\",\n",
    "                            \"ddof1\": \"df1\",\n",
    "                            \"ddof2\": \"df2\",\n",
    "                        },\n",
    "                        inplace=True,\n",
    "                    )\n",
    "                except (\n",
    "                    KeyError,\n",
    "                    ValueError,\n",
    "                    ZeroDivisionError,\n",
    "                    np.linalg.LinAlgError,\n",
    "                ):  # handle columns with all NaN values\n",
    "                    aov_final = pd.DataFrame(\n",
    "                        {\n",
    "                            \"p_value\": [float(\"NaN\")],\n",
    "                            \"F_value\": [float(\"NaN\")],\n",
    "                            \"df1\": [float(\"NaN\")],\n",
    "                            \"df2\": [float(\"NaN\")],\n",
    "                        }\n",
    "                    )\n",
    "            aov_final[\"is_sig\"] = aov_final[\"p_value\"] < 0.05\n",
    "            if corr:  # apply Bonferroni correction\n",
    "                alpha_corr = 0.05 / num_channels\n",
    "                aov_final.insert(0, \"alpha_corr\", alpha_corr)\n",
    "                aov_final[\"is_sig_corr\"] = aov_final[\"p_value\"] < alpha_corr\n",
    "            aov_final.insert(0, \"channel_num\", channel)\n",
    "            aov_final[\"channel_num\"] = aov_final[\"channel_num\"].astype(int)\n",
    "            aov_list.append(aov_final)\n",
    "        exp_aov_results = pd.concat(aov_list)\n",
    "        return exp_aov_results\n",
    "\n",
    "    def run_all_rm_anovas(\n",
    "        self,\n",
    "        filter_type: str = None,\n",
    "        corr: bool = True,\n",
    "        inter_module_only=True,\n",
    "        brain_regions: bool = False,\n",
    "        depth: Union[int, float] = None,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Run a repeated measures ANOVA for all experiments and hemodynamic types. Results are\n",
    "        written to CSV files and corresponding brain regions can be added.\n",
    "\n",
    "        Args:\n",
    "            filter_type (str): Filter to apply to the data. Defaults to None.\n",
    "            corr (bool): Apply a Bonferroni correction to the p-values. Defaults to True.\n",
    "            inter_module_only (bool): Select only inter-module channels. Defaults to True.\n",
    "            brain_regions (bool): Include AAL and BA brain region columns. Defaults to False.\n",
    "            depth (Union[int, float], optional): Depth into the brain. Defaults to None (brain surface).\n",
    "        \"\"\"\n",
    "        for exp_name in [\n",
    "            \"go_no_go\",\n",
    "            \"king_devick\",\n",
    "            \"n_back\",\n",
    "            \"tower_of_london\",\n",
    "            \"vSAT\",\n",
    "        ]:\n",
    "            for hemo_type in self.hemo_types:\n",
    "                if not filter_type:\n",
    "                    filter_type = \"unfiltered\"\n",
    "                if inter_module_only:\n",
    "                    write_filedir = os.path.join(\n",
    "                        self.results_dir, \"inter_module_channels\", exp_name, hemo_type\n",
    "                    )\n",
    "                else:\n",
    "                    write_filedir = os.path.join(\n",
    "                        self.results_dir, \"all_channels\", exp_name, hemo_type\n",
    "                    )\n",
    "                if not os.path.exists(write_filedir):\n",
    "                    os.makedirs(write_filedir)\n",
    "                exp_aov_results = self.run_rm_anova(\n",
    "                    exp_name, hemo_type, filter_type, corr, inter_module_only\n",
    "                )\n",
    "                if brain_regions:\n",
    "                    if depth is None:\n",
    "                        depth = 0\n",
    "                    flow_atlas = self.par.flow.load_flow_atlas(depth, minimal=True)\n",
    "                    flow_atlas.dropna(subset=[\"channel_num\"], inplace=True)\n",
    "                    write_filename = f\"{exp_name}_flow_stats_{hemo_type}_{filter_type}_depth_{depth}.csv\"\n",
    "                    exp_aov_results = pd.merge(\n",
    "                        exp_aov_results, flow_atlas, on=\"channel_num\", how=\"left\"\n",
    "                    )\n",
    "                else:\n",
    "                    write_filename = (\n",
    "                        f\"{exp_name}_flow_stats_{hemo_type}_{filter_type}.csv\"\n",
    "                    )\n",
    "                write_filepath = os.path.join(write_filedir, write_filename)\n",
    "                exp_aov_results.to_csv(write_filepath, index=False)\n",
    "\n",
    "    def run_pos_hoc_test(\n",
    "        self,\n",
    "        exp_name: str,\n",
    "        hemo_type: str,\n",
    "        filter_type: str = None,\n",
    "        drop: bool = False,\n",
    "        inter_module_only: bool = True,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Run pairwise t-tests for post-hoc ANOVA analysis.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "            hemo_type (str): Hemodynamic type. \"HbO\", \"HbR\", \"HbTot\", or \"HbDiff\".\n",
    "            filter_type (str): Filter to apply to the data. Defaults to None.\n",
    "            drop (bool): Drop columns with extra post-hoc info. Defaults to False.\n",
    "            inter_module_only (bool): Select only inter-module channels. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Post-hoc test results.\n",
    "        \"\"\"\n",
    "        flow_df = self.load_processed_flow_data(\n",
    "            exp_name, hemo_type, filter_type, inter_module_only\n",
    "        )\n",
    "        sig_df = self.load_flow_stats(\n",
    "            exp_name,\n",
    "            hemo_type,\n",
    "            filter_type,\n",
    "            inter_module_only=inter_module_only,\n",
    "            sig_only=True,\n",
    "        )\n",
    "        sig_channels = list(sig_df[\"channel_num\"].astype(str))\n",
    "        sig_flow_df = flow_df.loc[:, flow_df.columns.isin(sig_channels)]\n",
    "\n",
    "        pos_hoc_list = []\n",
    "        for channel in sig_flow_df.columns:\n",
    "            results = pg.pairwise_tests(\n",
    "                data=flow_df, dv=channel, within=\"block\", subject=\"participant\"\n",
    "            )\n",
    "            aov_p_value = float(\n",
    "                sig_df[sig_df[\"channel_num\"] == int(channel)][\"p_value\"]\n",
    "            )\n",
    "            results.insert(0, \"aov_p_value\", aov_p_value)\n",
    "            results.insert(0, \"channel_num\", channel)\n",
    "            pos_hoc_list.append(results)\n",
    "        post_hoc_results = pd.concat(pos_hoc_list, ignore_index=True)\n",
    "        post_hoc_results = post_hoc_results.rename(\n",
    "            columns={\n",
    "                \"Contrast\": \"within\",\n",
    "                \"A\": \"condition_A\",\n",
    "                \"B\": \"condition_B\",\n",
    "                \"T\": \"t_stat\",\n",
    "                \"dof\": \"df\",\n",
    "                \"p-unc\": \"p_value\",\n",
    "            }\n",
    "        )\n",
    "        if drop:\n",
    "            post_hoc_results = post_hoc_results.drop(\n",
    "                columns=[\n",
    "                    \"Paired\",\n",
    "                    \"Parametric\",\n",
    "                    \"alternative\",\n",
    "                    \"BF10\",\n",
    "                    \"hedges\",\n",
    "                ]\n",
    "            )\n",
    "        return post_hoc_results\n",
    "\n",
    "    def run_all_pos_hoc_tests(\n",
    "        self,\n",
    "        filter_type: str = None,\n",
    "        drop: bool = False,\n",
    "        inter_module_only: bool = True,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Run pairwise t-tests for post-hoc ANOVA analysis for all experiments and hemodynamic types.\n",
    "        Results are written to CSV files and corresponding brain regions can be added..\n",
    "\n",
    "        Args:\n",
    "            filter_type (str): Filter to apply to the data. Defaults to None.\n",
    "            drop (bool): Drop columns with extra post-hoc info. Defaults to False.\n",
    "            inter_module_only (bool): Select only inter-module channels. Defaults to True.\n",
    "        \"\"\"\n",
    "        for exp_name in [\n",
    "            \"go_no_go\",\n",
    "            \"king_devick\",\n",
    "            \"n_back\",\n",
    "            \"tower_of_london\",\n",
    "            \"vSAT\",\n",
    "        ]:\n",
    "            for hemo_type in self.hemo_types:\n",
    "                if not filter_type:\n",
    "                    filter_type = \"unfiltered\"\n",
    "                if inter_module_only:\n",
    "                    write_filedir = os.path.join(\n",
    "                        self.results_dir,\n",
    "                        \"inter_module_channels\",\n",
    "                        exp_name,\n",
    "                        hemo_type,\n",
    "                    )\n",
    "                else:\n",
    "                    write_filedir = os.path.join(\n",
    "                        self.results_dir,\n",
    "                        \"all_channels\",\n",
    "                        exp_name,\n",
    "                        hemo_type,\n",
    "                    )\n",
    "                write_filename = f\"{exp_name}_post_hoc_{hemo_type}_{filter_type}.csv\"\n",
    "                write_filepath = os.path.join(write_filedir, write_filename)\n",
    "                post_hoc_results = self.run_pos_hoc_test(\n",
    "                    exp_name, hemo_type, filter_type, drop\n",
    "                )\n",
    "                post_hoc_results.to_csv(write_filepath, index=False)\n",
    "\n",
    "    def load_flow_stats(\n",
    "        self,\n",
    "        exp_name: str,\n",
    "        hemo_type: str,\n",
    "        filter_type: str = None,\n",
    "        corr: bool = True,\n",
    "        inter_module_only: bool = True,\n",
    "        sig_only: bool = False,\n",
    "        brain_regions: bool = False,\n",
    "        depth: Union[int, float] = None,\n",
    "        print_sig_results: bool = False,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load Kernel Flow statistical results.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "            hemo_type (str): Hemodynamic type. \"HbO\", \"HbR\", \"HbTot\", or \"HbDiff\".\n",
    "            filter_type (str): Filter to apply to the data. Defaults to None.\n",
    "            corr (bool): Apply a Bonferroni correction to the p-values. Defaults to True.\n",
    "            inter_module_only (bool): Select only inter-module channels. Defaults to True.\n",
    "            sig_only (bool): Return only significant results (p < 0.05). Defaults to False.\n",
    "            brain_regions (bool): Include AAL and BA brain region columns. Defaults to False.\n",
    "            depth (Union[int, float], optional): Depth into the brain. Defaults to None (brain surface).\n",
    "            print_sig_results (bool): Print significant results. Defaults to False.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Statistical results for an experiment and hemodynamic type.\n",
    "        \"\"\"\n",
    "        if not filter_type:\n",
    "            filter_type = \"unfiltered\"\n",
    "        if depth is None:\n",
    "            depth = 0\n",
    "        if inter_module_only:\n",
    "            filename = (\n",
    "                f\"{exp_name}_flow_stats_{hemo_type}_{filter_type}_depth_{depth}.csv\"\n",
    "            )\n",
    "        else:\n",
    "            filename = f\"{exp_name}_flow_stats_{hemo_type}_{filter_type}.csv\"\n",
    "        if inter_module_only:\n",
    "            filepath = os.path.join(\n",
    "                self.results_dir,\n",
    "                \"inter_module_channels\",\n",
    "                exp_name,\n",
    "                hemo_type,\n",
    "                filename,\n",
    "            )\n",
    "        else:\n",
    "            filepath = os.path.join(\n",
    "                self.results_dir,\n",
    "                \"all_channels\",\n",
    "                exp_name,\n",
    "                hemo_type,\n",
    "                filename,\n",
    "            )\n",
    "        flow_stats = pd.read_csv(filepath)\n",
    "        if corr:\n",
    "            sig_col_name = \"is_sig_corr\"\n",
    "            flow_stats_out = flow_stats[\n",
    "                [\n",
    "                    \"channel_num\",\n",
    "                    \"alpha_corr\",\n",
    "                    \"p_value\",\n",
    "                    \"F_value\",\n",
    "                    \"df1\",\n",
    "                    \"df2\",\n",
    "                    sig_col_name,\n",
    "                ]\n",
    "            ]\n",
    "        else:\n",
    "            sig_col_name = \"is_sig\"\n",
    "            flow_stats_out = flow_stats[\n",
    "                [\"channel_num\", \"p_value\", \"F_value\", \"df1\", \"df2\", sig_col_name]\n",
    "            ]\n",
    "        if brain_regions:\n",
    "            flow_atlas = self.par.flow.load_flow_atlas(depth, minimal=True)\n",
    "            flow_atlas.dropna(subset=[\"channel_num\"], inplace=True)\n",
    "            flow_stats_out = pd.merge(\n",
    "                flow_stats_out, flow_atlas, on=\"channel_num\", how=\"left\"\n",
    "            )\n",
    "        sig_stats = flow_stats_out[flow_stats_out[sig_col_name] == True].sort_values(\n",
    "            by=\"p_value\", ascending=True\n",
    "        )\n",
    "        if print_sig_results:\n",
    "            print(sig_stats.to_string(index=False))\n",
    "        if sig_only:\n",
    "            return sig_stats\n",
    "        else:\n",
    "            return flow_stats_out\n",
    "\n",
    "    def load_post_hoc_stats(\n",
    "        self,\n",
    "        exp_name: str,\n",
    "        hemo_type: str,\n",
    "        filter_type: str = None,\n",
    "        drop: bool = False,\n",
    "        inter_module_only=True,\n",
    "    ) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load Kernel Flow ANOVA post-hoc statistical results.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "            hemo_type (str): Hemodynamic type. \"HbO\", \"HbR\", \"HbTot\", or \"HbDiff\".\n",
    "            filter_type (str): Filter to apply to the data. Defaults to None.\n",
    "            drop (bool): Drop columns with extra post-hoc info. Defaults to False.\n",
    "            inter_module_only (bool): Select only inter-module channels. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Post-hoc statistical results for an experiment and hemodynamic type.\n",
    "        \"\"\"\n",
    "        if not filter_type:\n",
    "            filter_type = \"unfiltered\"\n",
    "        filename = f\"{exp_name}_post_hoc_{hemo_type}_{filter_type}.csv\"\n",
    "        if inter_module_only:\n",
    "            filepath = os.path.join(\n",
    "                self.results_dir,\n",
    "                \"inter_module_channels\",\n",
    "                exp_name,\n",
    "                hemo_type,\n",
    "                filename,\n",
    "            )\n",
    "        else:\n",
    "            filepath = os.path.join(\n",
    "                self.results_dir,\n",
    "                \"all_channels\",\n",
    "                exp_name,\n",
    "                hemo_type,\n",
    "                filename,\n",
    "            )\n",
    "        post_hoc_stats = pd.read_csv(filepath)\n",
    "        if drop:\n",
    "            try:\n",
    "                post_hoc_stats = post_hoc_stats.drop(\n",
    "                    columns=[\"Paired\", \"Parametric\", \"alternative\", \"BF10\", \"hedges\"]\n",
    "                )\n",
    "            except KeyError:\n",
    "                pass\n",
    "        return post_hoc_stats\n",
    "\n",
    "    def create_flow_stats_df(\n",
    "        self,\n",
    "        exp_name: str,\n",
    "        hemo_type: str,\n",
    "        filter_type: str = None,\n",
    "        corr: bool = False,\n",
    "        inter_module_only=True,\n",
    "    ) -> pd.DataFrame:  # TODO needs a fix\n",
    "        \"\"\"\n",
    "        Create a DataFrame with significant channels and corresponding brain regions.\n",
    "\n",
    "        Args:\n",
    "            exp_name (str): Name of the experiment.\n",
    "            hemo_type (str): Hemodynamic type. \"HbO\", \"HbR\", \"HbTot\", or \"HbDiff\".\n",
    "            filter_type (str): Filter to apply to the data. Defaults to None.\n",
    "            corr (bool): Apply a Bonferroni correction to the p-values. Defaults to False.\n",
    "            inter_module_only (bool): Select only inter-module channels. Defaults to True.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Significant stats DataFrame with brain regions.\n",
    "        \"\"\"\n",
    "        sig_stats = self.load_flow_stats(\n",
    "            exp_name,\n",
    "            hemo_type,\n",
    "            filter_type,\n",
    "            inter_module_only=inter_module_only,\n",
    "            sig_only=True,\n",
    "        )\n",
    "        sig_channels = list(sig_stats[\"channel_num\"])\n",
    "        source_detector_df = self.flow_session.create_source_detector_df(\n",
    "            \"3D\", brain_regions=True, channels=sig_channels\n",
    "        )\n",
    "        merged_df = pd.merge(\n",
    "            sig_stats, source_detector_df, on=\"channel_num\", how=\"left\"\n",
    "        )\n",
    "        if corr:\n",
    "            flow_stats_df = merged_df.loc[\n",
    "                :,\n",
    "                [\n",
    "                    \"channel_num\",\n",
    "                    \"p_value\",\n",
    "                    \"F_value\",\n",
    "                    \"AAL_distance\",\n",
    "                    \"AAL_region\",\n",
    "                    \"BA_distance\",\n",
    "                    \"BA_region\",\n",
    "                ],\n",
    "            ]\n",
    "        else:\n",
    "            flow_stats_df = merged_df.loc[\n",
    "                :,\n",
    "                [\n",
    "                    \"channel_num\",\n",
    "                    \"p_value\",\n",
    "                    \"F_value\",\n",
    "                    \"AAL_distance\",\n",
    "                    \"AAL_region\",\n",
    "                    \"BA_distance\",\n",
    "                    \"BA_region\",\n",
    "                ],\n",
    "            ]\n",
    "        return flow_stats_df\n",
    "\n",
    "    def plot_stat_results(\n",
    "        self,\n",
    "        dim: str,\n",
    "        exp_name: str,\n",
    "        hemo_type: str,\n",
    "        filter_type: str = None,\n",
    "        corr: bool = False,\n",
    "        add_labels: bool = False,\n",
    "        filepath: str = None,\n",
    "        show: bool = True,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Plot Kernel Flow statistical results.\n",
    "\n",
    "        Args:\n",
    "            dim (str): Position data dimension \"2D\" or \"3D\".\n",
    "            exp_name (str): Name of the experiment.\n",
    "            hemo_type (str): Hemodynamic type. \"HbO\", \"HbR\", \"HbTot\", or \"HbDiff\".\n",
    "            filter_type (str): Filter to apply to the data. Defaults to None.\n",
    "            corr (bool): Apply a Bonferroni correction to the p-values. Defaults to False.\n",
    "            add_labels (bool): Add a channel number label at each detector position. Defaults to False.\n",
    "            filepath (str): Filepath to save figure. Default to None (no output).\n",
    "            show (bool): Display the figure. Defaults to True.\n",
    "        \"\"\"\n",
    "\n",
    "        def _add_missing_pos(dim: str) -> pd.DataFrame:\n",
    "            \"\"\"\n",
    "            Add missing detector/source positions to the plot DataFrame.\n",
    "\n",
    "            Args:\n",
    "                dim (str): Position data dimension \"2D\" or \"3D\".\n",
    "\n",
    "            Returns:\n",
    "                pd.DataFrame: Plot DataFrame with missing positions added.\n",
    "            \"\"\"\n",
    "            nan_columns = [\n",
    "                \"channel_num\",\n",
    "                \"F_value\",\n",
    "                \"p_value\",\n",
    "                \"measurement_list_index\",\n",
    "                \"data_type\",\n",
    "                \"data_type_index\",\n",
    "                \"detector_index\",\n",
    "                \"source_index\",\n",
    "                \"source_label\",\n",
    "                \"detector_label\",\n",
    "            ]\n",
    "            plot_df_temp = pd.merge(flow_stats, source_detector_df, on=\"channel_num\")\n",
    "            row_list = []\n",
    "            if dim.lower() == \"2d\":\n",
    "                for detector_pos in self.flow_session.missing_detector_pos_2d:\n",
    "                    new_row = pd.Series(\n",
    "                        {\n",
    "                            \"source_x_pos\": self.flow_session.missing_source_pos_2d[0],\n",
    "                            \"source_y_pos\": self.flow_session.missing_source_pos_2d[1],\n",
    "                            \"detector_x_pos\": detector_pos[0],\n",
    "                            \"detector_y_pos\": detector_pos[1],\n",
    "                        }\n",
    "                    )\n",
    "                    row_list.append(new_row)\n",
    "                missing_pos_df = pd.DataFrame(row_list)\n",
    "                plot_df = pd.concat(\n",
    "                    [plot_df_temp, missing_pos_df], axis=0, ignore_index=True\n",
    "                )\n",
    "                plot_df.loc[\n",
    "                    plot_df.shape[0] - len(self.flow_session.missing_detector_pos_2d) :,\n",
    "                    nan_columns,\n",
    "                ] = float(\"NaN\")\n",
    "            elif dim.lower() == \"3d\":\n",
    "                for detector_pos in self.flow_session.missing_detector_pos_3d:\n",
    "                    new_row = pd.Series(\n",
    "                        {\n",
    "                            \"source_x_pos\": self.flow_session.missing_source_pos_3d[0],\n",
    "                            \"source_y_pos\": self.flow_session.missing_source_pos_3d[1],\n",
    "                            \"source_z_pos\": self.flow_session.missing_source_pos_3d[2],\n",
    "                            \"detector_x_pos\": detector_pos[0],\n",
    "                            \"detector_y_pos\": detector_pos[1],\n",
    "                            \"detector_z_pos\": detector_pos[2],\n",
    "                        }\n",
    "                    )\n",
    "                    row_list.append(new_row)\n",
    "                missing_pos_df = pd.DataFrame(row_list)\n",
    "                plot_df = pd.concat(\n",
    "                    [plot_df_temp, missing_pos_df], axis=0, ignore_index=True\n",
    "                )\n",
    "                plot_df.loc[\n",
    "                    plot_df.shape[0] - len(self.flow_session.missing_detector_pos_3d) :,\n",
    "                    nan_columns,\n",
    "                ] = float(\"NaN\")\n",
    "            return plot_df\n",
    "\n",
    "        flow_stats = self.load_flow_stats(exp_name, hemo_type, filter_type, corr)\n",
    "        if dim.lower() == \"2d\":\n",
    "            source_detector_df = self.flow_session.create_source_detector_df(\"2D\")\n",
    "            plot_df = _add_missing_pos(dim)\n",
    "            fig = plt.figure(figsize=(6, 5))\n",
    "            ax = fig.add_subplot(111)\n",
    "            if corr:\n",
    "                sig_detector_plot_df = plot_df[plot_df[\"is_sig_corr\"] == True]\n",
    "                not_sig_detector_plot_df = plot_df[\n",
    "                    (plot_df[\"is_sig_corr\"] == False) | (pd.isna(plot_df[\"p_value\"]))\n",
    "                ]\n",
    "            else:\n",
    "                sig_detector_plot_df = plot_df[plot_df[\"is_sig\"] == True]\n",
    "                not_sig_detector_plot_df = plot_df[\n",
    "                    (plot_df[\"is_sig\"] == False) | (pd.isna(plot_df[\"p_value\"]))\n",
    "                ]\n",
    "            scatter = ax.scatter(\n",
    "                sig_detector_plot_df[\"detector_x_pos\"],\n",
    "                sig_detector_plot_df[\"detector_y_pos\"],\n",
    "                s=70,\n",
    "                c=sig_detector_plot_df[\"p_value\"],\n",
    "                cmap=\"autumn_r\",\n",
    "                edgecolors=\"black\",\n",
    "                alpha=1,\n",
    "                zorder=3,\n",
    "            )\n",
    "            ax.scatter(\n",
    "                not_sig_detector_plot_df[\"detector_x_pos\"],\n",
    "                not_sig_detector_plot_df[\"detector_y_pos\"],\n",
    "                s=20,\n",
    "                c=\"dodgerblue\",\n",
    "                edgecolors=\"black\",\n",
    "                alpha=1,\n",
    "                zorder=2,\n",
    "            )\n",
    "            ax.scatter(\n",
    "                plot_df[\"source_x_pos\"],\n",
    "                plot_df[\"source_y_pos\"],\n",
    "                s=30,\n",
    "                c=\"black\",\n",
    "                zorder=1,\n",
    "            )\n",
    "            if add_labels:\n",
    "                try:\n",
    "                    labels = [\n",
    "                        plt.text(\n",
    "                            sig_detector_plot_df[\"detector_x_pos\"].iloc[i],\n",
    "                            sig_detector_plot_df[\"detector_y_pos\"].iloc[i],\n",
    "                            int(sig_detector_plot_df[\"channel_num\"].iloc[i]),\n",
    "                            fontsize=8,\n",
    "                            ha=\"center\",\n",
    "                            va=\"center\",\n",
    "                            bbox=dict(\n",
    "                                boxstyle=\"round,pad=0.15\",\n",
    "                                edgecolor=\"black\",\n",
    "                                facecolor=\"white\",\n",
    "                                alpha=1,\n",
    "                            ),\n",
    "                            zorder=4,\n",
    "                        )\n",
    "                        for i in range(sig_detector_plot_df.shape[0])\n",
    "                    ]\n",
    "                    adjust_text(\n",
    "                        labels,\n",
    "                        ax=ax,\n",
    "                        arrowprops=dict(\n",
    "                            arrowstyle=\"-|>\",\n",
    "                            facecolor=\"black\",\n",
    "                            linewidth=2,\n",
    "                            shrinkA=0,\n",
    "                            shrinkB=0,\n",
    "                            zorder=0,\n",
    "                        ),\n",
    "                        expand_points=(4, 4),\n",
    "                        expand_text=(1.5, 1.5),\n",
    "                        force_points=(0.8, 0.8),\n",
    "                    )\n",
    "                except IndexError:  # no significant channels to label\n",
    "                    pass\n",
    "            ax.spines[\"top\"].set_visible(False)\n",
    "            ax.spines[\"right\"].set_visible(False)\n",
    "            ax.spines[\"bottom\"].set_visible(False)\n",
    "            ax.spines[\"left\"].set_visible(False)\n",
    "            ax.set_xticklabels([])\n",
    "            ax.set_yticklabels([])\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            ax.set_title(\"Anterior\", fontweight=\"bold\", fontsize=14, y=1)\n",
    "            ax.text(\n",
    "                0.5,\n",
    "                -0.06,\n",
    "                \"Posterior\",\n",
    "                fontweight=\"bold\",\n",
    "                fontsize=14,\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "                transform=ax.transAxes,\n",
    "            )\n",
    "            ax.text(\n",
    "                -0.02,\n",
    "                0.5,\n",
    "                \"Left\",\n",
    "                fontsize=14,\n",
    "                fontweight=\"bold\",\n",
    "                rotation=90,\n",
    "                va=\"center\",\n",
    "                ha=\"center\",\n",
    "                transform=ax.transAxes,\n",
    "            )\n",
    "            ax.text(\n",
    "                1.02,\n",
    "                0.5,\n",
    "                \"Right\",\n",
    "                fontsize=14,\n",
    "                fontweight=\"bold\",\n",
    "                rotation=90,\n",
    "                va=\"center\",\n",
    "                ha=\"center\",\n",
    "                transform=ax.transAxes,\n",
    "            )\n",
    "            font_props = FontProperties(size=12)\n",
    "            if corr:\n",
    "                alpha = round(plot_df[\"alpha_corr\"].iloc[0], 5)\n",
    "                scatter.set_clim([0, alpha])\n",
    "                colorbar = plt.colorbar(\n",
    "                    scatter, ticks=np.linspace(0, alpha, 6), shrink=0.7, pad=0.1\n",
    "                )\n",
    "                tick_labels = colorbar.get_ticks()\n",
    "                formatted_tick_labels = [format(tick, \".2e\") for tick in tick_labels]\n",
    "                colorbar.ax.set_yticklabels(formatted_tick_labels)\n",
    "                colorbar.set_label(\n",
    "                    \"Bonferroni-corrected p-value\", fontproperties=font_props\n",
    "                )\n",
    "            else:\n",
    "                scatter.set_clim([0, 0.05])\n",
    "                colorbar = plt.colorbar(\n",
    "                    scatter,\n",
    "                    ticks=[0, 0.01, 0.02, 0.03, 0.04, 0.05],\n",
    "                    shrink=0.7,\n",
    "                    pad=0.1,\n",
    "                )\n",
    "                colorbar.set_label(\"p-value\", fontproperties=font_props)\n",
    "            try:\n",
    "                title_text = f\"{exp_name_to_title(exp_name)} - {hemo_type} - {filter_type.title()}\"\n",
    "            except AttributeError:\n",
    "                title_text = f\"{exp_name_to_title(exp_name)} - {hemo_type} - Unfiltered\"\n",
    "            ax.text(\n",
    "                0.5,\n",
    "                1.12,\n",
    "                title_text,\n",
    "                fontweight=\"bold\",\n",
    "                fontsize=14,\n",
    "                ha=\"center\",\n",
    "                va=\"bottom\",\n",
    "                transform=ax.transAxes,\n",
    "            )\n",
    "            if show:  # TODO\n",
    "                plt.show()\n",
    "            if filepath:\n",
    "                fig.savefig(filepath, dpi=300, bbox_inches=\"tight\")\n",
    "        elif dim.lower() == \"3d\":\n",
    "            source_detector_df = self.flow_session.create_source_detector_df(\"3D\")\n",
    "            plot_df = _add_missing_pos(dim)\n",
    "            fig = plt.figure(figsize=[8, 8])\n",
    "            views = {\n",
    "                \"right\": {\"idx\": 1, \"azim\": 0},\n",
    "                \"left\": {\"idx\": 2, \"azim\": 180},\n",
    "                \"anterior\": {\"idx\": 3, \"azim\": 90},\n",
    "                \"posterior\": {\"idx\": 4, \"azim\": 270},\n",
    "            }\n",
    "            for view_name, view_info in views.items():\n",
    "                ax = fig.add_subplot(\n",
    "                    2, 2, view_info[\"idx\"], projection=\"3d\", computed_zorder=False\n",
    "                )\n",
    "                ax.view_init(elev=0, azim=view_info[\"azim\"])\n",
    "                if view_name == \"right\":\n",
    "                    source_plot_df = plot_df[plot_df[\"source_x_pos\"] >= 0]\n",
    "                    detector_plot_df = plot_df[plot_df[\"detector_x_pos\"] >= 0]\n",
    "                    ax.set_title(\"Right View\", fontweight=\"bold\", fontsize=14, y=0.85)\n",
    "                elif view_name == \"left\":\n",
    "                    source_plot_df = plot_df[plot_df[\"source_x_pos\"] <= 0]\n",
    "                    detector_plot_df = plot_df[plot_df[\"detector_x_pos\"] <= 0]\n",
    "                    ax.set_title(\"Left View\", fontweight=\"bold\", fontsize=14, y=0.85)\n",
    "                elif view_name == \"anterior\":\n",
    "                    source_plot_df = plot_df[plot_df[\"source_y_pos\"] > 0]\n",
    "                    detector_plot_df = plot_df[plot_df[\"detector_y_pos\"] > 0]\n",
    "                    ax.set_title(\n",
    "                        \"Anterior View\", fontweight=\"bold\", fontsize=14, y=0.85\n",
    "                    )\n",
    "                elif view_name == \"posterior\":\n",
    "                    source_plot_df = plot_df[plot_df[\"source_y_pos\"] <= 0]\n",
    "                    detector_plot_df = plot_df[plot_df[\"detector_y_pos\"] <= 0]\n",
    "                    ax.set_title(\n",
    "                        \"Posterior View\", fontweight=\"bold\", fontsize=14, y=0.85\n",
    "                    )\n",
    "                sig_detector_plot_df = detector_plot_df[\n",
    "                    detector_plot_df[\"p_value\"] <= 0.05\n",
    "                ]\n",
    "                if corr:\n",
    "                    sig_detector_plot_df = detector_plot_df[\n",
    "                        detector_plot_df[\"is_sig_corr\"] == True\n",
    "                    ]\n",
    "                    not_sig_detector_plot_df = detector_plot_df[\n",
    "                        (detector_plot_df[\"is_sig_corr\"] == False)\n",
    "                        | (pd.isna(detector_plot_df[\"p_value\"]))\n",
    "                    ]\n",
    "                else:\n",
    "                    sig_detector_plot_df = detector_plot_df[\n",
    "                        detector_plot_df[\"is_sig\"] == True\n",
    "                    ]\n",
    "                    not_sig_detector_plot_df = detector_plot_df[\n",
    "                        (detector_plot_df[\"is_sig\"] == False)\n",
    "                        | (pd.isna(detector_plot_df[\"p_value\"]))\n",
    "                    ]\n",
    "                scatter = ax.scatter(\n",
    "                    sig_detector_plot_df[\"detector_x_pos\"],\n",
    "                    sig_detector_plot_df[\"detector_y_pos\"],\n",
    "                    sig_detector_plot_df[\"detector_z_pos\"],\n",
    "                    s=70,\n",
    "                    c=sig_detector_plot_df[\"p_value\"],\n",
    "                    cmap=\"autumn_r\",\n",
    "                    edgecolors=\"black\",\n",
    "                    alpha=1,\n",
    "                    zorder=3,\n",
    "                )\n",
    "                ax.scatter(\n",
    "                    not_sig_detector_plot_df[\"detector_x_pos\"],\n",
    "                    not_sig_detector_plot_df[\"detector_y_pos\"],\n",
    "                    not_sig_detector_plot_df[\"detector_z_pos\"],\n",
    "                    s=20,\n",
    "                    c=\"dodgerblue\",\n",
    "                    edgecolors=\"black\",\n",
    "                    alpha=1,\n",
    "                    zorder=2,\n",
    "                )\n",
    "                ax.scatter(\n",
    "                    source_plot_df[\"source_x_pos\"],\n",
    "                    source_plot_df[\"source_y_pos\"],\n",
    "                    source_plot_df[\"source_z_pos\"],\n",
    "                    s=30,\n",
    "                    c=\"black\",\n",
    "                    zorder=1,\n",
    "                )\n",
    "                ax.patch.set_alpha(0.0)\n",
    "                ax.xaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "                ax.xaxis.line.set_color(\"none\")\n",
    "                ax.yaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "                ax.yaxis.line.set_color(\"none\")\n",
    "                ax.zaxis.set_pane_color((1.0, 1.0, 1.0, 0.0))\n",
    "                ax.zaxis.line.set_color(\"none\")\n",
    "                ax.grid(False)\n",
    "                ax.set_xticklabels([])\n",
    "                ax.set_yticklabels([])\n",
    "                ax.set_zticklabels([])\n",
    "                ax.set_xticks([])\n",
    "                ax.set_yticks([])\n",
    "                ax.set_zticks([])\n",
    "            sm = plt.cm.ScalarMappable(\n",
    "                cmap=\"autumn_r\", norm=plt.Normalize(vmin=0, vmax=0.05)\n",
    "            )\n",
    "            sm.set_array([])\n",
    "            colorbar_ax = fig.add_axes([0.87, 0.32, 0.017, 0.4])\n",
    "            font_props = FontProperties(size=12)\n",
    "            if corr:\n",
    "                alpha = round(plot_df[\"alpha_corr\"].iloc[0], 5)\n",
    "                scatter.set_clim([0, alpha])\n",
    "                colorbar = fig.colorbar(\n",
    "                    scatter,\n",
    "                    cax=colorbar_ax,\n",
    "                    ticks=np.linspace(0, alpha, 6),\n",
    "                    shrink=0.7,\n",
    "                    pad=0.1,\n",
    "                )\n",
    "                tick_labels = colorbar.get_ticks()\n",
    "                formatted_tick_labels = [format(tick, \".2e\") for tick in tick_labels]\n",
    "                colorbar.ax.set_yticklabels(formatted_tick_labels)\n",
    "                colorbar.set_label(\n",
    "                    \"Bonferroni-corrected p-value\", fontproperties=font_props\n",
    "                )\n",
    "            else:\n",
    "                scatter.set_clim([0, 0.05])\n",
    "                colorbar = fig.colorbar(\n",
    "                    scatter,\n",
    "                    cax=colorbar_ax,\n",
    "                    ticks=[0, 0.01, 0.02, 0.03, 0.04, 0.05],\n",
    "                    shrink=0.7,\n",
    "                    pad=0.1,\n",
    "                )\n",
    "                colorbar.set_label(\"p-value\", fontproperties=font_props)\n",
    "            plt.subplots_adjust(wspace=-0.3, hspace=-0.4)\n",
    "            if show:  # TODO\n",
    "                plt.show()\n",
    "            if filepath:\n",
    "                fig.savefig(filepath, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "    def create_stat_results_figs(\n",
    "        self, overwrite: bool = True, corr: bool = False\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Create figures (.png images) for each experiment, hemodynamic type, and filter type.\n",
    "        There are individual figures for each filter type and a combined figure that has all filter types.\n",
    "        These figures are saved in the corresponding results directory.\n",
    "\n",
    "        Args:\n",
    "            overwrite (bool): Overwrite existing filter figures. Significant performance increase when False.\n",
    "                              Defaults to True.\n",
    "            corr (bool): Apply a Bonferroni correction to the p-values. Defaults to False.\n",
    "        \"\"\"\n",
    "\n",
    "        def _combine_figs(filedir: str) -> None:\n",
    "            \"\"\"\n",
    "            Combine three individual filter figures into one figure.\n",
    "\n",
    "            Args:\n",
    "                filedir (str): Directory of an experiment hemodynamic type.\n",
    "            \"\"\"\n",
    "            all_filenames = os.listdir(filedir)\n",
    "            all_fig_filenames = [f for f in all_filenames if not f.endswith(\".csv\")]\n",
    "            if corr:\n",
    "                all_fig_filenames = [f for f in all_fig_filenames if \"corr\" in f]\n",
    "            else:\n",
    "                all_fig_filenames = [f for f in all_fig_filenames if \"corr\" not in f]\n",
    "            order = [\"unfiltered\", \"lowpass\", \"bandpass\"]\n",
    "            fig_filenames = sorted(\n",
    "                [f for f in all_fig_filenames if any(o in f for o in order)],\n",
    "                key=lambda f: next(i for i, o in enumerate(order) if o in f),\n",
    "            )\n",
    "            figs = [\n",
    "                Image.open(os.path.join(filedir, fig_name))\n",
    "                for fig_name in fig_filenames\n",
    "            ]\n",
    "            widths, heights = zip(*(fig.size for fig in figs))\n",
    "            total_width = sum(widths)\n",
    "            max_height = max(heights)\n",
    "            fig_out = Image.new(\"RGB\", (total_width, max_height))\n",
    "            x_offset = 0\n",
    "            for fig in figs:\n",
    "                fig_out.paste(fig, (x_offset, 0))\n",
    "                x_offset += fig.size[0]\n",
    "            if corr:\n",
    "                filename = fig_filenames[0].rpartition(\"_\")[0] + \"_all_corr.png\"\n",
    "            else:\n",
    "                filename = fig_filenames[0].rpartition(\"_\")[0] + \"_all.png\"\n",
    "            fig_out.save(os.path.join(filedir, filename))\n",
    "\n",
    "        filter_types = [\"unfiltered\", \"lowpass\", \"bandpass\"]\n",
    "        for exp_name in [\n",
    "            \"go_no_go\",\n",
    "            \"king_devick\",\n",
    "            \"n_back\",\n",
    "            \"tower_of_london\",\n",
    "            \"vSAT\",\n",
    "        ]:\n",
    "            for hemo_type in self.hemo_types:\n",
    "                for filter_type in filter_types:\n",
    "                    filedir = os.path.join(\n",
    "                        self.results_dir,\n",
    "                        \"inter_module_channels\",\n",
    "                        exp_name,\n",
    "                        hemo_type,\n",
    "                        \"figures\",\n",
    "                    )\n",
    "                    if not os.path.exists(filedir):\n",
    "                        os.makedirs(filedir)\n",
    "                    if corr:\n",
    "                        filename = f\"{exp_name}_{hemo_type}_{filter_type}_corr.png\"\n",
    "                    else:\n",
    "                        filename = f\"{exp_name}_{hemo_type}_{filter_type}.png\"\n",
    "                    filepath = os.path.join(filedir, filename)\n",
    "                    if not os.path.exists(filepath) or overwrite:\n",
    "                        out = self.plot_stat_results(\n",
    "                            dim=\"2D\",\n",
    "                            exp_name=exp_name,\n",
    "                            hemo_type=hemo_type,\n",
    "                            filter_type=filter_type,\n",
    "                            corr=corr,\n",
    "                            add_labels=True,\n",
    "                            filepath=filepath,\n",
    "                            show=False,\n",
    "                        )\n",
    "                _combine_figs(filedir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# par = Participant_Flow(1)\n",
    "# par.create_exp_stim_response_df(\"n_back\", filter_type=\"lowpass\", hemo_type=\"HbDiff\")\n",
    "FR = Flow_Results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_num</th>\n",
       "      <th>alpha_corr</th>\n",
       "      <th>p_value</th>\n",
       "      <th>F_value</th>\n",
       "      <th>df1</th>\n",
       "      <th>df2</th>\n",
       "      <th>is_sig_corr</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.183353</td>\n",
       "      <td>1.803392</td>\n",
       "      <td>2.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.137221</td>\n",
       "      <td>2.145915</td>\n",
       "      <td>2.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.324674</td>\n",
       "      <td>1.175041</td>\n",
       "      <td>2.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.475035</td>\n",
       "      <td>0.764510</td>\n",
       "      <td>2.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.246394</td>\n",
       "      <td>1.473301</td>\n",
       "      <td>2.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2141</th>\n",
       "      <td>4282</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.074418</td>\n",
       "      <td>2.900749</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2142</th>\n",
       "      <td>4284</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.376805</td>\n",
       "      <td>1.025248</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2143</th>\n",
       "      <td>4286</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.724955</td>\n",
       "      <td>0.326875</td>\n",
       "      <td>2.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2144</th>\n",
       "      <td>4288</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.810335</td>\n",
       "      <td>0.212330</td>\n",
       "      <td>2.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2145</th>\n",
       "      <td>4290</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.453930</td>\n",
       "      <td>0.818857</td>\n",
       "      <td>2.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2146 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      channel_num  alpha_corr   p_value   F_value  df1   df2  is_sig_corr\n",
       "0               0    0.000023  0.183353  1.803392  2.0  28.0        False\n",
       "1               2    0.000023  0.137221  2.145915  2.0  26.0        False\n",
       "2               4    0.000023  0.324674  1.175041  2.0  26.0        False\n",
       "3               6    0.000023  0.475035  0.764510  2.0  28.0        False\n",
       "4               8    0.000023  0.246394  1.473301  2.0  28.0        False\n",
       "...           ...         ...       ...       ...  ...   ...          ...\n",
       "2141         4282    0.000023  0.074418  2.900749  2.0  24.0        False\n",
       "2142         4284    0.000023  0.376805  1.025248  2.0  20.0        False\n",
       "2143         4286    0.000023  0.724955  0.326875  2.0  20.0        False\n",
       "2144         4288    0.000023  0.810335  0.212330  2.0  22.0        False\n",
       "2145         4290    0.000023  0.453930  0.818857  2.0  22.0        False\n",
       "\n",
       "[2146 rows x 7 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_name = \"n_back\" \n",
    "hemo_type = \"HbO\" \n",
    "filter_type = \"lowpass\"\n",
    "inter_module_only = False\n",
    "FR.load_flow_stats(exp_name, hemo_type, filter_type, inter_module_only=inter_module_only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_num</th>\n",
       "      <th>measurement_list_index</th>\n",
       "      <th>data_type</th>\n",
       "      <th>data_type_index</th>\n",
       "      <th>detector_index</th>\n",
       "      <th>source_index</th>\n",
       "      <th>source_label</th>\n",
       "      <th>source_x_pos</th>\n",
       "      <th>source_y_pos</th>\n",
       "      <th>detector_label</th>\n",
       "      <th>detector_x_pos</th>\n",
       "      <th>detector_y_pos</th>\n",
       "      <th>midpoint_x_pos</th>\n",
       "      <th>midpoint_y_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>HbO</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>S01</td>\n",
       "      <td>-0.210040</td>\n",
       "      <td>0.896057</td>\n",
       "      <td>D01d0</td>\n",
       "      <td>-0.150029</td>\n",
       "      <td>0.930705</td>\n",
       "      <td>-0.180035</td>\n",
       "      <td>0.913381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>99999</td>\n",
       "      <td>HbR</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>S01</td>\n",
       "      <td>-0.210040</td>\n",
       "      <td>0.896057</td>\n",
       "      <td>D01d0</td>\n",
       "      <td>-0.150029</td>\n",
       "      <td>0.930705</td>\n",
       "      <td>-0.180035</td>\n",
       "      <td>0.913381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>99999</td>\n",
       "      <td>HbO</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>S01</td>\n",
       "      <td>-0.210040</td>\n",
       "      <td>0.896057</td>\n",
       "      <td>D01d1</td>\n",
       "      <td>-0.210040</td>\n",
       "      <td>0.965352</td>\n",
       "      <td>-0.210040</td>\n",
       "      <td>0.930705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>99999</td>\n",
       "      <td>HbR</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>S01</td>\n",
       "      <td>-0.210040</td>\n",
       "      <td>0.896057</td>\n",
       "      <td>D01d1</td>\n",
       "      <td>-0.210040</td>\n",
       "      <td>0.965352</td>\n",
       "      <td>-0.210040</td>\n",
       "      <td>0.930705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>99999</td>\n",
       "      <td>HbO</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>S01</td>\n",
       "      <td>-0.210040</td>\n",
       "      <td>0.896057</td>\n",
       "      <td>D01d2</td>\n",
       "      <td>-0.270052</td>\n",
       "      <td>0.930705</td>\n",
       "      <td>-0.240046</td>\n",
       "      <td>0.913381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3845</th>\n",
       "      <td>4287</td>\n",
       "      <td>4288</td>\n",
       "      <td>99999</td>\n",
       "      <td>HbR</td>\n",
       "      <td>304</td>\n",
       "      <td>51</td>\n",
       "      <td>S51</td>\n",
       "      <td>-0.450087</td>\n",
       "      <td>-0.861409</td>\n",
       "      <td>D51d3</td>\n",
       "      <td>-0.390075</td>\n",
       "      <td>-0.896057</td>\n",
       "      <td>-0.420081</td>\n",
       "      <td>-0.878733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3858</th>\n",
       "      <td>4288</td>\n",
       "      <td>4289</td>\n",
       "      <td>99999</td>\n",
       "      <td>HbO</td>\n",
       "      <td>305</td>\n",
       "      <td>51</td>\n",
       "      <td>S51</td>\n",
       "      <td>-0.450087</td>\n",
       "      <td>-0.861409</td>\n",
       "      <td>D51d4</td>\n",
       "      <td>-0.390075</td>\n",
       "      <td>-0.826762</td>\n",
       "      <td>-0.420081</td>\n",
       "      <td>-0.844085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3859</th>\n",
       "      <td>4289</td>\n",
       "      <td>4290</td>\n",
       "      <td>99999</td>\n",
       "      <td>HbR</td>\n",
       "      <td>305</td>\n",
       "      <td>51</td>\n",
       "      <td>S51</td>\n",
       "      <td>-0.450087</td>\n",
       "      <td>-0.861409</td>\n",
       "      <td>D51d4</td>\n",
       "      <td>-0.390075</td>\n",
       "      <td>-0.826762</td>\n",
       "      <td>-0.420081</td>\n",
       "      <td>-0.844085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3870</th>\n",
       "      <td>4290</td>\n",
       "      <td>4291</td>\n",
       "      <td>99999</td>\n",
       "      <td>HbO</td>\n",
       "      <td>306</td>\n",
       "      <td>51</td>\n",
       "      <td>S51</td>\n",
       "      <td>-0.450087</td>\n",
       "      <td>-0.861409</td>\n",
       "      <td>D51d5</td>\n",
       "      <td>-0.450087</td>\n",
       "      <td>-0.792114</td>\n",
       "      <td>-0.450087</td>\n",
       "      <td>-0.826762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3871</th>\n",
       "      <td>4291</td>\n",
       "      <td>4292</td>\n",
       "      <td>99999</td>\n",
       "      <td>HbR</td>\n",
       "      <td>306</td>\n",
       "      <td>51</td>\n",
       "      <td>S51</td>\n",
       "      <td>-0.450087</td>\n",
       "      <td>-0.861409</td>\n",
       "      <td>D51d5</td>\n",
       "      <td>-0.450087</td>\n",
       "      <td>-0.792114</td>\n",
       "      <td>-0.450087</td>\n",
       "      <td>-0.826762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4292 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      channel_num  measurement_list_index  data_type data_type_index  \\\n",
       "0               0                       1      99999             HbO   \n",
       "1               1                       2      99999             HbR   \n",
       "8               2                       3      99999             HbO   \n",
       "9               3                       4      99999             HbR   \n",
       "14              4                       5      99999             HbO   \n",
       "...           ...                     ...        ...             ...   \n",
       "3845         4287                    4288      99999             HbR   \n",
       "3858         4288                    4289      99999             HbO   \n",
       "3859         4289                    4290      99999             HbR   \n",
       "3870         4290                    4291      99999             HbO   \n",
       "3871         4291                    4292      99999             HbR   \n",
       "\n",
       "      detector_index  source_index source_label  source_x_pos  source_y_pos  \\\n",
       "0                  1             1          S01     -0.210040      0.896057   \n",
       "1                  1             1          S01     -0.210040      0.896057   \n",
       "8                  2             1          S01     -0.210040      0.896057   \n",
       "9                  2             1          S01     -0.210040      0.896057   \n",
       "14                 3             1          S01     -0.210040      0.896057   \n",
       "...              ...           ...          ...           ...           ...   \n",
       "3845             304            51          S51     -0.450087     -0.861409   \n",
       "3858             305            51          S51     -0.450087     -0.861409   \n",
       "3859             305            51          S51     -0.450087     -0.861409   \n",
       "3870             306            51          S51     -0.450087     -0.861409   \n",
       "3871             306            51          S51     -0.450087     -0.861409   \n",
       "\n",
       "     detector_label  detector_x_pos  detector_y_pos  midpoint_x_pos  \\\n",
       "0             D01d0       -0.150029        0.930705       -0.180035   \n",
       "1             D01d0       -0.150029        0.930705       -0.180035   \n",
       "8             D01d1       -0.210040        0.965352       -0.210040   \n",
       "9             D01d1       -0.210040        0.965352       -0.210040   \n",
       "14            D01d2       -0.270052        0.930705       -0.240046   \n",
       "...             ...             ...             ...             ...   \n",
       "3845          D51d3       -0.390075       -0.896057       -0.420081   \n",
       "3858          D51d4       -0.390075       -0.826762       -0.420081   \n",
       "3859          D51d4       -0.390075       -0.826762       -0.420081   \n",
       "3870          D51d5       -0.450087       -0.792114       -0.450087   \n",
       "3871          D51d5       -0.450087       -0.792114       -0.450087   \n",
       "\n",
       "      midpoint_y_pos  \n",
       "0           0.913381  \n",
       "1           0.913381  \n",
       "8           0.930705  \n",
       "9           0.930705  \n",
       "14          0.913381  \n",
       "...              ...  \n",
       "3845       -0.878733  \n",
       "3858       -0.844085  \n",
       "3859       -0.844085  \n",
       "3870       -0.826762  \n",
       "3871       -0.826762  \n",
       "\n",
       "[4292 rows x 14 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow = Participant_Flow().flow\n",
    "sd2 = flow.create_source_detector_df2()\n",
    "sd2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(sd2[\"midpoint_x_pos\"], sd2[\"midpoint_y_pos\"], s=10, c=\"black\")\n",
    "ax.scatter(sd2[\"source_x_pos\"], sd2[\"source_y_pos\"], s=40, c=\"orange\")\n",
    "ax.scatter(sd2[\"detector_x_pos\"], sd2[\"detector_y_pos\"], s=40, c=\"blue\")\n",
    "plt.show()\n",
    "# ax.scatter(x_source, y_source, s=70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>measurement_list_index</th>\n",
       "      <th>data_type</th>\n",
       "      <th>data_type_index</th>\n",
       "      <th>detector_index</th>\n",
       "      <th>source_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>HbO</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>99999</td>\n",
       "      <td>HbR</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>99999</td>\n",
       "      <td>HbO</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>99999</td>\n",
       "      <td>HbR</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>99999</td>\n",
       "      <td>HbO</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4287</th>\n",
       "      <td>4288</td>\n",
       "      <td>99999</td>\n",
       "      <td>HbR</td>\n",
       "      <td>304</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4288</th>\n",
       "      <td>4289</td>\n",
       "      <td>99999</td>\n",
       "      <td>HbO</td>\n",
       "      <td>305</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4289</th>\n",
       "      <td>4290</td>\n",
       "      <td>99999</td>\n",
       "      <td>HbR</td>\n",
       "      <td>305</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4290</th>\n",
       "      <td>4291</td>\n",
       "      <td>99999</td>\n",
       "      <td>HbO</td>\n",
       "      <td>306</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4291</th>\n",
       "      <td>4292</td>\n",
       "      <td>99999</td>\n",
       "      <td>HbR</td>\n",
       "      <td>306</td>\n",
       "      <td>51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4292 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      measurement_list_index  data_type data_type_index  detector_index  \\\n",
       "0                          1      99999             HbO               1   \n",
       "1                          2      99999             HbR               1   \n",
       "2                          3      99999             HbO               2   \n",
       "3                          4      99999             HbR               2   \n",
       "4                          5      99999             HbO               3   \n",
       "...                      ...        ...             ...             ...   \n",
       "4287                    4288      99999             HbR             304   \n",
       "4288                    4289      99999             HbO             305   \n",
       "4289                    4290      99999             HbR             305   \n",
       "4290                    4291      99999             HbO             306   \n",
       "4291                    4292      99999             HbR             306   \n",
       "\n",
       "      source_index  \n",
       "0                1  \n",
       "1                1  \n",
       "2                1  \n",
       "3                1  \n",
       "4                1  \n",
       "...            ...  \n",
       "4287            51  \n",
       "4288            51  \n",
       "4289            51  \n",
       "4290            51  \n",
       "4291            51  \n",
       "\n",
       "[4292 rows x 5 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow.create_measurement_list_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>detector_label</th>\n",
       "      <th>detector_index</th>\n",
       "      <th>source_index</th>\n",
       "      <th>detector_x_pos</th>\n",
       "      <th>detector_y_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D01d0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.150029</td>\n",
       "      <td>0.930705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D01d1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.210040</td>\n",
       "      <td>0.965352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D01d2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.270052</td>\n",
       "      <td>0.930705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D01d3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.270052</td>\n",
       "      <td>0.861409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D01d4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.210040</td>\n",
       "      <td>0.826762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>D51d1</td>\n",
       "      <td>302</td>\n",
       "      <td>51</td>\n",
       "      <td>-0.510098</td>\n",
       "      <td>-0.896057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>D51d2</td>\n",
       "      <td>303</td>\n",
       "      <td>51</td>\n",
       "      <td>-0.450087</td>\n",
       "      <td>-0.930705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>303</th>\n",
       "      <td>D51d3</td>\n",
       "      <td>304</td>\n",
       "      <td>51</td>\n",
       "      <td>-0.390075</td>\n",
       "      <td>-0.896057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>304</th>\n",
       "      <td>D51d4</td>\n",
       "      <td>305</td>\n",
       "      <td>51</td>\n",
       "      <td>-0.390075</td>\n",
       "      <td>-0.826762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>305</th>\n",
       "      <td>D51d5</td>\n",
       "      <td>306</td>\n",
       "      <td>51</td>\n",
       "      <td>-0.450087</td>\n",
       "      <td>-0.792114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>306 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    detector_label  detector_index  source_index  detector_x_pos  \\\n",
       "0            D01d0               1             1       -0.150029   \n",
       "1            D01d1               2             1       -0.210040   \n",
       "2            D01d2               3             1       -0.270052   \n",
       "3            D01d3               4             1       -0.270052   \n",
       "4            D01d4               5             1       -0.210040   \n",
       "..             ...             ...           ...             ...   \n",
       "301          D51d1             302            51       -0.510098   \n",
       "302          D51d2             303            51       -0.450087   \n",
       "303          D51d3             304            51       -0.390075   \n",
       "304          D51d4             305            51       -0.390075   \n",
       "305          D51d5             306            51       -0.450087   \n",
       "\n",
       "     detector_y_pos  \n",
       "0          0.930705  \n",
       "1          0.965352  \n",
       "2          0.930705  \n",
       "3          0.861409  \n",
       "4          0.826762  \n",
       "..              ...  \n",
       "301       -0.896057  \n",
       "302       -0.930705  \n",
       "303       -0.896057  \n",
       "304       -0.826762  \n",
       "305       -0.792114  \n",
       "\n",
       "[306 rows x 5 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow.create_source_df(\"2D\")[0:5]\n",
    "flow.create_detector_df(\"2D\")\n",
    "# print(flow.create_source_df(\"2D\")[0:5].to_string(index=False), \"\\n\")\n",
    "# print(flow.create_detector_df(\"2D\")[0:5].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_num</th>\n",
       "      <th>measurement_list_index</th>\n",
       "      <th>data_type</th>\n",
       "      <th>data_type_index</th>\n",
       "      <th>detector_index</th>\n",
       "      <th>source_index</th>\n",
       "      <th>source_label</th>\n",
       "      <th>source_x_pos</th>\n",
       "      <th>source_y_pos</th>\n",
       "      <th>detector_label</th>\n",
       "      <th>detector_x_pos</th>\n",
       "      <th>detector_y_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>HbO</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>S01</td>\n",
       "      <td>-0.210040</td>\n",
       "      <td>0.896057</td>\n",
       "      <td>D01d0</td>\n",
       "      <td>-0.150029</td>\n",
       "      <td>0.930705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>99999</td>\n",
       "      <td>HbR</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>S01</td>\n",
       "      <td>-0.210040</td>\n",
       "      <td>0.896057</td>\n",
       "      <td>D01d0</td>\n",
       "      <td>-0.150029</td>\n",
       "      <td>0.930705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>99999</td>\n",
       "      <td>HbO</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>S01</td>\n",
       "      <td>-0.210040</td>\n",
       "      <td>0.896057</td>\n",
       "      <td>D01d1</td>\n",
       "      <td>-0.210040</td>\n",
       "      <td>0.965352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>99999</td>\n",
       "      <td>HbR</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>S01</td>\n",
       "      <td>-0.210040</td>\n",
       "      <td>0.896057</td>\n",
       "      <td>D01d1</td>\n",
       "      <td>-0.210040</td>\n",
       "      <td>0.965352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>99999</td>\n",
       "      <td>HbO</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>S01</td>\n",
       "      <td>-0.210040</td>\n",
       "      <td>0.896057</td>\n",
       "      <td>D01d2</td>\n",
       "      <td>-0.270052</td>\n",
       "      <td>0.930705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>607</th>\n",
       "      <td>4287</td>\n",
       "      <td>4288</td>\n",
       "      <td>99999</td>\n",
       "      <td>HbR</td>\n",
       "      <td>304</td>\n",
       "      <td>51</td>\n",
       "      <td>S51</td>\n",
       "      <td>-0.450087</td>\n",
       "      <td>-0.861409</td>\n",
       "      <td>D51d3</td>\n",
       "      <td>-0.390075</td>\n",
       "      <td>-0.896057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>608</th>\n",
       "      <td>4288</td>\n",
       "      <td>4289</td>\n",
       "      <td>99999</td>\n",
       "      <td>HbO</td>\n",
       "      <td>305</td>\n",
       "      <td>51</td>\n",
       "      <td>S51</td>\n",
       "      <td>-0.450087</td>\n",
       "      <td>-0.861409</td>\n",
       "      <td>D51d4</td>\n",
       "      <td>-0.390075</td>\n",
       "      <td>-0.826762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>609</th>\n",
       "      <td>4289</td>\n",
       "      <td>4290</td>\n",
       "      <td>99999</td>\n",
       "      <td>HbR</td>\n",
       "      <td>305</td>\n",
       "      <td>51</td>\n",
       "      <td>S51</td>\n",
       "      <td>-0.450087</td>\n",
       "      <td>-0.861409</td>\n",
       "      <td>D51d4</td>\n",
       "      <td>-0.390075</td>\n",
       "      <td>-0.826762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>610</th>\n",
       "      <td>4290</td>\n",
       "      <td>4291</td>\n",
       "      <td>99999</td>\n",
       "      <td>HbO</td>\n",
       "      <td>306</td>\n",
       "      <td>51</td>\n",
       "      <td>S51</td>\n",
       "      <td>-0.450087</td>\n",
       "      <td>-0.861409</td>\n",
       "      <td>D51d5</td>\n",
       "      <td>-0.450087</td>\n",
       "      <td>-0.792114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>4291</td>\n",
       "      <td>4292</td>\n",
       "      <td>99999</td>\n",
       "      <td>HbR</td>\n",
       "      <td>306</td>\n",
       "      <td>51</td>\n",
       "      <td>S51</td>\n",
       "      <td>-0.450087</td>\n",
       "      <td>-0.861409</td>\n",
       "      <td>D51d5</td>\n",
       "      <td>-0.450087</td>\n",
       "      <td>-0.792114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>612 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     channel_num  measurement_list_index  data_type data_type_index  \\\n",
       "0              0                       1      99999             HbO   \n",
       "1              1                       2      99999             HbR   \n",
       "2              2                       3      99999             HbO   \n",
       "3              3                       4      99999             HbR   \n",
       "4              4                       5      99999             HbO   \n",
       "..           ...                     ...        ...             ...   \n",
       "607         4287                    4288      99999             HbR   \n",
       "608         4288                    4289      99999             HbO   \n",
       "609         4289                    4290      99999             HbR   \n",
       "610         4290                    4291      99999             HbO   \n",
       "611         4291                    4292      99999             HbR   \n",
       "\n",
       "     detector_index  source_index source_label  source_x_pos  source_y_pos  \\\n",
       "0                 1             1          S01     -0.210040      0.896057   \n",
       "1                 1             1          S01     -0.210040      0.896057   \n",
       "2                 2             1          S01     -0.210040      0.896057   \n",
       "3                 2             1          S01     -0.210040      0.896057   \n",
       "4                 3             1          S01     -0.210040      0.896057   \n",
       "..              ...           ...          ...           ...           ...   \n",
       "607             304            51          S51     -0.450087     -0.861409   \n",
       "608             305            51          S51     -0.450087     -0.861409   \n",
       "609             305            51          S51     -0.450087     -0.861409   \n",
       "610             306            51          S51     -0.450087     -0.861409   \n",
       "611             306            51          S51     -0.450087     -0.861409   \n",
       "\n",
       "    detector_label  detector_x_pos  detector_y_pos  \n",
       "0            D01d0       -0.150029        0.930705  \n",
       "1            D01d0       -0.150029        0.930705  \n",
       "2            D01d1       -0.210040        0.965352  \n",
       "3            D01d1       -0.210040        0.965352  \n",
       "4            D01d2       -0.270052        0.930705  \n",
       "..             ...             ...             ...  \n",
       "607          D51d3       -0.390075       -0.896057  \n",
       "608          D51d4       -0.390075       -0.826762  \n",
       "609          D51d4       -0.390075       -0.826762  \n",
       "610          D51d5       -0.450087       -0.792114  \n",
       "611          D51d5       -0.450087       -0.792114  \n",
       "\n",
       "[612 rows x 12 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flow.create_source_detector_df(\"2D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>channel_num</th>\n",
       "      <th>measurement_list_index</th>\n",
       "      <th>data_type</th>\n",
       "      <th>data_type_index</th>\n",
       "      <th>detector_index</th>\n",
       "      <th>source_index</th>\n",
       "      <th>source_label</th>\n",
       "      <th>source_x_pos</th>\n",
       "      <th>source_y_pos</th>\n",
       "      <th>detector_label</th>\n",
       "      <th>detector_x_pos</th>\n",
       "      <th>detector_y_pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>99999</td>\n",
       "      <td>HbO</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>S01</td>\n",
       "      <td>-0.210040</td>\n",
       "      <td>0.896057</td>\n",
       "      <td>D01d0</td>\n",
       "      <td>-0.150029</td>\n",
       "      <td>0.930705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>99999</td>\n",
       "      <td>HbR</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>S01</td>\n",
       "      <td>-0.210040</td>\n",
       "      <td>0.896057</td>\n",
       "      <td>D01d0</td>\n",
       "      <td>-0.150029</td>\n",
       "      <td>0.930705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>99999</td>\n",
       "      <td>HbO</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>S01</td>\n",
       "      <td>-0.210040</td>\n",
       "      <td>0.896057</td>\n",
       "      <td>D01d1</td>\n",
       "      <td>-0.210040</td>\n",
       "      <td>0.965352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>99999</td>\n",
       "      <td>HbR</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>S01</td>\n",
       "      <td>-0.210040</td>\n",
       "      <td>0.896057</td>\n",
       "      <td>D01d1</td>\n",
       "      <td>-0.210040</td>\n",
       "      <td>0.965352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>99999</td>\n",
       "      <td>HbO</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>S01</td>\n",
       "      <td>-0.210040</td>\n",
       "      <td>0.896057</td>\n",
       "      <td>D01d2</td>\n",
       "      <td>-0.270052</td>\n",
       "      <td>0.930705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3845</th>\n",
       "      <td>4287</td>\n",
       "      <td>4288</td>\n",
       "      <td>99999</td>\n",
       "      <td>HbR</td>\n",
       "      <td>304</td>\n",
       "      <td>51</td>\n",
       "      <td>S51</td>\n",
       "      <td>-0.450087</td>\n",
       "      <td>-0.861409</td>\n",
       "      <td>D51d3</td>\n",
       "      <td>-0.390075</td>\n",
       "      <td>-0.896057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3858</th>\n",
       "      <td>4288</td>\n",
       "      <td>4289</td>\n",
       "      <td>99999</td>\n",
       "      <td>HbO</td>\n",
       "      <td>305</td>\n",
       "      <td>51</td>\n",
       "      <td>S51</td>\n",
       "      <td>-0.450087</td>\n",
       "      <td>-0.861409</td>\n",
       "      <td>D51d4</td>\n",
       "      <td>-0.390075</td>\n",
       "      <td>-0.826762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3859</th>\n",
       "      <td>4289</td>\n",
       "      <td>4290</td>\n",
       "      <td>99999</td>\n",
       "      <td>HbR</td>\n",
       "      <td>305</td>\n",
       "      <td>51</td>\n",
       "      <td>S51</td>\n",
       "      <td>-0.450087</td>\n",
       "      <td>-0.861409</td>\n",
       "      <td>D51d4</td>\n",
       "      <td>-0.390075</td>\n",
       "      <td>-0.826762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3870</th>\n",
       "      <td>4290</td>\n",
       "      <td>4291</td>\n",
       "      <td>99999</td>\n",
       "      <td>HbO</td>\n",
       "      <td>306</td>\n",
       "      <td>51</td>\n",
       "      <td>S51</td>\n",
       "      <td>-0.450087</td>\n",
       "      <td>-0.861409</td>\n",
       "      <td>D51d5</td>\n",
       "      <td>-0.450087</td>\n",
       "      <td>-0.792114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3871</th>\n",
       "      <td>4291</td>\n",
       "      <td>4292</td>\n",
       "      <td>99999</td>\n",
       "      <td>HbR</td>\n",
       "      <td>306</td>\n",
       "      <td>51</td>\n",
       "      <td>S51</td>\n",
       "      <td>-0.450087</td>\n",
       "      <td>-0.861409</td>\n",
       "      <td>D51d5</td>\n",
       "      <td>-0.450087</td>\n",
       "      <td>-0.792114</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4292 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      channel_num  measurement_list_index  data_type data_type_index  \\\n",
       "0               0                       1      99999             HbO   \n",
       "1               1                       2      99999             HbR   \n",
       "8               2                       3      99999             HbO   \n",
       "9               3                       4      99999             HbR   \n",
       "14              4                       5      99999             HbO   \n",
       "...           ...                     ...        ...             ...   \n",
       "3845         4287                    4288      99999             HbR   \n",
       "3858         4288                    4289      99999             HbO   \n",
       "3859         4289                    4290      99999             HbR   \n",
       "3870         4290                    4291      99999             HbO   \n",
       "3871         4291                    4292      99999             HbR   \n",
       "\n",
       "      detector_index  source_index source_label  source_x_pos  source_y_pos  \\\n",
       "0                  1             1          S01     -0.210040      0.896057   \n",
       "1                  1             1          S01     -0.210040      0.896057   \n",
       "8                  2             1          S01     -0.210040      0.896057   \n",
       "9                  2             1          S01     -0.210040      0.896057   \n",
       "14                 3             1          S01     -0.210040      0.896057   \n",
       "...              ...           ...          ...           ...           ...   \n",
       "3845             304            51          S51     -0.450087     -0.861409   \n",
       "3858             305            51          S51     -0.450087     -0.861409   \n",
       "3859             305            51          S51     -0.450087     -0.861409   \n",
       "3870             306            51          S51     -0.450087     -0.861409   \n",
       "3871             306            51          S51     -0.450087     -0.861409   \n",
       "\n",
       "     detector_label  detector_x_pos  detector_y_pos  \n",
       "0             D01d0       -0.150029        0.930705  \n",
       "1             D01d0       -0.150029        0.930705  \n",
       "8             D01d1       -0.210040        0.965352  \n",
       "9             D01d1       -0.210040        0.965352  \n",
       "14            D01d2       -0.270052        0.930705  \n",
       "...             ...             ...             ...  \n",
       "3845          D51d3       -0.390075       -0.896057  \n",
       "3858          D51d4       -0.390075       -0.826762  \n",
       "3859          D51d4       -0.390075       -0.826762  \n",
       "3870          D51d5       -0.450087       -0.792114  \n",
       "3871          D51d5       -0.450087       -0.792114  \n",
       "\n",
       "[4292 rows x 12 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dim = \"2D\"\n",
    "measurement_list_df = flow.create_measurement_list_df()\n",
    "source_df = flow.create_source_df(dim)\n",
    "detector_df = flow.create_detector_df(dim)\n",
    "source_merge = pd.merge(measurement_list_df, source_df, on=\"source_index\")\n",
    "source_merge\n",
    "merged_source_detector_df = pd.merge(\n",
    "    source_merge, detector_df, on=\"detector_index\"\n",
    ")\n",
    "source_detector_df = merged_source_detector_df.copy()\n",
    "source_detector_df.insert(\n",
    "    0, \"channel_num\", source_detector_df[\"measurement_list_index\"] - 1\n",
    ")\n",
    "source_detector_df.drop(\"source_index_y\", axis=1, inplace=True)\n",
    "source_detector_df.rename(columns={\"source_index_x\": \"source_index\"}, inplace=True)\n",
    "source_detector_df.sort_values(\"channel_num\", inplace=True)\n",
    "source_detector_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9c1e24752b3065052c27c07c0a22748a6118d05dd89b50e77c79e7ce74e5970"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
